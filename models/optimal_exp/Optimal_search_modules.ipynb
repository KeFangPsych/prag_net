{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e0adbb0-394f-41ae-aa78-5458b381d6ae",
   "metadata": {},
   "source": [
    "# Core Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ec36ea3-df44-4680-bed6-d0907952961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import warnings\n",
    "import itertools\n",
    "from typing import List, Dict, Union, Optional, Tuple, TypeVar, Iterator, Callable, Any, Literal\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import gammaln, logsumexp\n",
    "\n",
    "USE_PRECISE_LOGSPACE = False\n",
    "\n",
    "np.seterr(divide='ignore', under='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2959c53e-1454-4823-b57b-8b0a6fdec06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DEMO =True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67e4402-59b3-4d29-96d7-46ae3d1e4d83",
   "metadata": {},
   "source": [
    "## Core Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27523d56-16c4-4fc3-a341-36f05470fb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_M_product(LA, LB, precise=False, margin=None):\n",
    "    \"\"\"\n",
    "    Compute the matrix product of two matrices A and B in log-space\n",
    "    The inputs LA, LB are log-transformed values of non-negative matrix A and B.\n",
    "    The output LC is the product of A and B in log-space.\n",
    "\n",
    "    In the log-space, the dot product is computed as:\n",
    "        LC[i,j] = log( sum_{s=1}^k exp(LA[i,s] + LB[s,j]) )\n",
    "                = m +  log( sum_{s=1}^k exp(LA[i,s] + LB[s,j] - m) )\n",
    "\n",
    "    Uses a more spacious shift that still prevents overflow but more robust to underflow:\n",
    "          m = max_s (LA[i,s]+LB[s,j]) + log(n) - log(M_max) + margin\n",
    "    where margin defaults to -log(eps) for float64.\n",
    "\n",
    "    Input Validations:\n",
    "      - Both LA and LB must be numpy ndarrays.\n",
    "      - Both LA and LB must be 2-dimensional.\n",
    "      - The number of columns in LA must equal the number of rows in LB.\n",
    "      - Both LA and LB must have a floating-point data type (to represent log values).\n",
    "      - Neither LA nor LB should contain positive infinity.\n",
    "\n",
    "    Parameters:\n",
    "      LA : np.ndarray\n",
    "          2D numpy array of shape (m, k) representing the log-transformed values of matrix A\n",
    "          (i.e., LA[i,s] = log(a[i,s]), with a[i,s] >= 0 and log(0) = -np.inf).\n",
    "      LB : np.ndarray\n",
    "          2D numpy array of shape (k, n) representing the log-transformed values of matrix B\n",
    "          (i.e., LB[s,j] = log(b[s,j]), with b[s,j] >= 0 and log(0) = -np.inf).\n",
    "\n",
    "    Returns:\n",
    "      LC : np.ndarray\n",
    "          2D numpy array of shape (m, n) representing the log-space dot product:\n",
    "          LC[i,j] = log(sum_{s=1}^k exp(LA[i,s] + LB[s,j])).\n",
    "\n",
    "    Raises:\n",
    "      TypeError: If LA or LB is not a numpy ndarray, or if the arrays are not of a floating-point type.\n",
    "      ValueError: If LA or LB is not 2-dimensional, if their inner dimensions are incompatible,\n",
    "                  or if either input contains positive infinity.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Validations ---\n",
    "    if not (isinstance(LA, np.ndarray) and isinstance(LB, np.ndarray)):\n",
    "        raise TypeError(\"Both LA and LB must be numpy ndarrays.\")\n",
    "    if LA.ndim != 2 or LB.ndim != 2:\n",
    "        raise ValueError(\"Both LA and LB must be 2-dimensional.\")\n",
    "    if LA.shape[1] != LB.shape[0]:\n",
    "        raise ValueError(\"The number of columns in LA must equal the number of rows in LB for multiplication.\")\n",
    "    if not (np.issubdtype(LA.dtype, np.floating) and np.issubdtype(LB.dtype, np.floating)):\n",
    "        raise TypeError(\"Both LA and LB must have a floating-point data type representing log values.\")\n",
    "    if np.any(LA == np.inf) or np.any(LB == np.inf):\n",
    "        raise ValueError(\"Input matrices must not contain positive infinity.\")\n",
    "    LA = np.asarray(LA, dtype=np.float64)\n",
    "    LB = np.asarray(LB, dtype=np.float64)\n",
    "\n",
    "    # --- Compute all pairwise products in log-space: shape (m, k, n) ---\n",
    "    log_products = LA[:, :, None] + LB[None, :, :]\n",
    "\n",
    "    if not precise:\n",
    "        LC = logsumexp(log_products, axis=1)\n",
    "\n",
    "    else:\n",
    "        # --- Prepare constants ---\n",
    "        if margin is None:\n",
    "            margin = -np.log(np.finfo(LA.dtype).eps)   # ≈ 36 for float64\n",
    "        M_max = np.finfo(LA.dtype).max\n",
    "        k = LA.shape[1]\n",
    "\n",
    "        # --- Spacious Log-Sum-Exp trick ---\n",
    "        max_lp = np.max(log_products, axis=1)   # shape (m,n)\n",
    "        all_neginf = (max_lp == -np.inf) # mask for all -inf slices\n",
    "        shift = max_lp + np.log(k) - np.log(M_max) + margin # the shift parameter\n",
    "        shift = np.where(all_neginf, 0.0, shift)   # avoiding shifting all -inf slices by -inf\n",
    "\n",
    "        shifted_lp = log_products - shift[:, None, :]\n",
    "\n",
    "        exp_shifted_lp = np.exp(shifted_lp)\n",
    "\n",
    "        sum_exp_shifted_lp = np.apply_along_axis(math.fsum, 1, exp_shifted_lp)  # shape (m,n)\n",
    "\n",
    "        LC = shift + np.log(np.clip(sum_exp_shifted_lp, np.finfo(float).tiny, None))\n",
    "        LC = np.where(sum_exp_shifted_lp == 0.0, -np.inf, LC)\n",
    "\n",
    "    return LC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd38c4e8-5f9e-4456-a45c-8288fcf77776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_column_normalize(LX, precise=False):\n",
    "    \"\"\"\n",
    "    Normalize a matrix of log-values so that each column values sum to one.\n",
    "\n",
    "    In the log-space, the normalization by columns is computed as:\n",
    "        NLX[i,j] = log( X[i,j] / sum_{s=1}^n X[s,j] )\n",
    "                 = log( X[i,j] ) - log( sum_{s=1}^n X[s,j] )\n",
    "                 = LX[i,j] - log( sum_{s=1}^n exp(LX[s,j]) )\n",
    "                 = LX[i,j] - [ m[j] + log( sum_{s=1}^n exp(LX[s,j] - m[j]) ) ]\n",
    "                 = ( LX[i,j] -  m[j] ) - log( sum_{s=1}^n exp(LX[s,j] - m[j]) )\n",
    "\n",
    "    Uses a more spacious shift that still prevents overflow but more robust to underflow:\n",
    "          m[j] = max_s {LX[s,j]} + log(n) - log(M_max) + margin\n",
    "    where margin defaults to -log(eps) for float64.\n",
    "\n",
    "    Input Validations:\n",
    "      - LX must be numpy ndarrays.\n",
    "      - LX must be 2-dimensional.\n",
    "      - LX must have a floating-point data type (to represent log values).\n",
    "      - LX must NOT contain positive infinity.\n",
    "\n",
    "    Parameters:\n",
    "    LX (np.array): A 2d-array containing the log(x_i) values.\n",
    "    precise (bool): If True, uses a highly precise custom implementation\n",
    "                   with enhanced numerical stability. If False, uses scipy.special.logsumexp\n",
    "                   for faster computation with standard numerical stability.\n",
    "\n",
    "    Returns:\n",
    "    np.array: A 2d-array containing the log of normalized values.\n",
    "\n",
    "    Raises:\n",
    "      TypeError: If LX is not a numpy ndarray, or if the arrays are not of a floating-point type.\n",
    "      ValueError: If LX is not 2-dimensional, or if input contains positive infinity.\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Validations ---\n",
    "    if not isinstance(LX, np.ndarray):\n",
    "        raise TypeError(\"LX must be numpy ndarrays.\")\n",
    "    if LX.ndim != 2:\n",
    "        raise ValueError(\"LX must be 2-dimensional.\")\n",
    "    if not np.issubdtype(LX.dtype, np.floating):\n",
    "        raise TypeError(\"LX must have a floating-point data type representing log values.\")\n",
    "    if np.any(LX == np.inf):\n",
    "        raise ValueError(\"Input matrices must not contain positive infinity.\")\n",
    "    if np.any(np.isnan(LX)):\n",
    "        raise ValueError(\"Input contains NaN\")\n",
    "        \n",
    "    if not precise:\n",
    "        # Compute log column sums using scipy's optimized implementation\n",
    "        log_column_sums = logsumexp(LX, axis=0)\n",
    "\n",
    "        # Handle all -inf columns (same as original)\n",
    "        all_neg_inf_cols = np.all(LX == -np.inf, axis=0)\n",
    "        if np.any(all_neg_inf_cols):\n",
    "            warnings.warn(\"Input has column of all -inf\", UserWarning)\n",
    "            log_column_sums = np.where(all_neg_inf_cols, 0, log_column_sums)\n",
    "\n",
    "        return LX - log_column_sums\n",
    "\n",
    "    else:\n",
    "        # --- Perform the precise log-space normalization ---\n",
    "        M_LX = np.max(LX, axis=0) # shape: (n,)\n",
    "        if np.any(M_LX == -np.inf):\n",
    "            warnings.warn(\"Input has column of all -inf\", UserWarning)\n",
    "\n",
    "        n = LX.shape[0]\n",
    "        margin = -np.log(np.finfo(LX.dtype).eps)\n",
    "        shift = M_LX + np.log(n) - np.log(np.finfo(LX.dtype).max) + margin\n",
    "\n",
    "        # all -inf slice will have max of -inf and thus no need to shift\n",
    "        zeros_shift = shift == -np.inf\n",
    "        shift = np.where(zeros_shift, 0, shift)\n",
    "\n",
    "        LX_shifted = LX - shift  # shape: (m, n) - (1, n) = (m, n)\n",
    "\n",
    "        # Exponentials of the normalized values.\n",
    "        exp_LX_shifted = np.exp(LX_shifted)\n",
    "\n",
    "        # Sum along the axis s.\n",
    "        sum_exp_LX_shifted = np.apply_along_axis(math.fsum, axis=0, arr=exp_LX_shifted)\n",
    "        log_sum_exp_LX_shifted = np.log(np.clip(sum_exp_LX_shifted, np.finfo(float).tiny, None))\n",
    "        log_sum_exp_LX_shifted[zeros_shift] = 0\n",
    "\n",
    "        return LX_shifted - log_sum_exp_LX_shifted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0834e719-5124-4b9c-b277-2b4da4ffd95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_column_softmax(X, alpha, precise=False):\n",
    "    \"\"\"\n",
    "    Take in a score matrix X of shape (m, n), compute either\n",
    "      - the log‑softmax (alpha is float) or\n",
    "      - the log‑argmax (alpha == \"determ\") down each column,\n",
    "    and return a (m, n) array of log‑probabilities.\n",
    "\n",
    "    In log‑space, for each column j, log-softmax is calculated as:\n",
    "        m_j = max_k X[k,j] + log(m) - log(M_max) + margin\n",
    "        log_prob[i,j] = alpha * (X[i,j] − m_j)\n",
    "                         − log( sum_{l=1}^m exp( alpha * (X[l,j] − m_j) ) )\n",
    "\n",
    "    For alpha==\"determ\", ties at the max split equally:\n",
    "        log_prob[i,j] = -log(count_max_j)  if X[i,j] == max_k X[k,j]\n",
    "                      = -inf               otherwise\n",
    "\n",
    "    Input Validations:\n",
    "      - X must be a numpy.ndarray of floats, 2-dimensional, with no NaN/inf\n",
    "      - alpha must be a positive float or the string \"determ\"\n",
    "\n",
    "    Returns:\n",
    "      log_prob : np.ndarray of shape (m, n), the log-probabilities\n",
    "    \"\"\"\n",
    "\n",
    "    # Validate X\n",
    "    if not isinstance(X, np.ndarray):\n",
    "        raise TypeError(\"X must be a numpy.ndarray\")\n",
    "    if X.ndim != 2:\n",
    "        raise ValueError(f\"X must be 2-dimensional (got ndim={X.ndim})\")\n",
    "    if not np.issubdtype(X.dtype, np.floating):\n",
    "        raise TypeError(\"X must have a floating-point dtype\")\n",
    "    if np.isnan(X).any():\n",
    "        raise ValueError(\"X must not contain NaN values\")\n",
    "    if np.any(X == np.inf):\n",
    "        raise ValueError(\"X must not contain positive infinity.\")\n",
    "\n",
    "    # Validate alpha\n",
    "    if not (isinstance(alpha, float) or (isinstance(alpha, str) and alpha == \"determ\")):\n",
    "        raise TypeError(\"alpha must be a float or the string 'determ'\")\n",
    "    if isinstance(alpha, float) and alpha <= 0:\n",
    "        raise ValueError(\"alpha must be positive when using softmax\")\n",
    "\n",
    "    # Branch on alpha\n",
    "    if alpha == \"determ\":\n",
    "        # Hard argmax with equal splitting of ties\n",
    "        col_max = np.max(X, axis=0)\n",
    "        if np.any(col_max == -np.inf):  # Reject columns that are all -inf\n",
    "            raise ValueError(\"Cannot compute softmax: some columns are all -inf\")\n",
    "        mask = X == col_max\n",
    "        counts = mask.sum(axis=0)\n",
    "        log_prob = np.where(mask, -np.log(counts)[np.newaxis, :], -np.inf)\n",
    "\n",
    "    else:\n",
    "\n",
    "        if not precise:\n",
    "            # Softmax branch - simplified with scipy.special.logsumexp\n",
    "            if np.any(np.max(X, axis=0) == -np.inf):  # Reject columns that are all -inf\n",
    "                raise ValueError(\"Cannot compute softmax: some columns are all -inf\")\n",
    "            scaled = alpha * X\n",
    "            log_prob = scaled - logsumexp(scaled, axis=0)\n",
    "\n",
    "        else:\n",
    "            # Softmax branch\n",
    "            m, n = X.shape\n",
    "            # Compute spacious shift\n",
    "            scaled = alpha * X\n",
    "            col_max = np.max(scaled, axis=0)\n",
    "            if np.any(col_max == -np.inf):  # Reject columns that are all -inf\n",
    "                raise ValueError(\"Cannot compute softmax: some columns are all -inf\")\n",
    "            margin = -np.log(np.finfo(X.dtype).eps)\n",
    "            M_max = np.finfo(X.dtype).max\n",
    "            shift = col_max + np.log(m) - np.log(M_max) + margin\n",
    "            # Scale, shift, exponentiate, and sum\n",
    "            scaled_shifted = scaled - shift\n",
    "            exp_scaled_shifted = np.exp(scaled_shifted)\n",
    "            sum_exp_scaled_shifted = np.apply_along_axis(math.fsum, 0, exp_scaled_shifted)\n",
    "\n",
    "            # Compute log-sum-exp and normalize\n",
    "            log_sum_exp_scaled_shifted = np.log(sum_exp_scaled_shifted)\n",
    "            log_prob = scaled_shifted - log_sum_exp_scaled_shifted\n",
    "\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7114f981-7b88-4c39-bfb4-008cc546e646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In linear space:\n",
      "[[0.25 0.3  0.45 0.4 ]\n",
      " [0.2  0.2  0.2  0.2 ]\n",
      " [0.55 0.5  0.35 0.4 ]]\n",
      "\n",
      "In log-space (default): False\n",
      "[[0.25 0.3  0.45 0.4 ]\n",
      " [0.2  0.2  0.2  0.2 ]\n",
      " [0.55 0.5  0.35 0.4 ]]\n",
      "\n",
      "In log-space (precise):\n",
      "[[0.25 0.3  0.45 0.4 ]\n",
      " [0.2  0.2  0.2  0.2 ]\n",
      " [0.55 0.5  0.35 0.4 ]]\n",
      "\n",
      "In log-space (inprecise):\n",
      "[[0.25 0.3  0.45 0.4 ]\n",
      " [0.2  0.2  0.2  0.2 ]\n",
      " [0.55 0.5  0.35 0.4 ]]\n"
     ]
    }
   ],
   "source": [
    "## CODE_DEMO ##\n",
    "## matrix product\n",
    "if RUN_DEMO:\n",
    "    def demo_log_M_product():\n",
    "        test_mtx_1 = np.array([\n",
    "            [0.1,0.6],\n",
    "            [0.2,0.2],\n",
    "            [0.7,0.2]])\n",
    "    \n",
    "        test_mtx_2 = np.array([\n",
    "            [0.7,0.6,0.3,0.4],\n",
    "            [0.3,0.4,0.7,0.6]])\n",
    "    \n",
    "        log_test_mtx_1 = np.log(test_mtx_1)\n",
    "        log_test_mtx_2 = np.log(test_mtx_2)\n",
    "    \n",
    "        print(\"In linear space:\")\n",
    "        print(test_mtx_1 @ test_mtx_2)\n",
    "        print()\n",
    "        print(\"In log-space (default):\", USE_PRECISE_LOGSPACE)\n",
    "        print(np.exp(log_M_product(log_test_mtx_1, log_test_mtx_2)))\n",
    "        print()\n",
    "        print(\"In log-space (precise):\")\n",
    "        print(np.exp(log_M_product(log_test_mtx_1, log_test_mtx_2, precise=True)))\n",
    "        print()\n",
    "        print(\"In log-space (inprecise):\")\n",
    "        print(np.exp(log_M_product(log_test_mtx_1, log_test_mtx_2, precise=False)))\n",
    "\n",
    "\n",
    "    demo_log_M_product()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "faea372e-328f-4b1d-a02a-b51f928c8d3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "[[  0   1   2]\n",
      " [100   2   2]\n",
      " [  0   3   2]\n",
      " [  0   4   0]]\n",
      "\n",
      "In log-space (default): False\n",
      "[[0.         0.1        0.33333333]\n",
      " [1.         0.2        0.33333333]\n",
      " [0.         0.3        0.33333333]\n",
      " [0.         0.4        0.        ]]\n",
      "\n",
      "In log-space (precise):\n",
      "[[0.         0.1        0.33333333]\n",
      " [1.         0.2        0.33333333]\n",
      " [0.         0.3        0.33333333]\n",
      " [0.         0.4        0.        ]]\n",
      "\n",
      "In log-space (inprecise):\n",
      "[[0.         0.1        0.33333333]\n",
      " [1.         0.2        0.33333333]\n",
      " [0.         0.3        0.33333333]\n",
      " [0.         0.4        0.        ]]\n"
     ]
    }
   ],
   "source": [
    "## CODE_DEMO ##\n",
    "# Column normalization\n",
    "if RUN_DEMO:\n",
    "    def demo_log_column_normalize():\n",
    "        test_mtx = np.array(\n",
    "            [[0, 1, 2],\n",
    "            [100, 2, 2],\n",
    "            [0, 3, 2],\n",
    "            [0, 4, 0]])\n",
    "    \n",
    "        log_test_mtx = np.log(test_mtx)\n",
    "        print(\"Input\")\n",
    "        print(test_mtx)\n",
    "        print()\n",
    "        print(\"In log-space (default):\", USE_PRECISE_LOGSPACE)\n",
    "        print(np.exp(log_column_normalize(log_test_mtx)))\n",
    "        print()\n",
    "        print(\"In log-space (precise):\")\n",
    "        print(np.exp(log_column_normalize(log_test_mtx, precise=True)))\n",
    "        print()\n",
    "        print(\"In log-space (inprecise):\")\n",
    "        print(np.exp(log_column_normalize(log_test_mtx, precise=False)))\n",
    "\n",
    "    demo_log_column_normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0089537c-f047-4a23-9dc5-79e12323c526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      "[[ 0.   0.2 20. ]\n",
      " [ 1.   0.2 20. ]\n",
      " [ 0.   0.1 10. ]]\n",
      "\n",
      "In linear space:\n",
      "[[4.53958078e-05 4.22318798e-01 5.00000000e-01]\n",
      " [9.99909208e-01 4.22318798e-01 5.00000000e-01]\n",
      " [4.53958078e-05 1.55362403e-01 1.86003799e-44]]\n",
      "\n",
      "In log-space (default): precise =  False\n",
      "[[4.53958078e-05 4.22318798e-01 5.00000000e-01]\n",
      " [9.99909208e-01 4.22318798e-01 5.00000000e-01]\n",
      " [4.53958078e-05 1.55362403e-01 1.86003799e-44]]\n",
      "\n",
      "In log-space (precise):\n",
      "[[4.53958078e-05 4.22318798e-01 5.00000000e-01]\n",
      " [9.99909208e-01 4.22318798e-01 5.00000000e-01]\n",
      " [4.53958078e-05 1.55362403e-01 1.86003799e-44]]\n",
      "\n",
      "In log-space (inprecise):\n",
      "[[4.53958078e-05 4.22318798e-01 5.00000000e-01]\n",
      " [9.99909208e-01 4.22318798e-01 5.00000000e-01]\n",
      " [4.53958078e-05 1.55362403e-01 1.86003799e-44]]\n"
     ]
    }
   ],
   "source": [
    "## CODE_DEMO ##\n",
    "# Column Softmax\n",
    "if RUN_DEMO:\n",
    "    def demo_log_column_softmax():\n",
    "        test_mtx = np.array([\n",
    "            [0, 0.2, 20.],\n",
    "            [1, 0.2, 20.],\n",
    "            [0, 0.1, 10.]\n",
    "        ])\n",
    "        \n",
    "        from scipy.special import softmax\n",
    "\n",
    "        print(\"Input\")\n",
    "        print(test_mtx)\n",
    "        print()\n",
    "\n",
    "        print(\"In linear space:\")\n",
    "        print(softmax(10 * test_mtx, axis=0))\n",
    "        print()\n",
    "\n",
    "        print(\"In log-space (default): precise = \", USE_PRECISE_LOGSPACE)\n",
    "        print(np.exp(log_column_softmax(X=test_mtx, alpha=10.0)))\n",
    "        print()\n",
    "\n",
    "        print(\"In log-space (precise):\")\n",
    "        print(np.exp(log_column_softmax(X=test_mtx, alpha=10.0, precise=True)))\n",
    "        print()\n",
    "\n",
    "        print(\"In log-space (inprecise):\")\n",
    "        print(np.exp(log_column_softmax(X=test_mtx, alpha=10.0, precise=False)))\n",
    "\n",
    "    demo_log_column_softmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031b61ad-4c26-414e-8152-3bdd1e58e932",
   "metadata": {},
   "source": [
    "## World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36c9b12-97a0-4e1a-832c-845147f068a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World:\n",
    "    \"\"\"\n",
    "    A class representing the world state in the pragmatic communication game.\n",
    "\n",
    "    This class encapsulates the complete state space of possible observations,\n",
    "    their likelihoods under different theta values, and semantic truth values of utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    # Class constants\n",
    "    SEMANTIC_OPERATORS = {\n",
    "        \"all\": lambda x, N: int(x == N),\n",
    "        \"most\": lambda x, N: int(x > N / 2),\n",
    "        \"some\": lambda x, N: int(x >= 1),\n",
    "        \"no\": lambda x, N: int(x == 0)\n",
    "    }\n",
    "    QUANTIFIERS = [\"all\", \"most\", \"some\", \"no\"]\n",
    "    PREDICATES = [\"successful\", \"unsuccessful\"]\n",
    "    DEFAULT_THETA_VALUES: np.ndarray = np.round(np.linspace(0, 1, 11), 1)\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        n: int,\n",
    "        m: int,\n",
    "        theta_values: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the world with given parameters and compute all necessary tables.\"\"\"\n",
    "        # Validate n and m parameters\n",
    "        if not isinstance(n, int) or not isinstance(m, int):\n",
    "            raise ValueError(\"n and m must be integers\")\n",
    "        if n < 1 or m < 1:\n",
    "            raise ValueError(\"n and m must be positive\")\n",
    "\n",
    "        self.n = n\n",
    "        self.m = m\n",
    "        self.complex = n > 1\n",
    "\n",
    "        # Validate and process theta values\n",
    "        self.theta_values = self._validate_theta_values(theta_values)\n",
    "\n",
    "        try:\n",
    "            # Generate possible outcomes\n",
    "            self.possible_outcomes = self._generate_possible_outcomes(self.n, self.m)\n",
    "\n",
    "            # Compute success likelihoods\n",
    "            self.suc_log_likelihood_theta = self._compute_successes_log_likelihoods(\n",
    "                self.n, self.m, self.theta_values\n",
    "            )\n",
    "\n",
    "            # Compute observation likelihoods\n",
    "            self.obs_log_likelihood_theta = self._compute_observation_log_likelihoods(\n",
    "                self.n, self.m, self.theta_values, self.possible_outcomes\n",
    "            )\n",
    "\n",
    "            # Compute utterance truth values\n",
    "            self.utterance_truth = self._compute_utterance_truth_values(\n",
    "                self.n, self.m, self.possible_outcomes,\n",
    "                self.QUANTIFIERS, self.PREDICATES, self.SEMANTIC_OPERATORS\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize world state: {str(e)}\")\n",
    "\n",
    "    def _validate_theta_values(\n",
    "        self,\n",
    "        theta_values: Optional[np.ndarray]\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Validate and process theta values.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        theta_values : Optional[np.ndarray]\n",
    "            Array of possible theta values between 0 and 1. If None, uses DEFAULT_THETA_VALUES.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Validated array of theta values.\n",
    "        \"\"\"\n",
    "        if theta_values is None:\n",
    "            return self.DEFAULT_THETA_VALUES\n",
    "\n",
    "        if not isinstance(theta_values, np.ndarray):\n",
    "            raise ValueError(\"theta_values must be a numpy array\")\n",
    "        if not np.all((theta_values >= 0) & (theta_values <= 1)):\n",
    "            raise ValueError(\"All theta values must be between 0 and 1\")\n",
    "        if not np.array_equal(theta_values, np.unique(theta_values)):\n",
    "            raise ValueError(\"theta values must be arranged and not duplicating\")\n",
    "        if not np.array_equal(theta_values, np.round(theta_values, decimals=10)):\n",
    "            warnings.warn(\"theta values above precision is rounded to 10 decimals\",\n",
    "                         UserWarning)\n",
    "            if not np.array_equal(np.round(theta_values, decimals=10),\n",
    "                                 np.unique(np.round(theta_values, decimals=10))):\n",
    "                warnings.warn(\"Rounded theta values are duplicating, they will be collapsed\",\n",
    "                             UserWarning)\n",
    "\n",
    "        return np.unique(np.round(theta_values, decimals=10))\n",
    "\n",
    "    def _generate_possible_outcomes(self, n: int, m: int) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"\n",
    "        Generate all possible outcomes as frequency tuples.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of independent Binomial experiments.\n",
    "        m : int\n",
    "            Number of Bernoulli trials per experiment.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        List[Tuple[int, ...]]\n",
    "            List of all possible frequency tuples.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert the generator to a list and return\n",
    "            return list(self._generate_outcome_tuples(n, m))\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to generate possible outcomes: {str(e)}\")\n",
    "\n",
    "    def _generate_outcome_tuples(self, n: int, m: int) -> Iterator[Tuple[int, ...]]:\n",
    "        \"\"\"\n",
    "        Generate all tuples (n_0, n_1, ..., n_m) of nonnegative integers\n",
    "        such that sum(n_i) = n.\n",
    "        Uses the stars and bars method.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of independent Binomial experiments.\n",
    "        m : int\n",
    "            Number of Bernoulli trials per experiment.\n",
    "\n",
    "        Yields\n",
    "        ------\n",
    "        Tuple[int, ...]\n",
    "            Each possible outcome frequency tuple.\n",
    "        \"\"\"\n",
    "        for dividers in itertools.combinations(range(n + m), m):\n",
    "            counts = []\n",
    "            prev = -1\n",
    "            for d in dividers:\n",
    "                counts.append(d - prev - 1)\n",
    "                prev = d\n",
    "            counts.append(n + m - prev - 1)\n",
    "            yield tuple(counts)\n",
    "\n",
    "    def _compute_successes_log_likelihoods(\n",
    "        self,\n",
    "        n: int,\n",
    "        m: int,\n",
    "        theta_values: np.ndarray\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute log P(S=s | theta) for S ~ Binomial(N, theta), where N = n*m.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of independent Binomial experiments.\n",
    "        m : int\n",
    "            Number of Bernoulli trials per experiment.\n",
    "        theta_values : np.ndarray\n",
    "            Array of possible theta values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows indexed by s (total successes),\n",
    "            columns by theta values, and values are log-probabilities.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            N = n * m\n",
    "            thetas = theta_values\n",
    "\n",
    "            # Precompute log binomial coefficient ln[ C(N, s) ] for s=0..N\n",
    "            s = np.arange(N+1)\n",
    "            log_binom = (gammaln(N+1) - gammaln(s+1) - gammaln(N-s+1))\n",
    "\n",
    "            # Prepare an array of shape (N+1, len(thetas))\n",
    "            log_probs = np.empty((N+1, thetas.size))\n",
    "\n",
    "            # Handle interior thetas (0 < theta < 1)\n",
    "            mask = (thetas > 0) & (thetas < 1)\n",
    "            theta_int = thetas[mask]\n",
    "            if mask.any():\n",
    "                log_theta = np.log(theta_int)[None, :]   # shape (1, k)\n",
    "                log_one_minus = np.log(1 - theta_int)[None, :]\n",
    "                # broadcast: log_binom[:,None] + s[:,None]*log_theta + (N-s)[:,None]*log_one_minus\n",
    "                log_probs[:, mask] = (log_binom[:, None] + s[:, None] * log_theta +\n",
    "                                      (N - s)[:, None] * log_one_minus)\n",
    "\n",
    "            # theta = 0: only s=0 has log‐prob 0, others −inf\n",
    "            mask_zero = thetas == 0\n",
    "            log_probs[:, mask_zero] = -np.inf\n",
    "            log_probs[0, mask_zero] = 0.0\n",
    "\n",
    "            # theta = 1: only s=N has log‐prob 0, others −inf\n",
    "            mask_one = thetas == 1\n",
    "            log_probs[:, mask_one] = -np.inf\n",
    "            log_probs[N, mask_one] = 0.0\n",
    "\n",
    "            # Build DataFrame: rows indexed by s, columns by theta\n",
    "            df = pd.DataFrame(log_probs,\n",
    "                             index=range(N+1),\n",
    "                             columns=thetas)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute success likelihoods: {str(e)}\")\n",
    "\n",
    "    def _compute_observation_log_likelihoods(\n",
    "        self,\n",
    "        n: int,\n",
    "        m: int,\n",
    "        theta_values: np.ndarray,\n",
    "        possible_outcomes: List[Tuple[int, ...]]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute a table of log-probabilities for each frequency tuple under each theta value.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of independent Binomial experiments.\n",
    "        m : int\n",
    "            Number of Bernoulli trials per experiment.\n",
    "        theta_values : np.ndarray\n",
    "            Array of possible theta values.\n",
    "        possible_outcomes : List[Tuple[int, ...]]\n",
    "            List of all possible frequency tuples.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows as frequency tuples and columns as theta values,\n",
    "            containing log-probabilities.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            mask_interior = (theta_values > 0) & (theta_values < 1)\n",
    "            theta_interior = theta_values[mask_interior]\n",
    "\n",
    "            j_vals = np.arange(m + 1)  # 0, 1, ..., m\n",
    "            log_binom = gammaln(m + 1) - gammaln(j_vals + 1) - gammaln(m - j_vals + 1)\n",
    "            base_const = gammaln(n + 1)\n",
    "\n",
    "            results = []      # list to hold log-probability vectors (one per frequency tuple)\n",
    "            index_labels = [] # frequency tuples\n",
    "\n",
    "            for counts in possible_outcomes:\n",
    "                counts_arr = np.array(counts)\n",
    "                full_log_prob = np.empty(theta_values.size, dtype=float)\n",
    "\n",
    "                # For theta = 0 and theta = 1, assign manually:\n",
    "                for idx, theta in enumerate(theta_values):\n",
    "                    if theta == 0:\n",
    "                        full_log_prob[idx] = 0.0 if counts[0] == n else -np.inf\n",
    "                    elif theta == 1:\n",
    "                        full_log_prob[idx] = 0.0 if counts[m] == n else -np.inf\n",
    "\n",
    "                if np.any(mask_interior):\n",
    "                    interior_log_theta = np.log(theta_interior)\n",
    "                    interior_log_one_minus_theta = np.log(1 - theta_interior)\n",
    "                    base = base_const - np.sum(gammaln(counts_arr + 1))\n",
    "                    terms = (log_binom[:, None] +\n",
    "                            j_vals[:, None] * interior_log_theta +\n",
    "                            (m - j_vals)[:, None] * interior_log_one_minus_theta)\n",
    "                    log_term = np.sum(counts_arr[:, None] * terms, axis=0)\n",
    "                    interior_result = base + log_term\n",
    "                    full_log_prob[mask_interior] = interior_result\n",
    "\n",
    "                results.append(full_log_prob)\n",
    "                index_labels.append(counts)\n",
    "\n",
    "            results_array = np.array(results)\n",
    "            df = pd.DataFrame(results_array, index=index_labels, columns=theta_values)\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute observation likelihoods: {str(e)}\")\n",
    "\n",
    "    def _compute_utterance_truth_values(\n",
    "        self,\n",
    "        n: int,\n",
    "        m: int,\n",
    "        counts_list: List[Tuple[int, ...]],\n",
    "        quantifiers: List[str],\n",
    "        predicates: List[str],\n",
    "        semantic_operators: Dict[str, Callable]\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Generate a truth table where each row is a possible utterance (as a string)\n",
    "        and each column is a possible frequency tuple (as a tuple of ints).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        n : int\n",
    "            Number of independent Binomial experiments.\n",
    "        m : int\n",
    "            Number of Bernoulli trials per experiment.\n",
    "        counts_list : List[Tuple[int, ...]]\n",
    "            List of all possible frequency tuples.\n",
    "        quantifiers : List[str]\n",
    "            List of quantifiers (\"all\", \"most\", \"some\", \"no\").\n",
    "        predicates : List[str]\n",
    "            List of predicates (\"successful\", \"unsuccessful\").\n",
    "        semantic_operators : Dict[str, Callable]\n",
    "            Dictionary mapping quantifiers to their semantic functions.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows as utterance strings, columns as frequency tuples,\n",
    "            and values as truth values (1 or 0).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Inner helper to list all utterances\n",
    "            def _generate_utterance(n: int, quantifiers: List[str], predicates: List[str]) -> List[Tuple[str, ...]]:\n",
    "                if n > 1:\n",
    "                    return list(itertools.product(quantifiers, quantifiers, predicates))\n",
    "                else:\n",
    "                    return list(itertools.product(quantifiers, predicates))\n",
    "\n",
    "            utterances = _generate_utterance(n, quantifiers, predicates)\n",
    "            counts_array = np.array(counts_list)             # shape (num_outcomes, n)\n",
    "            truth_dict = {}\n",
    "\n",
    "            if n == 1:\n",
    "                # Single experiment: utterance = (quantifier, predicate)\n",
    "                for utter in utterances:\n",
    "                    q, p = utter\n",
    "                    if p == \"successful\":\n",
    "                        vec = np.array([semantic_operators[q](j, m)\n",
    "                                      for j in range(m + 1)])\n",
    "                    else:  # p == \"unsuccessful\"\n",
    "                        vec = np.array([semantic_operators[q](m - j, m)\n",
    "                                      for j in range(m + 1)])\n",
    "                    truth_vals = counts_array.dot(vec)\n",
    "                    utter_str = \",\".join(utter)\n",
    "                    truth_dict[utter_str] = truth_vals\n",
    "            else:\n",
    "                # Multiple experiments: utterance = (quantifier1, quantifier2, predicate)\n",
    "                for utter in utterances:\n",
    "                    q1, q2, p = utter\n",
    "                    if p == \"successful\":\n",
    "                        vec = np.array([semantic_operators[q2](j, m)\n",
    "                                      for j in range(m + 1)])\n",
    "                    else:  # p == \"unsuccessful\"\n",
    "                        vec = np.array([semantic_operators[q2](m - j, m)\n",
    "                                      for j in range(m + 1)])\n",
    "                    inner_sum = counts_array.dot(vec)\n",
    "                    truth_func = np.vectorize(lambda x: semantic_operators[q1](x, n))\n",
    "                    truth_vals = truth_func(inner_sum)\n",
    "                    utter_str = \",\".join(utter)\n",
    "                    truth_dict[utter_str] = truth_vals\n",
    "\n",
    "            # Keep the actual tuples as column labels\n",
    "            freq_labels = counts_list\n",
    "            df = pd.DataFrame(\n",
    "                data = np.array(list(truth_dict.values())).T,\n",
    "                index = freq_labels,\n",
    "                columns = list(truth_dict.keys())\n",
    "            )\n",
    "            # Transpose so rows are utterances, columns are outcome‑tuples\n",
    "            df = df.T\n",
    "\n",
    "            uncovered = [obs for obs in df.columns if df[obs].sum() == 0]\n",
    "            if uncovered:\n",
    "                raise ValueError(\n",
    "                    f\"No utterance covers the following observations: {uncovered}\"\n",
    "                )\n",
    "\n",
    "            return df\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute utterance truth values: {str(e)}\")\n",
    "\n",
    "\n",
    "    def sample(\n",
    "        self, \n",
    "        theta: float, \n",
    "        seed: Optional[int] = None, \n",
    "        reuse: bool = False\n",
    "    ) -> Tuple[int, ...]:\n",
    "        \"\"\"\n",
    "        Sample an observation set according to its probability under given theta.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : float\n",
    "            The theta value to use for sampling (must be one of the predefined values).\n",
    "        seed : Optional[int], default=None\n",
    "            Random seed for reproducible sampling.\n",
    "        reuse : bool, default=False\n",
    "            Whether to reuse cached RNG if seed matches. If False, always creates new RNG.\n",
    "            If True, reuses cached RNG when seed matches the previously used seed.\n",
    "    \n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[int, ...]\n",
    "            The sampled observation tuple.\n",
    "        \"\"\"\n",
    "        if not 0 <= theta <= 1:\n",
    "            raise ValueError(\"theta must be between 0 and 1\")\n",
    "    \n",
    "        closest_theta = self.theta_values[np.abs(self.theta_values - theta).argmin()]\n",
    "        if not np.isclose(theta, closest_theta, rtol=1e-10, atol=1e-10):\n",
    "            raise ValueError(\n",
    "                f\"theta {theta} not found in theta_values. \"\n",
    "                f\"Closest available value is {closest_theta}. \"\n",
    "                f\"Available values are {self.theta_values}\"\n",
    "            )\n",
    "    \n",
    "        probabilities = np.exp(self.obs_log_likelihood_theta[closest_theta])\n",
    "        \n",
    "        # Manage cached RNG based on reuse parameter\n",
    "        if reuse:\n",
    "            # Try to reuse cached RNG if seed matches\n",
    "            if (hasattr(self, '_cached_rng') and \n",
    "                hasattr(self, '_cached_seed') and \n",
    "                self._cached_seed == seed):\n",
    "                rng = self._cached_rng\n",
    "            else:\n",
    "                # Create new RNG and cache it, with appropriate warning\n",
    "                if not hasattr(self, '_cached_rng'):\n",
    "                    warnings.warn(\n",
    "                        f\"reuse=True but no cached RNG exists. Creating new RNG with seed={seed}\",\n",
    "                        UserWarning\n",
    "                    )\n",
    "                else:\n",
    "                    warnings.warn(\n",
    "                        f\"reuse=True but seed mismatch (cached: {self._cached_seed}, requested: {seed}). \"\n",
    "                        f\"Creating new RNG with seed={seed}\",\n",
    "                        UserWarning\n",
    "                    )\n",
    "                rng = np.random.default_rng(seed)\n",
    "                self._cached_rng = rng\n",
    "                self._cached_seed = seed\n",
    "        else:\n",
    "            # Always create new RNG (original behavior)\n",
    "            rng = np.random.default_rng(seed)\n",
    "        \n",
    "        sampled_observation = rng.choice(\n",
    "            a=probabilities.index,\n",
    "            p=probabilities.values\n",
    "        )\n",
    "        return sampled_observation\n",
    "    \n",
    "\n",
    "    def sample_run(\n",
    "        self, \n",
    "        theta: float, \n",
    "        n_round: int, \n",
    "        run_seed: int) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample multiple observations for a single simulation run.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : float\n",
    "            The theta value to sample from (will find closest available theta)\n",
    "        n_round : int\n",
    "            Number of observations to sample in this run\n",
    "        run_seed : int\n",
    "            Random seed for reproducible sampling\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with columns: ['observation', 'theta', 'run_seed', 'round_index']\n",
    "            Each row represents one sampled observation with its position in the sequence.\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        if not isinstance(n_round, int) or n_round < 1:\n",
    "            raise ValueError(\"n_round must be a positive integer\") \n",
    "        \n",
    "        if not 0 <= theta <= 1:\n",
    "            raise ValueError(\"theta must be between 0 and 1\")\n",
    "        \n",
    "        # Find closest theta\n",
    "        closest_theta = self.theta_values[np.abs(self.theta_values - theta).argmin()]\n",
    "        if not np.isclose(theta, closest_theta, rtol=1e-10, atol=1e-10):\n",
    "            warnings.warn(\n",
    "                f\"theta {theta} not exactly in theta_values. Using closest: {closest_theta}\",\n",
    "                UserWarning\n",
    "            )\n",
    "        \n",
    "        # Get probabilities for this theta (computed once)\n",
    "        log_probs = self.obs_log_likelihood_theta[closest_theta]\n",
    "        probabilities = np.exp(log_probs)\n",
    "        observations_list = list(probabilities.index)\n",
    "        prob_values = probabilities.values\n",
    "        \n",
    "        # Validation checks\n",
    "        if not np.isclose(np.sum(prob_values), 1.0, rtol=1e-10):\n",
    "            raise ValueError(f\"Probabilities don't sum to 1: {np.sum(prob_values)}\")\n",
    "        if np.any(prob_values < 0):\n",
    "            raise ValueError(\"Found negative probabilities\")\n",
    "        \n",
    "        # Sample using seeded RNG for reproducibility\n",
    "        rng = np.random.default_rng(run_seed)\n",
    "        sampled_indices = rng.choice(\n",
    "            len(observations_list), \n",
    "            size=n_round, \n",
    "            p=prob_values\n",
    "        )\n",
    "        \n",
    "        # Convert indices to actual observations\n",
    "        sampled_observations = [observations_list[idx] for idx in sampled_indices]\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            \"observation\": sampled_observations,\n",
    "            \"theta\": closest_theta,\n",
    "            \"run_seed\": run_seed,\n",
    "            \"round_index\": range(n_round)\n",
    "        })\n",
    "    \n",
    "    def sample_multiple_runs(\n",
    "        self, \n",
    "        theta: float, \n",
    "        n_run: int, \n",
    "        n_round: int, \n",
    "        base_seed: int = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Sample observations reproducibly for multiple simulation runs.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        theta : float  \n",
    "            The theta value to sample from (will find closest available theta)\n",
    "        n_run : int\n",
    "            Number of independent simulation runs\n",
    "        n_round : int\n",
    "            Number of observations to sample per run\n",
    "        base_seed : int, default=None\n",
    "            Base random seed for reproducibility. Each run gets base_seed + run_id\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with columns: ['theta', 'run_id', 'round_index', 'observation', 'run_seed']\n",
    "            Each row represents one sampled observation, with round_index indicating \n",
    "            the sequence position (0 to n_round-1) within each run.\n",
    "        \"\"\"\n",
    "        # Validate inputs\n",
    "        if not isinstance(n_run, int) or n_run < 1:\n",
    "            raise ValueError(\"n_run must be a positive integer\")\n",
    "        \n",
    "        # Collect results from each run\n",
    "        run_dataframes = []\n",
    "        for run_id in range(n_run):\n",
    "            run_seed = None if base_seed is None else base_seed + run_id\n",
    "            \n",
    "            # Use existing sample_run method\n",
    "            run_df = self.sample_run(theta=theta, n_round=n_round, run_seed=run_seed)\n",
    "            \n",
    "            # Add run_id to distinguish between runs\n",
    "            run_df['run_id'] = run_id\n",
    "            \n",
    "            run_dataframes.append(run_df)\n",
    "        \n",
    "        # Combine all runs into single DataFrame\n",
    "        combined_df = pd.concat(run_dataframes, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns for consistency with the original specification\n",
    "        return combined_df[['theta', 'run_id', 'round_index', 'observation', 'run_seed']]\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def utterances(self) -> List[str]:\n",
    "        \"\"\"Get list of all possible utterances (as strings).\"\"\"\n",
    "        return list(self.utterance_truth.index)\n",
    "\n",
    "    @property\n",
    "    def observations(self) -> List[Tuple[int, ...]]:\n",
    "        \"\"\"Get list of all possible observations (frequency tuples).\"\"\"\n",
    "        return list(self.obs_log_likelihood_theta.index)\n",
    "\n",
    "    @property\n",
    "    def suc_likelihood_theta(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the success likelihood table (actual probabilities)\n",
    "        by exponentiating the log-likelihood table.\n",
    "        \"\"\"\n",
    "        return np.exp(self.suc_log_likelihood_theta)\n",
    "\n",
    "    @property\n",
    "    def obs_likelihood_theta(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Return the observation likelihood table (actual probabilities)\n",
    "        by exponentiating the log-likelihood table.\n",
    "        \"\"\"\n",
    "        return np.exp(self.obs_log_likelihood_theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f235e51b-b7cc-429b-ba69-a80c8375e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEMO:\n",
    "    test_world = World(n=5, m=7)\n",
    "    print(test_world.n, \"experments each with\", test_world.m, \"trials\")\n",
    "    print(test_world.sample(0.0))\n",
    "    print(test_world.sample(0.5))\n",
    "    print(test_world.sample(1.0))\n",
    "    print()\n",
    "\n",
    "    test_seed = 12\n",
    "    test_true_theta = 0.7\n",
    "    test_num_obs = 3\n",
    "    print(f\"Sample each observation inidividually\") \n",
    "    print(f\"Call .sample() {test_num_obs} times with initial seed ={test_seed} under theta of {test_true_theta}\")\n",
    "    for _ in range(test_num_obs):\n",
    "        print(test_world.sample(0.7, seed = 12, reuse=True))\n",
    "    print()\n",
    "    \n",
    "    print(f\"Sample one sequence of observations\") \n",
    "    print(f\"Call .sample_run() one time with n_round = {test_num_obs} and initial seed ={test_seed}\")\n",
    "    print(test_world.sample_run(\n",
    "        theta = test_true_theta,\n",
    "        n_round = test_num_obs,\n",
    "        run_seed =12))\n",
    "    print()\n",
    "\n",
    "    print(f\"Sample two sequences of observations\") \n",
    "    print(f\"Call .sample_multiple_runs() one time with n_run = 2, n_round = {test_num_obs} and initial seed ={test_seed}\")\n",
    "    print(test_world.sample_multiple_runs(\n",
    "        theta= test_true_theta, \n",
    "        n_run = 2, \n",
    "        n_round = test_num_obs, \n",
    "        base_seed = 12\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0477dae0-2514-49f2-abd1-b162dad2304a",
   "metadata": {},
   "source": [
    "## Literal Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8333ed52-c827-4ac6-bd81-94d334c5db34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteralSpeaker:\n",
    "    \"\"\"\n",
    "    A literal speaker in the RSA communication game.\n",
    "\n",
    "    This speaker observes outcomes from the `World` at each round, updates\n",
    "    its beliefs over theta using Bayes' rule on the total number of successes,\n",
    "    and selects an utterance uniformly at random among those that are\n",
    "    semantically (literally) true of the observed data.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    world : World\n",
    "        The shared World model containing likelihoods and truth tables.\n",
    "    un_current_log_belief : np.ndarray\n",
    "        Unnormalized log-probabilities over theta values reflecting the speaker's prior/posterior.\n",
    "    utterance_log_prob_obs : pd.DataFrame\n",
    "        Log-probabilities of each utterance given each observation (frequency tuple).\n",
    "        Rows are utterance strings, columns are observations.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        world: 'World',\n",
    "        initial_beliefs_theta: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the literal speaker.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        world : World\n",
    "            Instance of the World class, containing theta grid and likelihood tables.\n",
    "        initial_beliefs_theta : np.ndarray, optional\n",
    "            1D array of prior probabilities over theta values (must sum to 1).\n",
    "            If None, a uniform prior is assumed.\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "\n",
    "        # Process or initialize the log-prior over theta\n",
    "        self.un_current_log_belief = self._process_initial_beliefs(\n",
    "            initial_beliefs_theta, self.world\n",
    "        )\n",
    "\n",
    "        # Precompute log P(u | O) for all utterance-observation pairs\n",
    "        self.utterance_log_prob_obs = self._compute_utterance_log_prob_obs(\n",
    "            self.world.utterance_truth\n",
    "        )\n",
    "\n",
    "    def _process_initial_beliefs(\n",
    "        self,\n",
    "        initial_beliefs_theta: Optional[np.ndarray],\n",
    "        world: 'World'\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Validate and convert initial belief vector to log-space.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_beliefs_theta : Optional[np.ndarray]\n",
    "            Initial beliefs over theta values, or None for uniform prior.\n",
    "        world : World\n",
    "            The World object containing theta_values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of log-probabilities over theta_values.\n",
    "        \"\"\"\n",
    "        n_theta = len(world.theta_values)\n",
    "\n",
    "        # Uniform prior if none provided\n",
    "        if initial_beliefs_theta is None:\n",
    "            return np.full(n_theta, -np.log(n_theta), dtype=float)\n",
    "\n",
    "        # Validate shape and range\n",
    "        if not isinstance(initial_beliefs_theta, np.ndarray):\n",
    "            raise ValueError(\"initial_beliefs_theta must be a numpy array\")\n",
    "        if initial_beliefs_theta.shape != (n_theta,):\n",
    "            raise ValueError(\n",
    "                f\"initial_beliefs_theta length {initial_beliefs_theta.size} must match \"\n",
    "                f\"number of theta values {n_theta}.\"\n",
    "            )\n",
    "        if not np.all((0 <= initial_beliefs_theta) & (initial_beliefs_theta <= 1)):\n",
    "            raise ValueError(\"All probabilities must be between 0 and 1.\")\n",
    "        if not np.isclose(initial_beliefs_theta.sum(), 1.0):\n",
    "            raise ValueError(\"Probabilities must sum to 1.\")\n",
    "\n",
    "        return np.log(initial_beliefs_theta)\n",
    "\n",
    "    def _compute_utterance_log_prob_obs(\n",
    "        self,\n",
    "        utterance_truth: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute the log-probabilities P(u | O) for every utterance and observation.\n",
    "\n",
    "        For each observation O, the literal speaker chooses uniformly among all u\n",
    "        such that Truth(u; O) = 1.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        utterance_truth : pd.DataFrame\n",
    "            Truth values for utterance-observation pairs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows = utterances, columns = observations,\n",
    "            and values = log P(u | O).\n",
    "        \"\"\"\n",
    "        truth = utterance_truth.astype(bool)\n",
    "\n",
    "        # Count how many utterances are true for each observation\n",
    "        true_counts = truth.sum(axis=0)\n",
    "\n",
    "        # Log-prob for each (u, O) is -log(num_true_utterances(O)) if true, else -inf\n",
    "        base_logp = -np.log(true_counts.values)\n",
    "        logp_matrix = np.tile(base_logp, (truth.shape[0], 1))\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            data=logp_matrix,\n",
    "            index=truth.index,\n",
    "            columns=truth.columns\n",
    "        )\n",
    "\n",
    "        # Mask out false utterance-observation pairs\n",
    "        return df.where(truth, -np.inf)\n",
    "\n",
    "    def update_and_speak(self, observation: Tuple[int, ...]) -> str:\n",
    "        \"\"\"\n",
    "        Given a new frequency-tuple observation, update beliefs and sample an utterance.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        observation : tuple of int\n",
    "            A frequency tuple (n_0, n_1, ..., n_m) observed this round.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        str\n",
    "            The chosen utterance (as a comma-separated string).\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the observation is not in the world's possible outcomes.\n",
    "        RuntimeError\n",
    "            If belief update or utterance sampling fails.\n",
    "        \"\"\"\n",
    "        # 1) Validate observation\n",
    "        if observation not in self.world.observations:\n",
    "            raise ValueError(f\"Observation {observation} not supported by the world.\")\n",
    "\n",
    "        # 2) Compute total successes S = sum_j (j * n_j)\n",
    "        counts = np.array(observation)\n",
    "        successes = int(counts.dot(np.arange(self.world.m + 1)))\n",
    "\n",
    "        # 3) Belief update in log-space: log P_new(theta) ∝ log P_old(theta) + log P(S | theta)\n",
    "        try:\n",
    "            log_lik = self.world.suc_log_likelihood_theta.loc[successes].values\n",
    "            self.un_current_log_belief = self.un_current_log_belief + log_lik\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Belief update failed: {e}\")\n",
    "\n",
    "        # 4) Sample utterance: P(u|O) stored in utterance_log_prob_obs[observation]\n",
    "        try:\n",
    "            uttrs = self.world.utterance_truth.loc[:, [observation]]\n",
    "            uttrs_true = uttrs.index[uttrs.iloc[:, 0] == 1].tolist()\n",
    "            if not uttrs_true:\n",
    "                raise RuntimeError(f\"No valid utterances for observation {observation}\")\n",
    "            return np.random.choice(uttrs_true)\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Utterance sampling failed: {e}\")\n",
    "\n",
    "    @property\n",
    "    def current_belief_theta(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the current beliefs normalized so they form a valid distribution.\n",
    "\n",
    "        Exponentiating and summing to 1.\n",
    "        \"\"\"\n",
    "        return np.exp(log_column_normalize(self.un_current_log_belief[:, None],\n",
    "                                           precise= USE_PRECISE_LOGSPACE).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c97c5a7-e018-4cda-8b46-b870b968950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEMO:\n",
    "    \n",
    "    test_LS = LiteralSpeaker(test_world)\n",
    "    test_sample = test_world.sample(0.0)\n",
    "    print(\"Sample a \")\n",
    "    print(test_sample)\n",
    "    test_uttr = test_LS.update_and_speak(test_sample)\n",
    "    print(test_uttr)\n",
    "    print(np.round(test_LS.current_belief_theta, 3))\n",
    "\n",
    "    test_LS = LiteralSpeaker(test_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c2b7b-9e66-44d0-b4b1-7807d129240e",
   "metadata": {},
   "source": [
    "## Literal Listener"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79b55a54-1d9a-4d66-b97e-f3892235fccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiteralListener:\n",
    "    \"\"\"\n",
    "    A literal listener in the RSA communication game.\n",
    "\n",
    "    The listener hears an utterance from a literal speaker S_0 and updates\n",
    "    its beliefs over the hidden parameter theta using Bayes' rule.\n",
    "    The update uses the speaker model P_S0(u | theta) which is computed\n",
    "    by marginalizing over possible observations.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    world : World\n",
    "        The shared World model containing likelihoods and truth tables.\n",
    "    un_current_log_belief : np.ndarray\n",
    "        The listener's current unnormalized log-probabilities over theta values (log P(theta)).\n",
    "    literal_speaker : LiteralSpeaker\n",
    "        A helper speaker instance used to access P_S0(u | O) tables.\n",
    "    utterance_log_likelihood_theta : pd.DataFrame\n",
    "        Log-likelihoods log P_S0(u | theta) for all utterances u and theta values.\n",
    "        Rows are utterance strings, columns are theta values.\n",
    "    theta_log_post_utterance : pd.DataFrame\n",
    "        Unnormalized log-posteriors for each utterance and theta.\n",
    "        Rows are theta values, columns are utterances.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        world: 'World',\n",
    "        initial_beliefs_theta: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the literal listener.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        world : World\n",
    "            Instance of the World class that provides theta grid, likelihoods,\n",
    "            and truth tables.\n",
    "        initial_beliefs_theta : np.ndarray, optional\n",
    "            1D array of prior probabilities over theta values (must sum to 1).\n",
    "            If None, a uniform prior is assumed.\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "\n",
    "        try:\n",
    "            # Set up the initial log-prior over theta\n",
    "            self.un_current_log_belief = self._process_initial_beliefs(\n",
    "                initial_beliefs_theta, self.world\n",
    "            )\n",
    "\n",
    "            # Instantiate a literal speaker to access P(u|O) = utterance_log_prob_obs\n",
    "            self.literal_speaker = LiteralSpeaker(self.world, initial_beliefs_theta)\n",
    "\n",
    "            # Precompute P(u|theta) = sum_O P(u|O) P(O|theta) in log-space\n",
    "            self.utterance_log_likelihood_theta = self._compute_utterance_log_likelihood_theta(\n",
    "                self.literal_speaker.utterance_log_prob_obs,\n",
    "                self.world.obs_log_likelihood_theta\n",
    "                )\n",
    "\n",
    "            # Combine with prior to get unnormalized log-posteriors for each utterance\n",
    "            self.theta_log_post_utterance = self._compute_theta_log_post_utterance(\n",
    "                self.utterance_log_likelihood_theta,\n",
    "                self.un_current_log_belief\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize listener: {str(e)}\")\n",
    "\n",
    "    def _process_initial_beliefs(\n",
    "        self,\n",
    "        initial_beliefs_theta: Optional[np.ndarray],\n",
    "        world: 'World'\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Validate and convert the initial belief vector into log-space.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        initial_beliefs_theta : Optional[np.ndarray]\n",
    "            Initial beliefs over theta values, or None for uniform prior.\n",
    "        world : World\n",
    "            The World object containing theta_values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        np.ndarray\n",
    "            Array of log-probabilities over theta_values.\n",
    "        \"\"\"\n",
    "        n_theta = len(world.theta_values)\n",
    "\n",
    "        # Use a uniform prior if none provided: log(1 / n_theta)\n",
    "        if initial_beliefs_theta is None:\n",
    "            return np.full(n_theta, -np.log(n_theta), dtype=float)\n",
    "\n",
    "        # Validate that the supplied prior is a proper probability distribution\n",
    "        if not isinstance(initial_beliefs_theta, np.ndarray):\n",
    "            raise ValueError(\"initial_beliefs_theta must be a numpy array\")\n",
    "        if initial_beliefs_theta.shape != (n_theta,):\n",
    "            raise ValueError(\n",
    "                f\"initial_beliefs_theta length {initial_beliefs_theta.size} must match \"\n",
    "                f\"number of theta values {n_theta}.\"\n",
    "            )\n",
    "        if not np.all((0 <= initial_beliefs_theta) & (initial_beliefs_theta <= 1)):\n",
    "            raise ValueError(\"All probabilities must be between 0 and 1.\")\n",
    "        if not np.isclose(initial_beliefs_theta.sum(), 1.0):\n",
    "            raise ValueError(\"Probabilities must sum to 1.\")\n",
    "\n",
    "        # Convert to log-space for numerical stability\n",
    "        return np.log(initial_beliefs_theta)\n",
    "\n",
    "    def _compute_utterance_log_likelihood_theta(\n",
    "        self,\n",
    "        utterance_log_prob_obs: pd.DataFrame,\n",
    "        obs_log_likelihood_theta: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute the log-likelihood P(u | theta) for each utterance and theta.\n",
    "\n",
    "        This uses the precomputed P(u | O) from the literal speaker and the\n",
    "        observation likelihood P(O | theta) from the world, performing a\n",
    "        log-space matrix product to marginalize over observations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        utterance_log_prob_obs: pd.DataFrame\n",
    "            Log-probabilities of each utterance given each observation\n",
    "            (frequency tuple) from self.literal_speaker.\n",
    "            Rows are utterance strings, columns are observations.\n",
    "        obs_log_likelihood_theta: pd.DataFrame\n",
    "            Log-probabilities of each observation (frequency tuple) given each\n",
    "            theta from self.world.\n",
    "            Rows are frequency tuples, columns are thetas.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Rows are utterance strings, columns are theta values,\n",
    "            entries are log P(u | theta).\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            # log_M_product performs a numerically stable log-sum-exp matrix multiply\n",
    "            return pd.DataFrame(\n",
    "                log_M_product(\n",
    "                    utterance_log_prob_obs.values,\n",
    "                    obs_log_likelihood_theta.values,\n",
    "                    precise= USE_PRECISE_LOGSPACE\n",
    "                ),\n",
    "                index=utterance_log_prob_obs.index,\n",
    "                columns=obs_log_likelihood_theta.columns\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute utterance likelihoods: {str(e)}\")\n",
    "\n",
    "    def _compute_theta_log_post_utterance(\n",
    "        self,\n",
    "        utterance_log_likelihood_theta: pd.DataFrame,\n",
    "        un_current_log_belief: np.ndarray) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute unnormalized log-posteriors for every utterance.\n",
    "\n",
    "        Combines the listener's current log-prior with the\n",
    "        log-likelihood log P(u | theta) to produce\n",
    "        log P(theta, u) = log P(theta) + log P(u | theta).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        utterance_log_likelihood_theta: pd.DataFrame\n",
    "            Rows are utterance strings, columns are theta values,\n",
    "            entries are log P(u | theta).\n",
    "        un_current_log_belief: np.ndarray\n",
    "            Array of unnormalized log-belief/probabilities over theta_values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Rows are theta values, columns are utterances.\n",
    "            entries are unnormalized log P(theta | u).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Broadcasting adds the log-prior vector to each column of P(u|theta)\n",
    "             return (utterance_log_likelihood_theta + un_current_log_belief).T\n",
    "             # transpose so rows=theta, cols=utterance\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to update posterior distributions: {str(e)}\")\n",
    "\n",
    "    def listen_and_update(self, utterance: str) -> None:\n",
    "        \"\"\"\n",
    "        Incorporate a received utterance and update the listener's beliefs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        utterance : str\n",
    "            The utterance string produced by the speaker in this round.\n",
    "\n",
    "        Updates\n",
    "        -------\n",
    "        un_current_log_belief : np.ndarray\n",
    "            Replaces with log P(theta | utterance) (unnormalized).\n",
    "        theta_log_post_utterance : pd.DataFrame\n",
    "            Recomputed table for the next round's potential updates.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If the utterance is not recognized.\n",
    "        RuntimeError\n",
    "            If the belief update fails.\n",
    "        \"\"\"\n",
    "        # Ensure the utterance is valid in this world\n",
    "        if utterance not in self.world.utterances:\n",
    "            raise ValueError(\n",
    "                f\"Utterance '{utterance}' not found in possible utterances.\\n\"\n",
    "                f\"Valid utterances: {self.world.utterances}\"\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # 1) Fetch the log-posteriors for this utterance\n",
    "            self.un_current_log_belief = self.theta_log_post_utterance[utterance].values\n",
    "\n",
    "            # 2) Recompute posteriors for next round\n",
    "            self.theta_log_post_utterance = self._compute_theta_log_post_utterance(\n",
    "                self.utterance_log_likelihood_theta,\n",
    "                self.un_current_log_belief\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to update beliefs: {str(e)}\")\n",
    "\n",
    "    @property\n",
    "    def current_belief_theta(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the listener's normalized belief over theta as probabilities.\n",
    "\n",
    "        Exponentiates and normalizes the current log-belief to sum to 1.\n",
    "        \"\"\"\n",
    "        # Use stable log-column normalize and exponentiate\n",
    "        return np.exp(log_column_normalize(self.un_current_log_belief[:, None],\n",
    "                                           precise= USE_PRECISE_LOGSPACE).ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd43696-bd83-4485-af5b-7f4bfda4107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEMO:\n",
    "    test_LL = LiteralListener(test_world)\n",
    "\n",
    "    print(\"some,most,unsuccessful\")\n",
    "    test_LL.listen_and_update(\"some,most,unsuccessful\")\n",
    "    print(np.round(test_LL.current_belief_theta, 3))\n",
    "\n",
    "    test_LL = LiteralListener(test_world)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54169170-ee75-453f-b879-5a8390b4d745",
   "metadata": {},
   "source": [
    "## Pragmatic Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec96e7d2-8cc2-44f4-865e-a63acf3954d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PragmaticSpeaker_obs:\n",
    "\n",
    "    \"\"\"\n",
    "    A pragmatic speaker (S1) in an RSA-style communication game.\n",
    "\n",
    "    This speaker balances literal truth, informativeness, and persuasiveness\n",
    "    when choosing an utterance.  It relies on:\n",
    "      - a LiteralSpeaker to provide P_S0(u | O)\n",
    "      - a LiteralListener to track P_L0(theta) and related posteriors\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    world : World\n",
    "        The shared World model with likelihoods and truth tables.\n",
    "    omega : str\n",
    "        Type of world:\n",
    "        \"coop\" for cooperative world where speakers are all informative,\n",
    "        \"strat\" for stratigic world where speakers can be also persuasive,\n",
    "    psi : str\n",
    "        Speaker goal: \"inf\" for purely informative,\n",
    "        \"pers+\" to persuade the listener up,\n",
    "        \"pers-\" to persuade the listener down.\n",
    "    alpha : float or \"determ\"\n",
    "        Softmax temperature (or \"determ\" for deterministic tie-split).\n",
    "    beta : float\n",
    "        Weight on informativeness (beta=1 for pure info, 0 for pure persuasion).\n",
    "    update_internal : bool\n",
    "        If True, update the literal listener's internal state after speaking.\n",
    "    literal_speaker : LiteralSpeaker\n",
    "        Helper to access P_S0(u | O).\n",
    "    literal_listener : LiteralListener\n",
    "        Helper to track listener beliefs P_L0(theta) and P_L0(O).\n",
    "    utility : pd.DataFrame\n",
    "        (U × O) matrix of log-utilities V(u; O).\n",
    "    utterance_log_prob_obs : pd.DataFrame\n",
    "        (U × O) matrix of log P_S1(u | O).\n",
    "    \"\"\"\n",
    "\n",
    "    VALID_OMEGA_TYPES = {\"coop\", \"strat\"}\n",
    "    VALID_PSI_TYPES = {\"inf\", \"pers+\", \"pers-\"}\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        world: 'World',\n",
    "        omega: str,\n",
    "        psi: str,\n",
    "        update_internal: bool,\n",
    "        alpha: Union[float, str],\n",
    "        beta: float = 0.0,\n",
    "        initial_beliefs_theta: Optional[np.ndarray] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize the pragmatic speaker.\"\"\"\n",
    "        if omega not in self.VALID_OMEGA_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"omega must be one of {self.VALID_OMEGA_TYPES}, got '{omega}'\"\n",
    "            )\n",
    "        if psi not in self.VALID_PSI_TYPES:\n",
    "            raise ValueError(\n",
    "                f\"psi must be one of {self.VALID_PSI_TYPES}, got '{psi}'\"\n",
    "            )\n",
    "\n",
    "        self.world = world\n",
    "        self.omega = omega\n",
    "\n",
    "        if self.omega == \"coop\" and psi != \"inf\":\n",
    "            warnings.warn(\"when omega == coop, psi is forced to inf\",\n",
    "                            UserWarning)\n",
    "            self.psi = \"inf\"\n",
    "        else:\n",
    "            self.psi = psi\n",
    "\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.update_internal = update_internal\n",
    "\n",
    "        try:\n",
    "            # Initialize literal speaker keeping optimal Bayeisan belief in theta\n",
    "            self.literal_speaker = LiteralSpeaker(world, initial_beliefs_theta)\n",
    "            # Initialize literal listener as internal listner model\n",
    "            self.literal_listener = LiteralListener(world, initial_beliefs_theta)\n",
    "\n",
    "            # Compute utterance probabilities (this will cascade through all calculations)\n",
    "            self.utterance_log_prob_obs = self._compute_utterance_log_prob_obs(self.alpha)\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to initialize pragmatic speaker: {str(e)}\")\n",
    "\n",
    "    def _compute_log_informativeness(\n",
    "        self,\n",
    "        obs_log_likelihood_theta_values: np.ndarray,\n",
    "        un_current_log_belief: np.ndarray,\n",
    "        utterance_log_prob_obs: pd.DataFrame\n",
    "    ) -> Tuple[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Compute log-Inf(u; O) = log-P_L0(O|u).\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        obs_log_likelihood_theta_values : np.ndarray\n",
    "            Observation log-likelihoods for each theta.\n",
    "        un_current_log_belief : np.ndarray\n",
    "            Unnormalized log-beliefs over theta values.\n",
    "        utterance_log_prob_obs : pd.DataFrame\n",
    "            Log P(u|O) for all utterances and observations.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[np.ndarray, pd.DataFrame]\n",
    "            First: unnormalized_log_prob_O - Unnormalized log probability for each observation\n",
    "            Second: log_informativeness - DataFrame with log-informativeness for each utterance and observation\n",
    "        \"\"\"\n",
    "        # Compute unnormalized log-P(O)\n",
    "        unnormalized_log_prob_O = log_M_product(\n",
    "            obs_log_likelihood_theta_values,\n",
    "            un_current_log_belief[:, np.newaxis],\n",
    "            precise= USE_PRECISE_LOGSPACE\n",
    "        ).flatten()\n",
    "\n",
    "        # Compute log-P_L0(O|u)\n",
    "        unnormalized_obs_log_post_utterance = (\n",
    "            utterance_log_prob_obs +\n",
    "            unnormalized_log_prob_O[np.newaxis, :]\n",
    "        ).T\n",
    "\n",
    "        obs_log_post_utterance = pd.DataFrame(\n",
    "            log_column_normalize(unnormalized_obs_log_post_utterance.values,\n",
    "                                 precise= USE_PRECISE_LOGSPACE),\n",
    "            index=unnormalized_obs_log_post_utterance.index,\n",
    "            columns=unnormalized_obs_log_post_utterance.columns\n",
    "        )\n",
    "\n",
    "        # Return unnormalized_log_prob_O and log-informativeness\n",
    "        return unnormalized_log_prob_O, obs_log_post_utterance.T\n",
    "\n",
    "    def _compute_log_persuasiveness(\n",
    "        self,\n",
    "        psi: str,\n",
    "        theta_values: np.ndarray,\n",
    "        theta_log_post_utterance_values: np.ndarray\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Compute log-PersStr(u; psi) based on speaker type.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psi : str\n",
    "            Speaker goal: \"inf\", \"pers+\", or \"pers-\".\n",
    "        theta_values : np.ndarray\n",
    "            Array of possible theta values.\n",
    "        theta_log_post_utterance_values : np.ndarray\n",
    "            Unnormalized log-posteriors for each theta and utterance.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Tuple[pd.DataFrame, pd.DataFrame]\n",
    "            First: theta_log_expectation_utterance - DataFrame with expectation of theta for each utterance\n",
    "            Second: log_persuasiveness - DataFrame with log-persuasiveness for each utterance and observation\n",
    "        \"\"\"\n",
    "        # Compute expectation log-E[theta|u]\n",
    "        theta_values_zero = theta_values == 0\n",
    "        log_theta_values = np.log(np.clip(theta_values, np.finfo(float).tiny, None))\n",
    "        log_theta_values[theta_values_zero] = -np.inf\n",
    "\n",
    "        theta_log_expectation_utterance = pd.DataFrame(\n",
    "            log_M_product(\n",
    "                log_theta_values[np.newaxis, :],\n",
    "                log_column_normalize(theta_log_post_utterance_values,\n",
    "                                     precise= USE_PRECISE_LOGSPACE),\n",
    "                precise= USE_PRECISE_LOGSPACE\n",
    "            ),\n",
    "            columns=self.world.utterances\n",
    "        )\n",
    "\n",
    "        # Compute persuasiveness based on psi (speaker) type\n",
    "        n_cols = len(self.world.possible_outcomes)\n",
    "\n",
    "        # Persuade-up utility\n",
    "        if psi == \"pers+\":\n",
    "            values = theta_log_expectation_utterance.values.flatten()\n",
    "\n",
    "        # Persuade-down utility: log(1 - E(theta)) = log(E(1 -theta))\n",
    "        elif psi == \"pers-\":\n",
    "            theta_values_one = theta_values == 1\n",
    "            log_one_minus_theta_values = np.log(\n",
    "                np.clip(1 - theta_values, np.finfo(float).tiny, None)\n",
    "            )\n",
    "            log_one_minus_theta_values[theta_values_one] = -np.inf\n",
    "\n",
    "            values = log_M_product(\n",
    "                log_one_minus_theta_values[np.newaxis, :],\n",
    "                log_column_normalize(theta_log_post_utterance_values,\n",
    "                                     precise= USE_PRECISE_LOGSPACE),\n",
    "                precise= USE_PRECISE_LOGSPACE\n",
    "            ).flatten()\n",
    "\n",
    "        # Purely informative\n",
    "        else:  # psi == \"inf\"\n",
    "            values = np.zeros(len(self.world.utterances))\n",
    "\n",
    "        # Create persuasiveness DataFrame\n",
    "        log_persuasiveness = pd.DataFrame(\n",
    "            np.tile(values, (n_cols, 1)).T,\n",
    "            index=self.world.utterances,\n",
    "            columns=self.world.possible_outcomes\n",
    "        )\n",
    "\n",
    "        return theta_log_expectation_utterance, log_persuasiveness\n",
    "\n",
    "    def _compute_utility(self, psi: str, beta: float) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute utility V(u; O, psi) combining truth, informativeness, and persuasiveness.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        psi : str\n",
    "            Speaker goal: \"inf\", \"pers+\", or \"pers-\".\n",
    "        beta : float\n",
    "            Weight on informativeness (beta=1 for pure info, 0 for pure persuasion).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows as utterances, columns as observations, values as utility.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Compute informativeness\n",
    "            self.unnormalized_log_prob_O, self.log_informativeness = self._compute_log_informativeness(\n",
    "                self.world.obs_log_likelihood_theta.values,\n",
    "                self.literal_listener.un_current_log_belief,\n",
    "                self.literal_listener.literal_speaker.utterance_log_prob_obs\n",
    "            )\n",
    "\n",
    "            # Compute persuasiveness\n",
    "            self.theta_log_expectation_utterance, self.log_persuasiveness = self._compute_log_persuasiveness(\n",
    "                psi,\n",
    "                self.world.theta_values,\n",
    "                self.literal_listener.theta_log_post_utterance.values\n",
    "            )\n",
    "\n",
    "            # mask of literally false (Truth=0)\n",
    "            uttr_false = (self.world.utterance_truth == 0)\n",
    "\n",
    "            if psi == \"inf\":\n",
    "                # Purely informative\n",
    "                util = self.log_informativeness.copy()\n",
    "            elif beta == 0:\n",
    "                # Purely persuasive\n",
    "                util = self.log_persuasiveness.copy()\n",
    "            else:\n",
    "                # Mixed informative & persuasive\n",
    "                # Mask out any utterance that was impossible in either term\n",
    "                impossible = (\n",
    "                    (self.log_informativeness == -np.inf) |\n",
    "                    (self.log_persuasiveness == -np.inf)\n",
    "                )\n",
    "                # Weighted sum in log-space\n",
    "                inf_term = self.log_informativeness.copy().clip(lower=-np.finfo(float).max)\n",
    "                pers_term = self.log_persuasiveness.copy().clip(lower=-np.finfo(float).max)\n",
    "                util = beta * inf_term + (1 - beta) * pers_term\n",
    "                util[impossible] = -np.inf\n",
    "\n",
    "            # Finally enforce literal truth\n",
    "            util[uttr_false] = -np.inf\n",
    "\n",
    "            # Store utility for reference\n",
    "            #self.utility = util\n",
    "            return util\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute utility: {str(e)}\")\n",
    "\n",
    "    def _compute_utterance_log_prob_obs(self, alpha: Union[float, str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compute log probabilities of utterances given observations.\n",
    "        This is the main computation pipeline that calls all other computations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha : float or \"determ\"\n",
    "            Softmax temperature parameter.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            DataFrame with rows as utterances, columns as observations,\n",
    "            values as log P(u|O).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # First compute utility (this will cascade to compute informativeness and persuasiveness)\n",
    "            self.utility = self._compute_utility(self.psi, self.beta)\n",
    "\n",
    "            # Then apply softmax to get utterance probabilities\n",
    "            return pd.DataFrame(\n",
    "                log_column_softmax(\n",
    "                    self.utility.values,\n",
    "                    alpha,\n",
    "                    precise= USE_PRECISE_LOGSPACE),\n",
    "                index=self.utility.index,\n",
    "                columns=self.utility.columns\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to compute utterance probability table: {str(e)}\")\n",
    "\n",
    "    def update_and_speak(self, observation: Tuple[int, ...]) -> str:\n",
    "        \"\"\"Given an observation, update beliefs and sample an utterance.\"\"\"\n",
    "        try:\n",
    "            # Bayesian optimal update using observation\n",
    "            # i.e. update literal speaker's beliefs\n",
    "            self.literal_speaker.update_and_speak(observation)\n",
    "\n",
    "            # Sample utterance according to P(u|O)\n",
    "            utterance_log_probs = self.utterance_log_prob_obs.loc[:, [observation]]\n",
    "            selected_utterance = np.random.choice(\n",
    "                utterance_log_probs.index,\n",
    "                p=np.exp(utterance_log_probs.values.flatten())\n",
    "            )\n",
    "\n",
    "            # Only update internal state if update_internal is True\n",
    "            if self.update_internal:\n",
    "                self.literal_listener.listen_and_update(selected_utterance)\n",
    "\n",
    "                # Recompute utterance probabilities (this will cascade through all calculations)\n",
    "                self.utterance_log_prob_obs = self._compute_utterance_log_prob_obs(self.alpha)\n",
    "\n",
    "            return selected_utterance\n",
    "\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to update and select utterance: {str(e)}\")\n",
    "\n",
    "    @property\n",
    "    def current_belief_theta(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Return the speaker's current normalized belief over theta\n",
    "        (in linear space, summing to 1).\n",
    "        \"\"\"\n",
    "        # Grab the unnormalized log-belief from the embedded LiteralSpeaker\n",
    "        log_bel = self.literal_speaker.un_current_log_belief\n",
    "        # Normalize each theta-column in log-space and exponentiate\n",
    "        normalized = np.exp(\n",
    "            log_column_normalize(log_bel[:, None],\n",
    "                                 precise= USE_PRECISE_LOGSPACE)  # shape: (theta,1)\n",
    "        ).ravel()  # back to shape (theta,)\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b29b188-5a0b-4920-9515-7cad22fb65b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_DEMO:\n",
    "    test_PS = PragmaticSpeaker_obs(\n",
    "        world = test_world,\n",
    "        omega = \"strat\",\n",
    "        psi = \"pers+\",\n",
    "        update_internal = False,\n",
    "        alpha = 4.0)\n",
    "\n",
    "    print(test_PS.current_belief_theta)\n",
    "    test_sample = test_world.sample(0.0)\n",
    "    print(test_sample)\n",
    "    test_uttr = test_PS.update_and_speak(test_sample)\n",
    "    print(test_uttr)\n",
    "    print(np.round(test_PS.current_belief_theta, 3))\n",
    "\n",
    "    test_PS = PragmaticSpeaker_obs(\n",
    "        world = test_world,\n",
    "        omega = \"strat\",\n",
    "        psi = \"pers+\",\n",
    "        update_internal = False,\n",
    "        alpha = 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f5251f-1d63-4564-9a5a-e585d4d347df",
   "metadata": {},
   "source": [
    "# Sampling utterances and observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebebe8-3240-48c8-b172-35641a9027b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from typing import List, Tuple, Dict, Optional, Any, Union\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "from rsa_optimal_exp_core import (\n",
    "    World, LiteralSpeaker, PragmaticSpeaker_obs, USE_PRECISE_LOGSPACE\n",
    ")\n",
    "\n",
    "np.seterr(divide='ignore', under='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c2e027-93de-4913-aeac-fca7b31afaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRUE_ALPHA = 4.0\n",
    "TRUE_SPEAKER_CONFIGS = [\n",
    "    {\n",
    "        \"speaker_type\": \"literal\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c39c323-f154-4675-b256-a59e16010ad7",
   "metadata": {},
   "source": [
    "## Demostration (all-in-one, not optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a723355-ca61-4499-8626-73e96be9b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulation_data(\n",
    "    n: int,\n",
    "    m: int,\n",
    "    thetas: Union[List[float], np.ndarray],\n",
    "    T: int,\n",
    "    speaker_config: Dict[str, Any],\n",
    "    n_obs_seq: int,\n",
    "    n_utt_seq: int,\n",
    "    random_seed: Optional[int] = None,\n",
    "    theta_values: Optional[np.ndarray] = None,\n",
    "    return_format: str = \"nested\",\n",
    "    compute_obs_likelihood: str = \"none\"\n",
    ") -> Union[Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate simulated observation and utterance sequences for multiple theta values.\n",
    "\n",
    "    This function creates a complete simulation dataset by:\n",
    "    1. Creating a World instance\n",
    "    2. For each theta: sampling multiple observation sequences\n",
    "    3. For each observation sequence: generating multiple utterance sequences using the speaker model\n",
    "    4. Optionally: computing observation sequence likelihoods\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of independent experiments in the World.\n",
    "    m : int\n",
    "        Number of Bernoulli trials per experiment in the World.\n",
    "    thetas : List[float] or np.ndarray\n",
    "        List of theta values to simulate. Each must be in the World's theta_values.\n",
    "    T : int\n",
    "        Length of each observation/utterance sequence (number of rounds).\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration for generating utterances.\n",
    "        Required keys:\n",
    "        - speaker_type : str\n",
    "            \"literal\" or \"pragmatic\"\n",
    "        For pragmatic speaker (required):\n",
    "        - omega : str\n",
    "            \"coop\" or \"strat\"\n",
    "        - psi : str\n",
    "            \"inf\", \"pers+\", or \"pers-\"\n",
    "        - alpha : float or \"determ\"\n",
    "            Softmax temperature\n",
    "        - update_internal : bool\n",
    "            Whether to update internal listener model\n",
    "        For pragmatic speaker (optional):\n",
    "        - beta : float\n",
    "            Weight on informativeness (default 0.0)\n",
    "        - initial_beliefs_theta : np.ndarray or None\n",
    "            Initial prior over theta (default None = uniform)\n",
    "    n_obs_seq : int\n",
    "        Number of observation sequences to generate per theta.\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences to generate per observation sequence.\n",
    "    random_seed : Optional[int], default None\n",
    "        Base random seed for reproducibility.\n",
    "        - Observation sampling uses: random_seed + theta_idx * 1000 + obs_idx\n",
    "        - Utterance sampling uses: random_seed + theta_idx * 1000000 + obs_idx * 1000 + utt_idx\n",
    "    theta_values : Optional[np.ndarray], default None\n",
    "        Custom theta values for the World. If None, uses World's default.\n",
    "    return_format : str, default \"nested\"\n",
    "        Output format:\n",
    "        - \"nested\": Nested dictionary structure\n",
    "        - \"dataframe\": Flattened pandas DataFrame (one row per utterance sequence)\n",
    "    compute_obs_likelihood : str, default \"none\"\n",
    "        Whether to compute observation sequence log-likelihoods:\n",
    "        - \"none\": Don't compute likelihoods\n",
    "        - \"true\": Compute log P(obs_seq | true_theta) only\n",
    "            Adds column: obs_seq_log_likelihood_true\n",
    "        - \"all\": Compute log P(obs_seq | theta) for all thetas in the world\n",
    "            Adds columns: obs_seq_log_likelihood_true, obs_seq_log_likelihood_0p0, \n",
    "            obs_seq_log_likelihood_0p1, ..., obs_seq_log_likelihood_1p0, obs_seq_mle_theta\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If return_format == \"nested\":\n",
    "        Dict[str, Any] with structure:\n",
    "        {\n",
    "            \"world\": World,\n",
    "            \"config\": {\n",
    "                \"n\": int, \"m\": int, \"thetas\": list, \"T\": int,\n",
    "                \"speaker_config\": dict, \"n_obs_seq\": int, \"n_utt_seq\": int,\n",
    "                \"random_seed\": int or None, \"compute_obs_likelihood\": str\n",
    "            },\n",
    "            \"data\": {\n",
    "                theta_1: {\n",
    "                    \"obs_sequences\": [\n",
    "                        {\n",
    "                            \"obs_seq_idx\": int,\n",
    "                            \"obs_seq\": List[Tuple[int, ...]],\n",
    "                            \"run_seed\": int,\n",
    "                            \"log_likelihood\": float,  # if compute_obs_likelihood == \"true\"\n",
    "                            \"log_likelihood_all_theta\": Dict[float, float],  # if compute_obs_likelihood == \"all\"\n",
    "                            \"mle_theta\": float,  # if compute_obs_likelihood == \"all\"\n",
    "                            \"utt_sequences\": [...]\n",
    "                        },\n",
    "                        ...\n",
    "                    ]\n",
    "                },\n",
    "                ...\n",
    "            }\n",
    "        }\n",
    "\n",
    "    If return_format == \"dataframe\":\n",
    "        pd.DataFrame with one row per utterance sequence:\n",
    "        - theta: float (true theta used for simulation)\n",
    "        - obs_seq_idx: int\n",
    "        - utt_seq_idx: int\n",
    "        - obs_seq: List[Tuple[int, ...]] (full observation sequence)\n",
    "        - utt_seq: List[str] (full utterance sequence)\n",
    "        - run_seed: int\n",
    "        - utt_seed: int\n",
    "        \n",
    "        If compute_obs_likelihood == \"true\":\n",
    "        - obs_seq_log_likelihood_true: float\n",
    "        \n",
    "        If compute_obs_likelihood == \"all\":\n",
    "        - obs_seq_log_likelihood_true: float\n",
    "        - obs_seq_log_likelihood_0p0: float (log P(obs_seq | theta=0.0))\n",
    "        - obs_seq_log_likelihood_0p1: float (log P(obs_seq | theta=0.1))\n",
    "        - ... (one column per theta in world.theta_values)\n",
    "        - obs_seq_log_likelihood_1p0: float (log P(obs_seq | theta=1.0))\n",
    "        - obs_seq_mle_theta: float\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If parameters are invalid or theta values not in World's theta_values.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> result = generate_simulation_data(\n",
    "    ...     n=3, m=2,\n",
    "    ...     thetas=[0.3, 0.5, 0.7],\n",
    "    ...     T=10,\n",
    "    ...     speaker_config={\n",
    "    ...         \"speaker_type\": \"pragmatic\",\n",
    "    ...         \"omega\": \"coop\",\n",
    "    ...         \"psi\": \"inf\",\n",
    "    ...         \"alpha\": 5.0,\n",
    "    ...         \"update_internal\": True\n",
    "    ...     },\n",
    "    ...     n_obs_seq=5,\n",
    "    ...     n_utt_seq=3,\n",
    "    ...     random_seed=42,\n",
    "    ...     compute_obs_likelihood=\"all\",\n",
    "    ...     return_format=\"dataframe\"\n",
    "    ... )\n",
    "    >>> # Each row is one utterance sequence\n",
    "    >>> print(result.columns.tolist())\n",
    "    ['theta', 'obs_seq_idx', 'utt_seq_idx', 'obs_seq', 'utt_seq', 'run_seed', 'utt_seed',\n",
    "     'obs_seq_log_likelihood_true', 'obs_seq_log_likelihood_0p0', ..., 'obs_seq_mle_theta']\n",
    "    \"\"\"\n",
    "\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(n, int) or n < 1:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "    if not isinstance(m, int) or m < 1:\n",
    "        raise ValueError(\"m must be a positive integer\")\n",
    "    if not isinstance(T, int) or T < 1:\n",
    "        raise ValueError(\"T must be a positive integer\")\n",
    "    if not isinstance(n_obs_seq, int) or n_obs_seq < 1:\n",
    "        raise ValueError(\"n_obs_seq must be a positive integer\")\n",
    "    if not isinstance(n_utt_seq, int) or n_utt_seq < 1:\n",
    "        raise ValueError(\"n_utt_seq must be a positive integer\")\n",
    "    if return_format not in [\"nested\", \"dataframe\"]:\n",
    "        raise ValueError(\"return_format must be 'nested' or 'dataframe'\")\n",
    "    if compute_obs_likelihood not in [\"none\", \"true\", \"all\"]:\n",
    "        raise ValueError(\"compute_obs_likelihood must be 'none', 'true', or 'all'\")\n",
    "\n",
    "    thetas = list(thetas) if isinstance(thetas, np.ndarray) else thetas\n",
    "    if len(thetas) == 0:\n",
    "        raise ValueError(\"thetas cannot be empty\")\n",
    "\n",
    "    # --- Validate speaker config ---\n",
    "    speaker_type = speaker_config.get(\"speaker_type\")\n",
    "    if speaker_type not in [\"literal\", \"pragmatic\"]:\n",
    "        raise ValueError(\n",
    "            f\"speaker_type must be 'literal' or 'pragmatic', got '{speaker_type}'\"\n",
    "        )\n",
    "\n",
    "    if speaker_type == \"pragmatic\":\n",
    "        required_keys = [\"omega\", \"psi\", \"alpha\", \"update_internal\"]\n",
    "        missing_keys = [k for k in required_keys if k not in speaker_config]\n",
    "        if missing_keys:\n",
    "            raise ValueError(\n",
    "                f\"Missing required keys for pragmatic speaker: {missing_keys}\"\n",
    "            )\n",
    "\n",
    "    # --- Create World ---\n",
    "    try:\n",
    "        world = World(n=n, m=m, theta_values=theta_values)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create World: {e}\")\n",
    "\n",
    "    # --- Validate thetas against World's theta_values ---\n",
    "    for theta in thetas:\n",
    "        closest = world.theta_values[np.abs(world.theta_values - theta).argmin()]\n",
    "        if not np.isclose(theta, closest, rtol=1e-10, atol=1e-10):\n",
    "            raise ValueError(\n",
    "                f\"theta {theta} not in World's theta_values. \"\n",
    "                f\"Closest: {closest}. Available: {world.theta_values}\"\n",
    "            )\n",
    "\n",
    "    # --- Prepare base config for speaker creation ---\n",
    "    base_speaker_config = {\n",
    "        \"speaker_type\": speaker_type,\n",
    "        \"initial_beliefs_theta\": speaker_config.get(\"initial_beliefs_theta\", None)\n",
    "    }\n",
    "    if speaker_type == \"pragmatic\":\n",
    "        base_speaker_config.update({\n",
    "            \"omega\": speaker_config[\"omega\"],\n",
    "            \"psi\": speaker_config[\"psi\"],\n",
    "            \"alpha\": speaker_config[\"alpha\"],\n",
    "            \"update_internal\": speaker_config[\"update_internal\"],\n",
    "            \"beta\": speaker_config.get(\"beta\", 0.0)\n",
    "        })\n",
    "\n",
    "    # --- Helper function to convert theta to column name ---\n",
    "    def theta_to_colname(theta_val: float) -> str:\n",
    "        \"\"\"Convert theta value to column name: 0.1 -> '0p1', 0.25 -> '0p25'\"\"\"\n",
    "        return f\"obs_seq_log_likelihood_{str(theta_val).replace('.', 'p')}\"\n",
    "\n",
    "    # --- Generate Data ---\n",
    "    data = {}\n",
    "    flat_records = []  # For dataframe format (one record per utterance sequence)\n",
    "\n",
    "    for theta_idx, theta in enumerate(thetas):\n",
    "        theta_data = {\"obs_sequences\": []}\n",
    "\n",
    "        # Sample observation sequences for this theta\n",
    "        obs_base_seed = (random_seed + theta_idx * 1000) if random_seed is not None else None\n",
    "\n",
    "        try:\n",
    "            obs_df = world.sample_multiple_runs(\n",
    "                theta=theta,\n",
    "                n_run=n_obs_seq,\n",
    "                n_round=T,\n",
    "                base_seed=obs_base_seed if obs_base_seed is not None else 123\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(\n",
    "                f\"Failed to sample observations for theta={theta}: {e}\"\n",
    "            )\n",
    "\n",
    "        # --- Collect all observation sequences for this theta (for batch likelihood) ---\n",
    "        all_obs_seqs_for_theta = []\n",
    "        obs_seq_metadata = []\n",
    "        \n",
    "        for obs_idx in range(n_obs_seq):\n",
    "            run_df = obs_df[obs_df[\"run_id\"] == obs_idx].sort_values(\"round_index\")\n",
    "            obs_seq = list(run_df[\"observation\"])\n",
    "            run_seed = run_df[\"run_seed\"].iloc[0]\n",
    "            \n",
    "            all_obs_seqs_for_theta.append(obs_seq)\n",
    "            obs_seq_metadata.append({\n",
    "                \"obs_seq_idx\": obs_idx,\n",
    "                \"obs_seq\": obs_seq,\n",
    "                \"run_seed\": run_seed\n",
    "            })\n",
    "\n",
    "        # --- Compute observation likelihoods (VECTORIZED) ---\n",
    "        obs_log_likelihoods_true = None   # log P(obs_seq | true_theta)\n",
    "        obs_log_likelihoods_all = None    # log P(obs_seq | all thetas) - shape (n_obs_seq, n_thetas)\n",
    "        obs_mle_thetas = None             # argmax_theta P(obs_seq | theta)\n",
    "        \n",
    "        if compute_obs_likelihood != \"none\" and len(all_obs_seqs_for_theta) > 0:\n",
    "            # Flatten all observations for batch lookup\n",
    "            all_obs_flat = [obs for seq in all_obs_seqs_for_theta for obs in seq]\n",
    "            all_obs_keys = [tuple(obs) if not isinstance(obs, tuple) else obs for obs in all_obs_flat]\n",
    "            \n",
    "            # Batch lookup: (n_obs_seq * T, n_thetas)\n",
    "            log_probs_flat = world.obs_log_likelihood_theta.loc[all_obs_keys].values\n",
    "            \n",
    "            # Reshape to (n_obs_seq, T, n_thetas)\n",
    "            n_thetas = len(world.theta_values)\n",
    "            log_probs_3d = log_probs_flat.reshape(n_obs_seq, T, n_thetas)\n",
    "            \n",
    "            # Sum over T → (n_obs_seq, n_thetas)\n",
    "            log_liks_all_theta = log_probs_3d.sum(axis=1)\n",
    "            \n",
    "            # Get theta index for true theta\n",
    "            theta_col_idx = np.where(np.isclose(world.theta_values, theta))[0][0]\n",
    "            \n",
    "            # Extract log-likelihood at true theta\n",
    "            obs_log_likelihoods_true = log_liks_all_theta[:, theta_col_idx]\n",
    "            \n",
    "            if compute_obs_likelihood == \"all\":\n",
    "                obs_log_likelihoods_all = log_liks_all_theta  # (n_obs_seq, n_thetas)\n",
    "                obs_mle_thetas = world.theta_values[np.argmax(log_liks_all_theta, axis=1)]\n",
    "\n",
    "        # --- Process each observation sequence ---\n",
    "        for obs_idx, meta in enumerate(obs_seq_metadata):\n",
    "            obs_seq = meta[\"obs_seq\"]\n",
    "            run_seed = meta[\"run_seed\"]\n",
    "            \n",
    "            obs_seq_data = {\n",
    "                \"obs_seq_idx\": meta[\"obs_seq_idx\"],\n",
    "                \"obs_seq\": obs_seq,\n",
    "                \"run_seed\": run_seed,\n",
    "                \"utt_sequences\": []\n",
    "            }\n",
    "            \n",
    "            # Add likelihood data to nested format\n",
    "            if compute_obs_likelihood in [\"true\", \"all\"]:\n",
    "                obs_seq_data[\"log_likelihood\"] = float(obs_log_likelihoods_true[obs_idx])\n",
    "            \n",
    "            if compute_obs_likelihood == \"all\":\n",
    "                obs_seq_data[\"log_likelihood_all_theta\"] = dict(\n",
    "                    zip(world.theta_values, obs_log_likelihoods_all[obs_idx])\n",
    "                )\n",
    "                obs_seq_data[\"mle_theta\"] = float(obs_mle_thetas[obs_idx])\n",
    "\n",
    "            # Generate multiple utterance sequences for this observation sequence\n",
    "            for utt_idx in range(n_utt_seq):\n",
    "                # Compute utterance seed for reproducibility\n",
    "                if random_seed is not None:\n",
    "                    utt_seed = random_seed + theta_idx * 1000000 + obs_idx * 1000 + utt_idx\n",
    "                    np.random.seed(utt_seed)\n",
    "                else:\n",
    "                    utt_seed = None\n",
    "\n",
    "                # Create fresh speaker instance\n",
    "                try:\n",
    "                    if speaker_type == \"literal\":\n",
    "                        speaker = LiteralSpeaker(\n",
    "                            world=world,\n",
    "                            initial_beliefs_theta=base_speaker_config.get(\"initial_beliefs_theta\")\n",
    "                        )\n",
    "                    else:  # pragmatic\n",
    "                        speaker = PragmaticSpeaker_obs(\n",
    "                            world=world,\n",
    "                            omega=base_speaker_config[\"omega\"],\n",
    "                            psi=base_speaker_config[\"psi\"],\n",
    "                            update_internal=base_speaker_config[\"update_internal\"],\n",
    "                            alpha=base_speaker_config[\"alpha\"],\n",
    "                            beta=base_speaker_config.get(\"beta\", 0.0),\n",
    "                            initial_beliefs_theta=base_speaker_config.get(\"initial_beliefs_theta\")\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Failed to create speaker for theta={theta}, \"\n",
    "                        f\"obs_idx={obs_idx}, utt_idx={utt_idx}: {e}\"\n",
    "                    )\n",
    "\n",
    "                # Generate utterance sequence\n",
    "                utt_seq = []\n",
    "                try:\n",
    "                    for obs in obs_seq:\n",
    "                        utt = speaker.update_and_speak(obs)\n",
    "                        utt_seq.append(utt)\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(\n",
    "                        f\"Failed to generate utterance at theta={theta}, \"\n",
    "                        f\"obs_idx={obs_idx}, utt_idx={utt_idx}: {e}\"\n",
    "                    )\n",
    "\n",
    "                utt_seq_data = {\n",
    "                    \"utt_seq_idx\": utt_idx,\n",
    "                    \"utt_seq\": utt_seq,\n",
    "                    \"utt_seed\": utt_seed\n",
    "                }\n",
    "                obs_seq_data[\"utt_sequences\"].append(utt_seq_data)\n",
    "\n",
    "                # --- Build flat record (one per utterance sequence) ---\n",
    "                record = {\n",
    "                    \"theta\": theta,\n",
    "                    \"obs_seq_idx\": obs_idx,\n",
    "                    \"utt_seq_idx\": utt_idx,\n",
    "                    \"obs_seq\": obs_seq,      # Full observation sequence as list\n",
    "                    \"utt_seq\": utt_seq,      # Full utterance sequence as list\n",
    "                    \"run_seed\": run_seed,\n",
    "                    \"utt_seed\": utt_seed\n",
    "                }\n",
    "                \n",
    "                # Add likelihood at true theta\n",
    "                if compute_obs_likelihood in [\"true\", \"all\"]:\n",
    "                    record[\"obs_seq_log_likelihood_true\"] = float(obs_log_likelihoods_true[obs_idx])\n",
    "                \n",
    "                # Add likelihoods for all thetas and MLE\n",
    "                if compute_obs_likelihood == \"all\":\n",
    "                    # Add column for each theta value\n",
    "                    for theta_col_idx, theta_val in enumerate(world.theta_values):\n",
    "                        col_name = theta_to_colname(theta_val)\n",
    "                        record[col_name] = float(obs_log_likelihoods_all[obs_idx, theta_col_idx])\n",
    "                    \n",
    "                    # Add MLE theta\n",
    "                    record[\"obs_seq_mle_theta\"] = float(obs_mle_thetas[obs_idx])\n",
    "                \n",
    "                flat_records.append(record)\n",
    "\n",
    "            theta_data[\"obs_sequences\"].append(obs_seq_data)\n",
    "\n",
    "        data[theta] = theta_data\n",
    "\n",
    "    # --- Prepare output ---\n",
    "    if return_format == \"dataframe\":\n",
    "        df = pd.DataFrame(flat_records)\n",
    "        \n",
    "        # Ensure consistent column ordering\n",
    "        base_cols = [\"theta\", \"obs_seq_idx\", \"utt_seq_idx\", \"obs_seq\", \"utt_seq\", \"run_seed\", \"utt_seed\"]\n",
    "        \n",
    "        if compute_obs_likelihood == \"true\":\n",
    "            col_order = base_cols + [\"obs_seq_log_likelihood_true\"]\n",
    "        elif compute_obs_likelihood == \"all\":\n",
    "            # Build column order: base + true + all thetas + mle\n",
    "            likelihood_cols = [theta_to_colname(tv) for tv in world.theta_values]\n",
    "            col_order = base_cols + [\"obs_seq_log_likelihood_true\"] + likelihood_cols + [\"obs_seq_mle_theta\"]\n",
    "        else:\n",
    "            col_order = base_cols\n",
    "        \n",
    "        return df[col_order]\n",
    "\n",
    "    else:  # nested\n",
    "        return {\n",
    "            \"world\": world,\n",
    "            \"config\": {\n",
    "                \"n\": n,\n",
    "                \"m\": m,\n",
    "                \"thetas\": thetas,\n",
    "                \"T\": T,\n",
    "                \"speaker_config\": speaker_config,\n",
    "                \"n_obs_seq\": n_obs_seq,\n",
    "                \"n_utt_seq\": n_utt_seq,\n",
    "                \"random_seed\": random_seed,\n",
    "                \"compute_obs_likelihood\": compute_obs_likelihood\n",
    "            },\n",
    "            \"data\": data\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e97753-0ec7-468b-934a-e9d9ae0eea77",
   "metadata": {},
   "source": [
    "## Single T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defbb6a1-9757-422f-b8c2-413edc2cea2e",
   "metadata": {},
   "source": [
    "### Observations Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b33ebd-aba6-42cd-b823-80fd622051ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_observation_sequences(\n",
    "    n: int,\n",
    "    m: int,\n",
    "    thetas: Union[List[float], np.ndarray],\n",
    "    T: int,\n",
    "    n_obs_seq: int,\n",
    "    random_seed: Optional[int] = None,\n",
    "    theta_values: Optional[np.ndarray] = None,\n",
    "    compute_obs_likelihood: str = \"none\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sample observation sequences for given theta values.\n",
    "    \n",
    "    This function handles Stage 1 of the simulation pipeline:\n",
    "    1. Create a World instance\n",
    "    2. Sample observation sequences for each theta\n",
    "    3. Optionally compute observation likelihoods\n",
    "    \n",
    "    The output can be passed to `generate_utterances_for_observations` \n",
    "    to generate utterances under different speaker configurations while\n",
    "    keeping observations fixed.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of independent experiments in the World.\n",
    "    m : int\n",
    "        Number of Bernoulli trials per experiment.\n",
    "    thetas : List[float] or np.ndarray\n",
    "        List of theta values to simulate. Each must be in the World's theta_values.\n",
    "    T : int\n",
    "        Length of each sequence (number of rounds).\n",
    "    n_obs_seq : int\n",
    "        Number of observation sequences per theta.\n",
    "    random_seed : Optional[int], default None\n",
    "        Base random seed for reproducibility.\n",
    "    theta_values : Optional[np.ndarray], default None\n",
    "        Custom theta grid for the World. If None, uses World's default [0, 0.1, ..., 1].\n",
    "    compute_obs_likelihood : str, default \"none\"\n",
    "        Whether to compute observation sequence log-likelihoods:\n",
    "        - \"none\": Don't compute likelihoods\n",
    "        - \"true\": Compute log P(obs_seq | true_theta) only\n",
    "                  (optimized: only loads single column from likelihood table)\n",
    "        - \"all\": Compute log P(obs_seq | theta) for all thetas in the world,\n",
    "                 plus MLE theta for each observation sequence\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary containing:\n",
    "        \n",
    "        - \"world\": World\n",
    "            The World object (reusable for utterance generation)\n",
    "        \n",
    "        - \"config\": Dict\n",
    "            Configuration parameters used:\n",
    "            {\n",
    "                \"n\": int,\n",
    "                \"m\": int, \n",
    "                \"thetas\": List[float],\n",
    "                \"T\": int,\n",
    "                \"n_obs_seq\": int,\n",
    "                \"random_seed\": Optional[int],\n",
    "                \"compute_obs_likelihood\": str\n",
    "            }\n",
    "        \n",
    "        - \"observations\": Dict[float, List[Dict]]\n",
    "            Mapping from theta -> list of observation data.\n",
    "            Each observation data dict contains:\n",
    "            {\n",
    "                \"obs_idx\": int,\n",
    "                \"obs_seq\": List[Tuple[int, ...]],\n",
    "                \"obs_run_seed\": int,\n",
    "                \"theta\": float,\n",
    "                \"log_lik_true_theta\": Optional[float],    # if compute != \"none\"\n",
    "                \"log_lik_all_theta\": Optional[np.ndarray], # if compute == \"all\"\n",
    "                \"mle_theta\": Optional[float],              # if compute == \"all\"\n",
    "                \"utterances\": None                         # Placeholder for Stage 2\n",
    "            }\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If parameters are invalid or theta values not in World's theta_values.\n",
    "    RuntimeError\n",
    "        If World creation or observation sampling fails.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Basic usage\n",
    "    >>> obs_data = sample_observation_sequences(\n",
    "    ...     n=3, m=2, thetas=[0.3, 0.5, 0.7], T=10, n_obs_seq=50,\n",
    "    ...     random_seed=42\n",
    "    ... )\n",
    "    >>> print(f\"Sampled {len(obs_data['observations'][0.3])} sequences for theta=0.3\")\n",
    "    \n",
    "    >>> # With likelihood computation\n",
    "    >>> obs_data = sample_observation_sequences(\n",
    "    ...     n=3, m=2, thetas=[0.3, 0.5, 0.7], T=10, n_obs_seq=50,\n",
    "    ...     random_seed=42, compute_obs_likelihood=\"all\"\n",
    "    ... )\n",
    "    >>> # Access likelihood and MLE\n",
    "    >>> log_lik = obs_data[\"observations\"][0.3][0][\"log_lik_true_theta\"]\n",
    "    >>> mle = obs_data[\"observations\"][0.3][0][\"mle_theta\"]\n",
    "    \"\"\"\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # INPUT VALIDATION\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    if not isinstance(n, int) or n < 1:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "    if not isinstance(m, int) or m < 1:\n",
    "        raise ValueError(\"m must be a positive integer\")\n",
    "    if not isinstance(T, int) or T < 1:\n",
    "        raise ValueError(\"T must be a positive integer\")\n",
    "    if not isinstance(n_obs_seq, int) or n_obs_seq < 1:\n",
    "        raise ValueError(\"n_obs_seq must be a positive integer\")\n",
    "    if compute_obs_likelihood not in [\"none\", \"true\", \"all\"]:\n",
    "        raise ValueError(\"compute_obs_likelihood must be 'none', 'true', or 'all'\")\n",
    "    \n",
    "    # Process thetas to list\n",
    "    thetas = list(thetas) if isinstance(thetas, np.ndarray) else list(thetas)\n",
    "    if len(thetas) == 0:\n",
    "        raise ValueError(\"thetas cannot be empty\")\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # CREATE WORLD\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    try:\n",
    "        world = World(n=n, m=m, theta_values=theta_values)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create World: {e}\")\n",
    "    \n",
    "    # Validate thetas against World's theta_values\n",
    "    for theta in thetas:\n",
    "        closest = world.theta_values[np.abs(world.theta_values - theta).argmin()]\n",
    "        if not np.isclose(theta, closest, rtol=1e-10, atol=1e-10):\n",
    "            raise ValueError(\n",
    "                f\"theta {theta} not in World's theta_values. \"\n",
    "                f\"Closest: {closest}. Available: {list(world.theta_values)}\"\n",
    "            )\n",
    "    \n",
    "    # -----------------------------------------------------------------\n",
    "    # SAMPLE OBSERVATIONS FOR EACH THETA\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    observations = {}\n",
    "    n_theta_vals = len(world.theta_values)\n",
    "    \n",
    "    for theta in thetas:\n",
    "        \n",
    "        # Sample observation sequences using World's method\n",
    "        # NOTE: Same seed for all thetas (aligned with multiT version)\n",
    "        try:\n",
    "            obs_df = world.sample_multiple_runs(\n",
    "                theta=theta,\n",
    "                n_run=n_obs_seq,\n",
    "                n_round=T,\n",
    "                base_seed=random_seed\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to sample observations for theta={theta}: {e}\")\n",
    "        \n",
    "        # EXTRACT OBSERVATION SEQUENCES (VECTORIZED VIA GROUPBY)\n",
    "        \n",
    "        out = (\n",
    "            obs_df\n",
    "            .sort_values([\"run_id\", \"round_index\"])\n",
    "            .groupby(\"run_id\", sort=True)\n",
    "            .agg(\n",
    "                obs_seq=(\"observation\", list),\n",
    "                obs_run_seed=(\"run_seed\", \"first\")  # Aligned field name\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        obs_seqs = out[\"obs_seq\"].tolist()\n",
    "        obs_run_seeds = out[\"obs_run_seed\"].tolist()\n",
    "        \n",
    "        # BUILD OBSERVATION RECORDS (aligned with multiT version)\n",
    "        \n",
    "        obs_list = [\n",
    "            {\n",
    "                \"obs_idx\": obs_idx,\n",
    "                \"obs_seq\": obs_seqs[obs_idx],\n",
    "                \"obs_run_seed\": obs_run_seeds[obs_idx], \n",
    "                \"theta\": theta,\n",
    "                \"log_lik_true_theta\": None,\n",
    "                \"log_lik_all_theta\": None,\n",
    "                \"mle_theta\": None,\n",
    "                \"utterances\": None \n",
    "            }\n",
    "            for obs_idx in range(n_obs_seq)\n",
    "        ]\n",
    "        \n",
    "        # COMPUTE OBSERVATION LIKELIHOODS\n",
    "        \n",
    "        if compute_obs_likelihood != \"none\":\n",
    "            \n",
    "            # Flatten all observations for batch lookup\n",
    "            all_obs_flat = [obs for seq in obs_seqs for obs in seq]\n",
    "            all_obs_keys = [tuple(obs) if not isinstance(obs, tuple) else obs \n",
    "                          for obs in all_obs_flat]\n",
    "            \n",
    "            # Find column index for true theta\n",
    "            theta_col_idx = np.where(np.isclose(world.theta_values, theta))[0][0]\n",
    "            true_theta_val = world.theta_values[theta_col_idx]\n",
    "            \n",
    "            if compute_obs_likelihood == \"true\":\n",
    "                \n",
    "                # Select ONLY the column for true theta\n",
    "                log_probs_flat_true = world.obs_log_likelihood_theta.loc[\n",
    "                    all_obs_keys, true_theta_val\n",
    "                ].values\n",
    "                \n",
    "                # Reshape to (n_obs_seq, T) and sum\n",
    "                log_probs_2d = log_probs_flat_true.reshape(n_obs_seq, T)\n",
    "                log_liks_true = log_probs_2d.sum(axis=1)\n",
    "                \n",
    "                # Distribute results\n",
    "                for i in range(n_obs_seq):\n",
    "                    obs_list[i][\"log_lik_true_theta\"] = float(log_liks_true[i])\n",
    "            \n",
    "            else:  # compute_obs_likelihood == \"all\"\n",
    "                \n",
    "                # Load full matrix\n",
    "                log_probs_flat = world.obs_log_likelihood_theta.loc[all_obs_keys].values\n",
    "                \n",
    "                # Reshape to (n_obs_seq, T, n_theta_vals) and sum over T\n",
    "                log_probs_3d = log_probs_flat.reshape(n_obs_seq, T, n_theta_vals)\n",
    "                log_liks_all = log_probs_3d.sum(axis=1)\n",
    "                \n",
    "                # Distribute results\n",
    "                for i in range(n_obs_seq):\n",
    "                    obs_list[i][\"log_lik_true_theta\"] = float(log_liks_all[i, theta_col_idx])\n",
    "                    obs_list[i][\"log_lik_all_theta\"] = log_liks_all[i].copy()\n",
    "                    obs_list[i][\"mle_theta\"] = float(\n",
    "                        world.theta_values[np.argmax(log_liks_all[i])]\n",
    "                    )\n",
    "        \n",
    "        # Store observations for this theta\n",
    "        observations[theta] = obs_list\n",
    "            \n",
    "    # -----------------------------------------------------------------\n",
    "    # RETURN RESULT\n",
    "    # -----------------------------------------------------------------\n",
    "    \n",
    "    return {\n",
    "        \"world\": world,\n",
    "        \"config\": {\n",
    "            \"n\": n,\n",
    "            \"m\": m,\n",
    "            \"thetas\": thetas,\n",
    "            \"T\": T,\n",
    "            \"n_obs_seq\": n_obs_seq,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"compute_obs_likelihood\": compute_obs_likelihood\n",
    "        },\n",
    "        \"observations\": observations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95ee5a8-17db-48d1-89ba-a444a4684932",
   "metadata": {},
   "source": [
    "### Utterances Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb5d936-2b15-4450-b164-b259d14b163f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_utterances_for_observations(\n",
    "    obs_data: Dict[str, Any],\n",
    "    speaker_config: Dict[str, Any],\n",
    "    n_utt_seq: int,\n",
    "    random_seed: Optional[int] = None,\n",
    "    return_format: str = \"dataframe\",\n",
    "    n_jobs: int = 1,\n",
    "    backend: str = \"loky\",\n",
    "    verbose: int = 0\n",
    ") -> Union[Dict[str, Any], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Generate utterance sequences for pre-sampled observations.\n",
    "    \n",
    "    This function handles Stage 2 of the simulation pipeline:\n",
    "    given pre-sampled observation sequences (from `sample_observation_sequences`),\n",
    "    generate utterance sequences under a specified speaker configuration.\n",
    "    \n",
    "    This design allows comparing different speaker configurations on the\n",
    "    exact same observation sequences, ensuring fair comparisons.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_data : Dict[str, Any]\n",
    "        Output from sample_observation_sequences containing:\n",
    "        - \"world\": World object\n",
    "        - \"config\": Dict with sampling configuration\n",
    "        - \"observations\": Dict mapping theta -> list of observation data\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration. Required keys:\n",
    "        - speaker_type : str (\"literal\" or \"pragmatic\")\n",
    "        For pragmatic speaker:\n",
    "        - omega, psi, alpha, update_internal (required)\n",
    "        - beta, initial_beliefs_theta (optional)\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences per observation sequence.\n",
    "    random_seed : Optional[int], default None\n",
    "        Base random seed for utterance generation.\n",
    "        Seed hierarchy: random_seed + task_idx * 10_000 + utt_idx\n",
    "    return_format : str, default \"dataframe\"\n",
    "        \"dataframe\" or \"nested\"\n",
    "    n_jobs : int, default 1\n",
    "        Number of parallel jobs (-1 for all cores).\n",
    "    backend : str, default \"loky\"\n",
    "        Joblib backend.\n",
    "    verbose : int, default 0\n",
    "        Verbosity level.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame (if return_format == \"dataframe\")\n",
    "        Columns (aligned with multiT):\n",
    "        - theta, obs_idx, utt_idx, obs_seq, utt_seq\n",
    "        - obs_run_seed, utt_seed\n",
    "        - log_lik_true_speaker\n",
    "        - log_lik_true_theta (if computed)\n",
    "        - log_lik_theta_0p0, log_lik_theta_0p1, ... (if compute == \"all\")\n",
    "        - mle_theta (if compute == \"all\")\n",
    "    \n",
    "    Dict[str, Any] (if return_format == \"nested\")\n",
    "        Hierarchical structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # INPUT VALIDATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    if not isinstance(n_utt_seq, int) or n_utt_seq < 1:\n",
    "        raise ValueError(\"n_utt_seq must be a positive integer\")\n",
    "    \n",
    "    if return_format not in [\"nested\", \"dataframe\"]:\n",
    "        raise ValueError(\"return_format must be 'nested' or 'dataframe'\")\n",
    "    \n",
    "    if backend not in [\"loky\", \"multiprocessing\", \"threading\"]:\n",
    "        raise ValueError(\"backend must be 'loky', 'multiprocessing', or 'threading'\")\n",
    "    \n",
    "    if not isinstance(obs_data, dict):\n",
    "        raise TypeError(\"obs_data must be a dictionary\")\n",
    "    \n",
    "    required_obs_data_keys = [\"world\", \"config\", \"observations\"]\n",
    "    missing_keys = [k for k in required_obs_data_keys if k not in obs_data]\n",
    "    if missing_keys:\n",
    "        raise ValueError(f\"obs_data missing required keys: {missing_keys}\")\n",
    "    \n",
    "    if not isinstance(speaker_config, dict):\n",
    "        raise TypeError(\"speaker_config must be a dictionary\")\n",
    "    \n",
    "    speaker_type = speaker_config.get(\"speaker_type\")\n",
    "    if speaker_type not in [\"literal\", \"pragmatic\"]:\n",
    "        raise ValueError(f\"speaker_type must be 'literal' or 'pragmatic', got '{speaker_type}'\")\n",
    "    \n",
    "    if speaker_type == \"pragmatic\":\n",
    "        required_pragmatic_keys = [\"omega\", \"psi\", \"alpha\", \"update_internal\"]\n",
    "        missing_pragmatic_keys = [k for k in required_pragmatic_keys if k not in speaker_config]\n",
    "        if missing_pragmatic_keys:\n",
    "            raise ValueError(f\"Missing required keys for pragmatic speaker: {missing_pragmatic_keys}\")\n",
    "        \n",
    "        if speaker_config[\"omega\"] not in [\"coop\", \"strat\"]:\n",
    "            raise ValueError(f\"omega must be 'coop' or 'strat', got '{speaker_config['omega']}'\")\n",
    "        if speaker_config[\"psi\"] not in [\"inf\", \"pers+\", \"pers-\"]:\n",
    "            raise ValueError(f\"psi must be 'inf', 'pers+', or 'pers-', got '{speaker_config['psi']}'\")\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # EXTRACT DATA FROM obs_data\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    world = obs_data[\"world\"]\n",
    "    observations = obs_data[\"observations\"]\n",
    "    obs_config = obs_data[\"config\"]\n",
    "    \n",
    "    thetas = obs_config[\"thetas\"]\n",
    "    compute_obs_likelihood = obs_config[\"compute_obs_likelihood\"]\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # SETUP PARALLELIZATION\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    if n_jobs == -1:\n",
    "        n_workers = cpu_count()\n",
    "    elif n_jobs < 0:\n",
    "        n_workers = max(1, cpu_count() + 1 + n_jobs)\n",
    "    else:\n",
    "        n_workers = max(1, n_jobs)\n",
    "    \n",
    "    is_parallel = n_workers > 1\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # VERBOSE OUTPUT\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    if verbose > 0:\n",
    "        total_obs_seqs = sum(len(obs_list) for obs_list in observations.values())\n",
    "        total_utt_seqs = total_obs_seqs * n_utt_seq\n",
    "        \n",
    "        print(f\"Utterance generation configuration:\")\n",
    "        print(f\"  Thetas: {thetas}\")\n",
    "        print(f\"  {len(thetas)} thetas × {obs_config['n_obs_seq']} obs_seq × {n_utt_seq} utt_seq \"\n",
    "              f\"= {total_utt_seqs} total utterance sequences\")\n",
    "        print(f\"  Sequence length T = {obs_config['T']}\")\n",
    "        print(f\"  Speaker: {speaker_type}\", end=\"\")\n",
    "        if speaker_type == \"pragmatic\":\n",
    "            print(f\" (omega={speaker_config['omega']}, psi={speaker_config['psi']}, \"\n",
    "                  f\"alpha={speaker_config['alpha']})\")\n",
    "        else:\n",
    "            print()\n",
    "        \n",
    "        if is_parallel:\n",
    "            print(f\"  Parallel execution: {n_workers} workers, backend='{backend}'\")\n",
    "        else:\n",
    "            print(f\"  Sequential execution\")\n",
    "        print()\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # PREPARE FULL SPEAKER CONFIG\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    full_speaker_config = {\n",
    "        \"speaker_type\": speaker_type,\n",
    "        \"initial_beliefs_theta\": speaker_config.get(\"initial_beliefs_theta\", None)\n",
    "    }\n",
    "    \n",
    "    if speaker_type == \"pragmatic\":\n",
    "        full_speaker_config.update({\n",
    "            \"omega\": speaker_config[\"omega\"],\n",
    "            \"psi\": speaker_config[\"psi\"],\n",
    "            \"alpha\": speaker_config[\"alpha\"],\n",
    "            \"update_internal\": speaker_config[\"update_internal\"],\n",
    "            \"beta\": speaker_config.get(\"beta\", 0.0)\n",
    "        })\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # BUILD FLAT TASK LIST (aligned with multiT approach)\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    tasks = []\n",
    "    task_idx = 0\n",
    "    \n",
    "    for theta in thetas:\n",
    "        for obs_info in observations[theta]:\n",
    "            tasks.append({\n",
    "                \"theta\": theta,\n",
    "                \"task_idx\": task_idx,\n",
    "                \"obs_idx\": obs_info[\"obs_idx\"],\n",
    "                \"obs_seq\": obs_info[\"obs_seq\"],\n",
    "                \"obs_run_seed\": obs_info[\"obs_run_seed\"],\n",
    "                \"obs_log_lik_true_theta\": obs_info[\"log_lik_true_theta\"],\n",
    "                \"obs_log_lik_all_theta\": obs_info[\"log_lik_all_theta\"], \n",
    "                \"obs_mle_theta\": obs_info[\"mle_theta\"]\n",
    "            })\n",
    "            task_idx += 1\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # EXECUTE TASKS (PARALLEL OR SEQUENTIAL)\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    def run_task(task):\n",
    "        return _process_single_obs_seq(\n",
    "            theta=task[\"theta\"],\n",
    "            task_idx=task[\"task_idx\"],\n",
    "            obs_idx=task[\"obs_idx\"],\n",
    "            world=world,\n",
    "            obs_seq=task[\"obs_seq\"],\n",
    "            obs_run_seed=task[\"obs_run_seed\"],\n",
    "            n_utt_seq=n_utt_seq,\n",
    "            speaker_config=full_speaker_config,\n",
    "            speaker_type=speaker_type,\n",
    "            random_seed=random_seed,\n",
    "            obs_log_lik_true_theta=task[\"obs_log_lik_true_theta\"],\n",
    "            obs_log_lik_all_theta=task[\"obs_log_lik_all_theta\"],\n",
    "            obs_mle_theta=task[\"obs_mle_theta\"],\n",
    "            compute_obs_likelihood=compute_obs_likelihood\n",
    "        )\n",
    "    \n",
    "    if is_parallel:\n",
    "        results = Parallel(n_jobs=n_workers, backend=backend, verbose=verbose)(\n",
    "            delayed(run_task)(task) for task in tasks\n",
    "        )\n",
    "    else:\n",
    "        results = [run_task(task) for task in tasks]\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # AGGREGATE RESULTS\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    data = {theta: {\"obs_sequences\": []} for theta in thetas}\n",
    "    all_flat_records = []\n",
    "    \n",
    "    for task, (obs_seq_data, flat_records) in zip(tasks, results):\n",
    "        theta = task[\"theta\"]\n",
    "        data[theta][\"obs_sequences\"].append(obs_seq_data)\n",
    "        all_flat_records.extend(flat_records)\n",
    "    \n",
    "    # Sort obs_sequences by obs_idx for consistent ordering\n",
    "    for theta in thetas:\n",
    "        data[theta][\"obs_sequences\"].sort(key=lambda x: x[\"obs_idx\"])\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # PREPARE OUTPUT\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    if return_format == \"dataframe\":\n",
    "        df = pd.DataFrame(all_flat_records)\n",
    "        df = df.sort_values([\"theta\", \"obs_idx\", \"utt_idx\"]).reset_index(drop=True)\n",
    "        \n",
    "        # Define column order\n",
    "        base_cols = [\n",
    "            \"theta\",\n",
    "            \"obs_idx\", \n",
    "            \"utt_idx\", \n",
    "            \"obs_seq\",\n",
    "            \"utt_seq\",\n",
    "            \"obs_run_seed\",\n",
    "            \"utt_seed\",\n",
    "            \"log_lik_true_speaker\" \n",
    "        ]\n",
    "        \n",
    "        if compute_obs_likelihood == \"none\":\n",
    "            col_order = base_cols\n",
    "        \n",
    "        elif compute_obs_likelihood == \"true\":\n",
    "            col_order = base_cols + [\"log_lik_true_theta\"] \n",
    "        \n",
    "        elif compute_obs_likelihood == \"all\":\n",
    "            likelihood_cols = [\n",
    "                f\"log_lik_theta_{str(tv).replace('.', 'p')}\" \n",
    "                for tv in world.theta_values\n",
    "            ]\n",
    "            col_order = (\n",
    "                base_cols +\n",
    "                [\"log_lik_true_theta\"] +\n",
    "                likelihood_cols +\n",
    "                [\"mle_theta\"]\n",
    "            )\n",
    "        \n",
    "        # Only include columns that exist\n",
    "        col_order = [c for c in col_order if c in df.columns]\n",
    "        \n",
    "        return df[col_order]\n",
    "    \n",
    "    else:  # return_format == \"nested\"\n",
    "        return {\n",
    "            \"obs_data_config\": obs_config,\n",
    "            \"speaker_config\": speaker_config,\n",
    "            \"n_utt_seq\": n_utt_seq,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"data\": data\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def _process_single_obs_seq(\n",
    "    theta: float,\n",
    "    task_idx: int,\n",
    "    obs_idx: int,\n",
    "    world: World,\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    obs_run_seed: int,\n",
    "    n_utt_seq: int,\n",
    "    speaker_config: Dict[str, Any],\n",
    "    speaker_type: str,\n",
    "    random_seed: Optional[int],\n",
    "    obs_log_lik_true_theta: Optional[float],\n",
    "    obs_log_lik_all_theta: Optional[np.ndarray],\n",
    "    obs_mle_theta: Optional[float],\n",
    "    compute_obs_likelihood: str\n",
    ") -> Tuple[Dict[str, Any], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Process a single observation sequence: generate all utterance sequences.\n",
    "    \n",
    "    This function is designed to be called in parallel via joblib.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    theta : float\n",
    "        True theta value for this observation sequence.\n",
    "    task_idx : int\n",
    "        Global task index (used for seed computation, replaces theta_idx).\n",
    "    obs_idx : int\n",
    "        Index of this observation sequence within its theta group.\n",
    "    world : World\n",
    "        The World object.\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        The observation sequence.\n",
    "    obs_run_seed : int\n",
    "        Seed used to sample this observation sequence (aligned name).\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences to generate.\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration dictionary.\n",
    "    speaker_type : str\n",
    "        \"literal\" or \"pragmatic\".\n",
    "    random_seed : Optional[int]\n",
    "        Base random seed for utterance generation.\n",
    "        Actual seed: random_seed + task_idx * 10_000 (aligned with multiT)\n",
    "    obs_log_lik_true_theta : Optional[float]\n",
    "        Pre-computed log P(obs_seq | true theta) (aligned name).\n",
    "    obs_log_lik_all_theta : Optional[np.ndarray]\n",
    "        Pre-computed log P(obs_seq | theta) for all theta values (aligned name).\n",
    "    obs_mle_theta : Optional[float]\n",
    "        Pre-computed MLE theta.\n",
    "    compute_obs_likelihood : str\n",
    "        \"none\", \"true\", or \"all\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[Dict[str, Any], List[Dict[str, Any]]]\n",
    "        1. obs_seq_data: Nested data structure\n",
    "        2. flat_records: Flat records for DataFrame format\n",
    "    \"\"\"\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # INITIALIZE NESTED STRUCTURE (aligned field names)\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    obs_seq_data = {\n",
    "        \"obs_idx\": obs_idx, \n",
    "        \"obs_seq\": obs_seq,\n",
    "        \"obs_run_seed\": obs_run_seed, \n",
    "        \"theta\": theta,\n",
    "        \"utt_sequences\": []\n",
    "    }\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # ATTACH PRE-COMPUTED OBSERVATION LIKELIHOOD DATA (aligned field names)\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    if compute_obs_likelihood in [\"true\", \"all\"] and obs_log_lik_true_theta is not None:\n",
    "        obs_seq_data[\"log_lik_true_theta\"] = obs_log_lik_true_theta \n",
    "    \n",
    "    if compute_obs_likelihood == \"all\" and obs_log_lik_all_theta is not None:\n",
    "        obs_seq_data[\"log_lik_all_theta\"] = dict(  \n",
    "            zip(world.theta_values, obs_log_lik_all_theta)\n",
    "        )\n",
    "        obs_seq_data[\"mle_theta\"] = obs_mle_theta\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # COMPUTE BASE SEED \n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    base_utt_seed = None\n",
    "    if random_seed is not None:\n",
    "        base_utt_seed = random_seed + task_idx * 10_000\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # GENERATE UTTERANCE SEQUENCES\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    utt_records = _generate_utterances_for_obs_seq(\n",
    "        world=world,\n",
    "        obs_seq=obs_seq,\n",
    "        n_utt_seq=n_utt_seq,\n",
    "        speaker_config=speaker_config,\n",
    "        base_utt_seed=base_utt_seed,\n",
    "        speaker_type=speaker_type\n",
    "    )\n",
    "    \n",
    "    obs_seq_data[\"utt_sequences\"] = utt_records\n",
    "    \n",
    "    # ---------------------------------------------------------------------\n",
    "    # BUILD FLAT RECORDS FOR DATAFRAME FORMAT\n",
    "    # ---------------------------------------------------------------------\n",
    "    \n",
    "    flat_records = []\n",
    "    \n",
    "    for utt_data in utt_records:\n",
    "        # Base record with aligned field names\n",
    "        record = {\n",
    "            \"theta\": theta,\n",
    "            \"obs_idx\": obs_idx, \n",
    "            \"utt_idx\": utt_data[\"utt_idx\"], \n",
    "            \"obs_seq\": obs_seq,\n",
    "            \"utt_seq\": utt_data[\"utt_seq\"],\n",
    "            \"obs_run_seed\": obs_run_seed,  \n",
    "            \"utt_seed\": utt_data[\"utt_seed\"],\n",
    "            \"log_lik_true_speaker\": utt_data[\"log_lik_true_speaker\"] \n",
    "        }\n",
    "        \n",
    "        # Add observation log-likelihood at true theta \n",
    "        if compute_obs_likelihood in [\"true\", \"all\"] and obs_log_lik_true_theta is not None:\n",
    "            record[\"log_lik_true_theta\"] = obs_log_lik_true_theta \n",
    "        \n",
    "        # Add observation log-likelihoods for all theta values + MLE\n",
    "        if compute_obs_likelihood == \"all\" and obs_log_lik_all_theta is not None:\n",
    "            for t_idx, t_val in enumerate(world.theta_values):\n",
    "                col_name = f\"log_lik_theta_{str(t_val).replace('.', 'p')}\" \n",
    "                record[col_name] = float(obs_log_lik_all_theta[t_idx])\n",
    "            record[\"mle_theta\"] = obs_mle_theta\n",
    "        \n",
    "        flat_records.append(record)\n",
    "    \n",
    "    return obs_seq_data, flat_records\n",
    "\n",
    "\n",
    "\n",
    "def _generate_utterances_for_obs_seq(\n",
    "    world: World,\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    n_utt_seq: int,\n",
    "    speaker_config: Dict[str, Any],\n",
    "    base_utt_seed: Optional[int],\n",
    "    speaker_type: str\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate multiple utterance sequences for a single observation sequence.\n",
    "    \n",
    "    This is the core worker function that handles the actual utterance generation.\n",
    "    It creates a fresh speaker for each utterance sequence to ensure independence,\n",
    "    and tracks the log-likelihood of each generated sequence.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    world : World\n",
    "        The World object containing likelihoods and truth tables.\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        The observation sequence to generate utterances for.\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences to generate.\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration dictionary.\n",
    "    base_utt_seed : Optional[int]\n",
    "        Base seed for reproducibility. Each utterance sequence gets seed\n",
    "        (base_utt_seed + utt_idx). If None, no seeding is performed.\n",
    "    speaker_type : str\n",
    "        \"literal\" or \"pragmatic\".\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        List of n_utt_seq utterance sequence records, each containing:\n",
    "        - utt_idx: int (aligned with multiT)\n",
    "        - utt_seq: List[str]\n",
    "        - utt_seed: Optional[int]\n",
    "        - log_lik_true_speaker: float (aligned with multiT, single value)\n",
    "        - log_lik_all_speaker: None (placeholder for consistency)\n",
    "    \"\"\"\n",
    "    utt_records = []\n",
    "    \n",
    "    for utt_idx in range(n_utt_seq):\n",
    "        \n",
    "        # SEED MANAGEMENT\n",
    "        utt_seed = (base_utt_seed + utt_idx) if base_utt_seed is not None else None\n",
    "        \n",
    "        if utt_seed is not None:\n",
    "            np.random.seed(utt_seed)\n",
    "        \n",
    "        # CREATE FRESH SPEAKER INSTANCE\n",
    "        if speaker_type == \"literal\":\n",
    "            speaker = LiteralSpeaker(\n",
    "                world=world,\n",
    "                initial_beliefs_theta=speaker_config.get(\"initial_beliefs_theta\")\n",
    "            )\n",
    "        else:  # pragmatic\n",
    "            speaker = PragmaticSpeaker_obs(\n",
    "                world=world,\n",
    "                omega=speaker_config[\"omega\"],\n",
    "                psi=speaker_config[\"psi\"],\n",
    "                update_internal=speaker_config[\"update_internal\"],\n",
    "                alpha=speaker_config[\"alpha\"],\n",
    "                beta=speaker_config.get(\"beta\", 0.0),\n",
    "                initial_beliefs_theta=speaker_config.get(\"initial_beliefs_theta\")\n",
    "            )\n",
    "        \n",
    "        # GENERATE UTTERANCE SEQUENCE WITH LOG-LIKELIHOOD TRACKING\n",
    "        utt_seq = []\n",
    "        log_lik = 0.0\n",
    "        \n",
    "        for obs in obs_seq:\n",
    "            obs_key = tuple(obs) if not isinstance(obs, tuple) else obs\n",
    "            \n",
    "            # IMPORTANT: Capture log probabilities BEFORE speaking\n",
    "            log_probs_for_obs = speaker.utterance_log_prob_obs[obs_key].copy()\n",
    "            \n",
    "            # Generate utterance (may update speaker state)\n",
    "            utt = speaker.update_and_speak(obs)\n",
    "            utt_seq.append(utt)\n",
    "            \n",
    "            # Look up log probability\n",
    "            log_p = float(log_probs_for_obs.loc[utt])\n",
    "            \n",
    "            # Accumulate\n",
    "            if not np.isfinite(log_p):\n",
    "                log_lik = -np.inf\n",
    "            else:\n",
    "                log_lik += log_p\n",
    "        \n",
    "        # STORE RECORD \n",
    "        utt_records.append({\n",
    "            \"utt_idx\": utt_idx,\n",
    "            \"utt_seq\": utt_seq,\n",
    "            \"utt_seed\": utt_seed,\n",
    "            \"log_lik_true_speaker\": log_lik, \n",
    "            \"log_lik_all_speaker\": None \n",
    "        })\n",
    "    \n",
    "    return utt_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67278f0f-e871-46f5-a26e-46873c1e028e",
   "metadata": {},
   "source": [
    "## Multiple T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b32941-d908-4338-b648-cf44e1a481a0",
   "metadata": {},
   "source": [
    "### Observation Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab225c-d64c-4ec8-b507-ea4fb309f736",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_observation_sequences_multiT(\n",
    "    n: int,\n",
    "    m: int,\n",
    "    thetas: Union[List[float], np.ndarray],\n",
    "    Ts: Union[List[int], np.ndarray],\n",
    "    n_obs_seq: int,\n",
    "    random_seed: Optional[int] = None,\n",
    "    theta_values: Optional[np.ndarray] = None,\n",
    "    compute_obs_likelihood: str = \"none\"\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Sample observation sequences and compute likelihoods for multiple sequence lengths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    n : int\n",
    "        Number of independent experiments in the World.\n",
    "    m : int\n",
    "        Number of Bernoulli trials per experiment.\n",
    "    thetas : List[float] or np.ndarray\n",
    "        List of theta values to simulate. Each must be in the World's theta_values.\n",
    "    Ts : List[int] or np.ndarray\n",
    "        List of sequence lengths to compute likelihoods for.\n",
    "        Observations are sampled with length max(Ts).\n",
    "        Likelihoods are computed for each T using obs_seq[:T].\n",
    "    n_obs_seq : int\n",
    "        Number of observation sequences per theta.\n",
    "    random_seed : Optional[int], default None\n",
    "        Base random seed for reproducibility.\n",
    "        The same seed is used for all thetas (sequences differ due to different\n",
    "        theta parameters, not different seeds).\n",
    "    theta_values : Optional[np.ndarray], default None\n",
    "        Custom theta grid for the World. If None, uses World's default [0, 0.1, ..., 1].\n",
    "    compute_obs_likelihood : str, default \"none\"\n",
    "        Whether to compute observation sequence log-likelihoods:\n",
    "        - \"none\": Don't compute likelihoods\n",
    "        - \"true\": Compute log P(obs_seq[:T] | true_theta) for each T\n",
    "                  (optimized: only loads single column from likelihood table)\n",
    "        - \"all\": Compute log P(obs_seq[:T] | theta) for all thetas and each T,\n",
    "                 plus MLE theta for each T\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary containing:\n",
    "        \n",
    "        - \"world\": World\n",
    "            The World object (reusable for utterance generation)\n",
    "        \n",
    "        - \"config\": Dict\n",
    "            Configuration parameters used:\n",
    "            {\n",
    "                \"n\": int,\n",
    "                \"m\": int,\n",
    "                \"thetas\": List[float],\n",
    "                \"Ts\": List[int],\n",
    "                \"max_T\": int,\n",
    "                \"n_obs_seq\": int,\n",
    "                \"random_seed\": Optional[int],\n",
    "                \"compute_obs_likelihood\": str\n",
    "            }\n",
    "        \n",
    "        - \"observations\": Dict[float, List[Dict]]\n",
    "            Mapping from theta -> list of observation data.\n",
    "            Each observation data dict contains:\n",
    "            {\n",
    "                \"obs_idx\": int,\n",
    "                \"obs_seq\": List[Tuple[int, ...]],   # Full sequence (length max_T)\n",
    "                \"obs_run_seed\": int, # run_seed output from world.sample_multiple_runs()\n",
    "                \"theta\": float,\n",
    "                \"log_lik_true_theta\": Optional[Dict[int, float]],\n",
    "                    # {T: log P(obs_seq[:T] | true_theta)} for each T in Ts\n",
    "                    # None if compute_obs_likelihood == \"none\"\n",
    "                \"log_lik_all_theta\": Optional[Dict[int, np.ndarray]],\n",
    "                    # {T: array of log P(obs_seq[:T] | theta) for all thetas}\n",
    "                    # None if compute_obs_likelihood != \"all\"\n",
    "                \"mle_theta\": Optional[Dict[int, float]],\n",
    "                    # {T: argmax_theta P(obs_seq[:T] | theta)} for each T\n",
    "                    # None if compute_obs_likelihood != \"all\"\n",
    "                \"utterances\": None\n",
    "                    # Placeholder for utterance generation (to be filled by Stage 2)\n",
    "            }\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If parameters are invalid or theta values not in World's theta_values.\n",
    "    RuntimeError\n",
    "        If World creation or observation sampling fails.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Sample observations and compute likelihoods for T=5, 10, 15, 20\n",
    "    >>> obs_data = sample_observation_sequences_multiT(\n",
    "    ...     n=3, m=2,\n",
    "    ...     thetas=[0.3, 0.5, 0.7],\n",
    "    ...     Ts=[5, 10, 15, 20],\n",
    "    ...     n_obs_seq=50,\n",
    "    ...     random_seed=42,\n",
    "    ...     compute_obs_likelihood=\"all\"\n",
    "    ... )\n",
    "    \n",
    "    >>> # Access likelihood at T=10 for first observation of theta=0.5\n",
    "    >>> obs_info = obs_data[\"observations\"][0.5][0]\n",
    "    >>> log_lik_T10 = obs_info[\"log_lik_true_theta\"][10]\n",
    "    >>> mle_T10 = obs_info[\"mle_theta\"][10]\n",
    "    \n",
    "    >>> # Compare MLE accuracy across different T values\n",
    "    >>> for T in obs_data[\"config\"][\"Ts\"]:\n",
    "    ...     mle_errors = [\n",
    "    ...         abs(obs[\"mle_theta\"][T] - obs[\"theta\"])\n",
    "    ...         for obs in obs_data[\"observations\"][0.5]\n",
    "    ...     ]\n",
    "    ...     print(f\"T={T}: mean MLE error = {np.mean(mle_errors):.3f}\")\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # INPUT VALIDATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    if not isinstance(n, int) or n < 1:\n",
    "        raise ValueError(\"n must be a positive integer\")\n",
    "    if not isinstance(m, int) or m < 1:\n",
    "        raise ValueError(\"m must be a positive integer\")\n",
    "    if not isinstance(n_obs_seq, int) or n_obs_seq < 1:\n",
    "        raise ValueError(\"n_obs_seq must be a positive integer\")\n",
    "    if compute_obs_likelihood not in [\"none\", \"true\", \"all\"]:\n",
    "        raise ValueError(\"compute_obs_likelihood must be 'none', 'true', or 'all'\")\n",
    "    \n",
    "    # Process thetas to list\n",
    "    thetas = list(thetas) if isinstance(thetas, np.ndarray) else list(thetas)\n",
    "    if len(thetas) == 0:\n",
    "        raise ValueError(\"thetas cannot be empty\")\n",
    "    \n",
    "    # Process Ts to sorted list\n",
    "    Ts = list(Ts) if isinstance(Ts, np.ndarray) else list(Ts)\n",
    "    if len(Ts) == 0:\n",
    "        raise ValueError(\"Ts cannot be empty\")\n",
    "    \n",
    "    # Validate all Ts are positive integers\n",
    "    for T in Ts:\n",
    "        if not isinstance(T, (int, np.integer)) or T < 1:\n",
    "            raise ValueError(f\"All values in Ts must be positive integers, got {T}\")\n",
    "    \n",
    "    # Sort and deduplicate Ts\n",
    "    Ts = sorted(set(int(T) for T in Ts))\n",
    "    max_T = max(Ts)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CREATE WORLD\n",
    "    # =========================================================================\n",
    "    \n",
    "    try:\n",
    "        world = World(n=n, m=m, theta_values=theta_values)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to create World: {e}\")\n",
    "    \n",
    "    # Validate thetas against World's theta_values\n",
    "    for theta in thetas:\n",
    "        closest = world.theta_values[np.abs(world.theta_values - theta).argmin()]\n",
    "        if not np.isclose(theta, closest, rtol=1e-10, atol=1e-10):\n",
    "            raise ValueError(\n",
    "                f\"theta {theta} not in World's theta_values. \"\n",
    "                f\"Closest: {closest}. Available: {list(world.theta_values)}\"\n",
    "            )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SAMPLE OBSERVATIONS FOR EACH THETA\n",
    "    # =========================================================================\n",
    "    \n",
    "    observations = {}\n",
    "    n_theta_vals = len(world.theta_values)\n",
    "    \n",
    "    for theta in thetas:\n",
    "        \n",
    "        # Sample observation sequences of length max_T\n",
    "        try:\n",
    "            obs_df = world.sample_multiple_runs(\n",
    "                theta=theta,\n",
    "                n_run=n_obs_seq,\n",
    "                n_round=max_T,\n",
    "                base_seed=random_seed\n",
    "            )\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Failed to sample observations for theta={theta}: {e}\")\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # EXTRACT OBSERVATION SEQUENCES (VECTORIZED VIA GROUPBY)\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        out = (\n",
    "            obs_df\n",
    "            .sort_values([\"run_id\", \"round_index\"])\n",
    "            .groupby(\"run_id\", sort=True)\n",
    "            .agg(\n",
    "                obs_seq=(\"observation\", list), # Note: source column is \"observation\"\n",
    "                obs_run_seed=(\"run_seed\", \"first\")  # Note: source column is \"run_seed\"\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        obs_seqs = out[\"obs_seq\"].tolist()\n",
    "        obs_run_seeds = out[\"obs_run_seed\"].tolist()\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # BUILD OBSERVATION RECORDS\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        obs_list = [\n",
    "            {\n",
    "                \"obs_idx\": obs_idx,\n",
    "                \"obs_seq\": obs_seqs[obs_idx],\n",
    "                \"obs_run_seed\": obs_run_seeds[obs_idx],\n",
    "                \"theta\": theta,\n",
    "                \"log_lik_true_theta\": None,\n",
    "                \"log_lik_all_theta\": None,\n",
    "                \"mle_theta\": None,\n",
    "                \"utterances\": {}\n",
    "            }\n",
    "            for obs_idx in range(n_obs_seq)\n",
    "        ]\n",
    "        \n",
    "        # -----------------------------------------------------------------\n",
    "        # COMPUTE OBSERVATION LIKELIHOODS FOR ALL Ts\n",
    "        # -----------------------------------------------------------------\n",
    "        \n",
    "        if compute_obs_likelihood != \"none\":\n",
    "            \n",
    "            # Flatten all observations for batch lookup\n",
    "            # obs_seqs is List[List[Tuple]], flatten to List[Tuple]\n",
    "            # Shape after flatten: (n_obs_seq * max_T,)\n",
    "            all_obs_flat = [obs for seq in obs_seqs for obs in seq]\n",
    "            all_obs_keys = [tuple(obs) if not isinstance(obs, tuple) else obs \n",
    "                          for obs in all_obs_flat]\n",
    "            \n",
    "            # Find column index for true theta (needed for both modes)\n",
    "            theta_col_idx = np.where(np.isclose(world.theta_values, theta))[0][0]\n",
    "            true_theta_val = world.theta_values[theta_col_idx]\n",
    "            \n",
    "            if compute_obs_likelihood == \"true\":\n",
    "                \n",
    "                # Select ONLY the column for true theta\n",
    "                # Shape: (n_obs_seq * max_T,)\n",
    "                log_probs_flat_true = world.obs_log_likelihood_theta.loc[\n",
    "                    all_obs_keys, true_theta_val\n",
    "                ].values\n",
    "                \n",
    "                # Reshape to (n_obs_seq, max_T)\n",
    "                log_probs_2d = log_probs_flat_true.reshape(n_obs_seq, max_T)\n",
    "                \n",
    "                # Cumulative sum over T dimension (axis=1)\n",
    "                # cumsum_log_probs[i, t] = log P(O_0, ..., O_t | true_theta)\n",
    "                # Shape: (n_obs_seq, max_T)\n",
    "                cumsum_log_probs_true = np.cumsum(log_probs_2d, axis=1)\n",
    "                \n",
    "                # Extract likelihoods for each T (vectorized)\n",
    "                # T is 1-indexed, so T-1 gives 0-based array index\n",
    "                log_lik_true_all = {\n",
    "                    T: cumsum_log_probs_true[:, T - 1] for T in Ts\n",
    "                }\n",
    "                \n",
    "                # Distribute results to observation records\n",
    "                for i in range(n_obs_seq):\n",
    "                    obs_list[i][\"log_lik_true_theta\"] = {\n",
    "                        T: float(log_lik_true_all[T][i]) for T in Ts\n",
    "                    }\n",
    "                    # log_lik_all_theta and mle_theta remain None\n",
    "            \n",
    "            else:  # compute_obs_likelihood == \"all\"\n",
    "                \n",
    "                # Load full matrix\n",
    "                # Shape: (n_obs_seq * max_T, n_theta_vals)\n",
    "                log_probs_flat = world.obs_log_likelihood_theta.loc[all_obs_keys].values\n",
    "                \n",
    "                # Reshape to (n_obs_seq, max_T, n_theta_vals)\n",
    "                log_probs_3d = log_probs_flat.reshape(n_obs_seq, max_T, n_theta_vals)\n",
    "                \n",
    "                # Cumulative sum over T dimension (axis=1)\n",
    "                # cumsum_log_probs[i, t, :] = log P(O_0, ..., O_t | all thetas)\n",
    "                # Shape: (n_obs_seq, max_T, n_theta_vals)\n",
    "                cumsum_log_probs = np.cumsum(log_probs_3d, axis=1)\n",
    "                \n",
    "                # Storage for vectorized results\n",
    "                log_lik_true_all = {}   # {T: shape (n_obs_seq,)}\n",
    "                log_lik_all_all = {}    # {T: shape (n_obs_seq, n_theta_vals)}\n",
    "                mle_all = {}            # {T: shape (n_obs_seq,)}\n",
    "                \n",
    "                for T in Ts:\n",
    "                    # Extract likelihoods at position T-1 for all obs_seqs\n",
    "                    # Shape: (n_obs_seq, n_theta_vals)\n",
    "                    log_liks_at_T = cumsum_log_probs[:, T - 1, :]\n",
    "                    \n",
    "                    # True theta likelihood\n",
    "                    log_lik_true_all[T] = log_liks_at_T[:, theta_col_idx]\n",
    "                    \n",
    "                    # Full likelihood array\n",
    "                    log_lik_all_all[T] = log_liks_at_T\n",
    "                    \n",
    "                    # MLE theta: argmax across theta dimension\n",
    "                    mle_indices = np.argmax(log_liks_at_T, axis=1)\n",
    "                    mle_all[T] = world.theta_values[mle_indices]\n",
    "                \n",
    "                # Distribute results to observation records\n",
    "                for i in range(n_obs_seq):\n",
    "                    obs_list[i][\"log_lik_true_theta\"] = {\n",
    "                        T: float(log_lik_true_all[T][i]) for T in Ts\n",
    "                    }\n",
    "                    obs_list[i][\"log_lik_all_theta\"] = {\n",
    "                        T: log_lik_all_all[T][i].copy() for T in Ts\n",
    "                    }\n",
    "                    obs_list[i][\"mle_theta\"] = {\n",
    "                        T: float(mle_all[T][i]) for T in Ts\n",
    "                    }\n",
    "        \n",
    "        # Store observations for this theta\n",
    "        observations[theta] = obs_list\n",
    "    \n",
    "    # =========================================================================\n",
    "    # RETURN RESULT\n",
    "    # =========================================================================\n",
    "    \n",
    "    return {\n",
    "        \"world\": world,\n",
    "        \"config\": {\n",
    "            \"n\": n,\n",
    "            \"m\": m,\n",
    "            \"thetas\": thetas,\n",
    "            \"Ts\": Ts,\n",
    "            \"max_T\": max_T,\n",
    "            \"n_obs_seq\": n_obs_seq,\n",
    "            \"random_seed\": random_seed,\n",
    "            \"compute_obs_likelihood\": compute_obs_likelihood\n",
    "        },\n",
    "        \"observations\": observations\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d908df-04cc-42ab-a350-5d02d9cf387b",
   "metadata": {},
   "source": [
    "### Utterances Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80461549-e233-4f82-823b-aa69d57e53fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_utterances_for_observations_multiT(\n",
    "    obs_data: Dict[str, Any],\n",
    "    speaker_config: Dict[str, Any],\n",
    "    n_utt_seq: int,\n",
    "    n_jobs: int = 1,\n",
    "    backend: str = \"loky\",\n",
    "    verbose: int = 0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Generate utterance sequences for pre-sampled observations (in-place).\n",
    "    \n",
    "    Mutates obs_data by filling in the 'utterances' field for each observation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_data : Dict[str, Any]\n",
    "        Output from sample_observation_sequences_multiT(). Will be mutated.\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration (see _generate_utterances_for_single_obs_seq_multiT).\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences per observation sequence.\n",
    "        Must be <= 10000 to avoid seed collisions.\n",
    "    n_jobs : int, default 1\n",
    "        Number of parallel jobs (-1 for all cores).\n",
    "    backend : str, default \"loky\"\n",
    "        Joblib backend.\n",
    "    verbose : int, default 0\n",
    "        Verbosity level.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Mutates obs_data IN PLACE.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Storage structure:\n",
    "        obs_data[\"observations\"][theta][obs_idx][\"utterances\"][speaker_key][alpha_key]\n",
    "        \n",
    "    Speaker keys: \"literal\", \"inf_T\", \"inf_F\", \"persp_T\", \"persp_F\", \"persm_T\", \"persm_F\"\n",
    "    Alpha keys: 0.0 (literal), float (pragmatic), or \"determ\"\n",
    "\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If utterances already exist for the given speaker/alpha combination.\n",
    "    \"\"\"\n",
    "    \n",
    "    # INPUT VALIDATION\n",
    "    \n",
    "    if not isinstance(n_utt_seq, int) or n_utt_seq < 1:\n",
    "        raise ValueError(\"n_utt_seq must be a positive integer\")\n",
    "    \n",
    "    # EXTRACT FROM obs_data\n",
    "    \n",
    "    world = obs_data[\"world\"]\n",
    "    config = obs_data[\"config\"]\n",
    "    Ts = config[\"Ts\"]\n",
    "    thetas = config[\"thetas\"]\n",
    "    random_seed = config[\"random_seed\"]\n",
    "    \n",
    "    # DETERMINE SPEAKER KEY AND ALPHA KEY\n",
    "    \n",
    "    speaker_type = speaker_config[\"speaker_type\"]\n",
    "    \n",
    "    if speaker_type == \"literal\":\n",
    "        speaker_key = \"literal\"\n",
    "        alpha_key = 0.0\n",
    "    else:\n",
    "        psi = speaker_config[\"psi\"]\n",
    "        update_internal = speaker_config[\"update_internal\"]\n",
    "        alpha = speaker_config[\"alpha\"]\n",
    "        \n",
    "        psi_prefix = {\"inf\": \"inf\", \"pers+\": \"persp\", \"pers-\": \"persm\"}[psi]\n",
    "        speaker_key = f\"{psi_prefix}_{'T' if update_internal else 'F'}\"\n",
    "        alpha_key = alpha if alpha == \"determ\" else float(alpha)\n",
    "\n",
    "    # CHECK FOR DUPLICATE GENERATION\n",
    "    \n",
    "    # Check first observation (all will have same structure)\n",
    "    first_obs = obs_data[\"observations\"][thetas[0]][0]\n",
    "    if (first_obs[\"utterances\"] is not None and\n",
    "        speaker_key in first_obs[\"utterances\"] and\n",
    "        alpha_key in first_obs[\"utterances\"][speaker_key]):\n",
    "        \n",
    "        delete_code = (\n",
    "            f\"for theta in obs_data['observations']:\\n\"\n",
    "            f\"    for obs in obs_data['observations'][theta]:\\n\"\n",
    "            f\"        if obs['utterances'] and '{speaker_key}' in obs['utterances']:\\n\"\n",
    "            f\"            obs['utterances']['{speaker_key}'].pop({alpha_key!r}, None)\"\n",
    "        )\n",
    "        \n",
    "        raise ValueError(\n",
    "            f\"Utterances already exist for speaker_key='{speaker_key}', alpha_key={alpha_key!r}.\\n\"\n",
    "            f\"To regenerate, first delete existing entries:\\n\\n{delete_code}\"\n",
    "        )\n",
    "    \n",
    "    # BUILD TASK LIST\n",
    "    \n",
    "    tasks = []\n",
    "    for theta in thetas:\n",
    "        for obs_info in obs_data[\"observations\"][theta]:\n",
    "            tasks.append({\n",
    "                \"theta\": theta,\n",
    "                \"obs_idx\": obs_info[\"obs_idx\"],\n",
    "                \"obs_seq\": obs_info[\"obs_seq\"]\n",
    "            })\n",
    "    \n",
    "    # Assign seeds based on task index\n",
    "    for task_idx, task in enumerate(tasks):\n",
    "        task[\"base_seed\"] = (random_seed + task_idx * 10_000 \n",
    "                            if random_seed is not None else None)\n",
    "    \n",
    "    # VERBOSE OUTPUT\n",
    "    \n",
    "    if verbose > 0:\n",
    "        n_workers = (cpu_count() if n_jobs == -1 \n",
    "                    else max(1, cpu_count() + 1 + n_jobs) if n_jobs < 0 \n",
    "                    else max(1, n_jobs))\n",
    "        print(f\"Generating utterances: {len(tasks)} obs_seq × {n_utt_seq} utt_seq\")\n",
    "        print(f\"  Speaker: {speaker_key}, alpha: {alpha_key}, Ts: {Ts}\")\n",
    "        print(f\"  Workers: {n_workers}\")\n",
    "    \n",
    "    # EXECUTE TASKS\n",
    "    \n",
    "    def run_task(task):\n",
    "        return _generate_utterances_for_single_obs_seq_multiT(\n",
    "            obs_seq=task[\"obs_seq\"],\n",
    "            world=world,\n",
    "            speaker_config=speaker_config,\n",
    "            n_utt_seq=n_utt_seq,\n",
    "            Ts=Ts,\n",
    "            base_seed=task[\"base_seed\"]\n",
    "        )\n",
    "    \n",
    "    if n_jobs == 1:\n",
    "        results = [run_task(task) for task in tasks]\n",
    "    else:\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend, verbose=verbose)(\n",
    "            delayed(run_task)(task) for task in tasks\n",
    "        )\n",
    "    \n",
    "    # STORE RESULTS (in-place mutation of obs_data)\n",
    "    \n",
    "    for task, utt_records in zip(tasks, results):\n",
    "        theta = task[\"theta\"]\n",
    "        obs_idx = task[\"obs_idx\"]\n",
    "        \n",
    "        obs_dict = obs_data[\"observations\"][theta][obs_idx]\n",
    "        \n",
    "        # Handle explicit None (from sample_observation_sequences_multiT)\n",
    "        if obs_dict[\"utterances\"] is None:\n",
    "            obs_dict[\"utterances\"] = {}\n",
    "        \n",
    "        obs_dict[\"utterances\"].setdefault(speaker_key, {})[alpha_key] = utt_records\n",
    "\n",
    "\n",
    "\n",
    "def _generate_utterances_for_single_obs_seq_multiT(\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    world: World,\n",
    "    speaker_config: Dict[str, Any],\n",
    "    n_utt_seq: int,\n",
    "    Ts: List[int],\n",
    "    base_seed: Optional[int]\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Generate utterance sequences for a single observation sequence.\n",
    "    \n",
    "    This is the core worker function for utterance generation. It creates a \n",
    "    fresh speaker for each utterance sequence and computes cumulative \n",
    "    log-likelihoods for each T in Ts.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        The observation sequence (length max_T).\n",
    "    world : World\n",
    "        The World object (used to create speaker instances).\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Complete speaker configuration containing:\n",
    "        - speaker_type: \"literal\" or \"pragmatic\"\n",
    "        For pragmatic speakers:\n",
    "        - omega: \"coop\" or \"strat\"\n",
    "        - psi: \"inf\", \"pers+\", or \"pers-\"\n",
    "        - alpha: float or \"determ\"\n",
    "        - update_internal: bool\n",
    "        - beta: float (default 0.0)\n",
    "        - initial_beliefs_theta: Optional[np.ndarray]\n",
    "    n_utt_seq : int\n",
    "        Number of utterance sequences to generate.\n",
    "    Ts : List[int]\n",
    "        List of sequence lengths to compute log-likelihoods for.\n",
    "        All values must satisfy 1 <= T <= len(obs_seq).\n",
    "    base_seed : Optional[int]\n",
    "        Base seed for reproducibility. Each utterance sequence uses\n",
    "        seed = base_seed + utt_idx. If None, no seeding.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    List[Dict[str, Any]]\n",
    "        List of n_utt_seq records, each containing:\n",
    "        - utt_idx: int\n",
    "            Index of this utterance sequence (0 to n_utt_seq-1)\n",
    "        - utt_seq: List[str]\n",
    "            The generated utterance sequence (same length as obs_seq)\n",
    "        - utt_seed: Optional[int]\n",
    "            The random seed used (for reproducibility)\n",
    "        - log_lik_true_speaker: Dict[int, float]\n",
    "            {T: log P(utt[:T] | obs[:T], speaker)} for each T in Ts\n",
    "        - log_lik_all_speaker: None\n",
    "            Placeholder for later analysis\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If any T in Ts is outside the valid range [1, len(obs_seq)].\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    - A fresh speaker is created for each utterance sequence to ensure\n",
    "      independence (speakers have internal state that evolves).\n",
    "    \n",
    "    - For pragmatic speakers with update_internal=True, the probability\n",
    "      table changes after each utterance. We capture log P(u_t | O_t)\n",
    "      BEFORE the update to correctly compute the likelihood.\n",
    "    \n",
    "    - Log-likelihood computation:\n",
    "      log P(utt[:T] | obs[:T]) = sum_{t=0}^{T-1} log P(u_t | O_t, state_t)\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> world = World(n=3, m=2)\n",
    "    >>> obs_seq = [(1, 1, 1), (0, 2, 1), (0, 0, 3), (1, 2, 0)]\n",
    "    >>> config = {\n",
    "    ...     \"speaker_type\": \"pragmatic\",\n",
    "    ...     \"omega\": \"coop\",\n",
    "    ...     \"psi\": \"inf\",\n",
    "    ...     \"alpha\": 5.0,\n",
    "    ...     \"update_internal\": True,\n",
    "    ...     \"beta\": 0.0\n",
    "    ... }\n",
    "    >>> records = _generate_utterances_for_single_obs_seq_multiT(\n",
    "    ...     obs_seq=obs_seq,\n",
    "    ...     world=world,\n",
    "    ...     speaker_config=config,\n",
    "    ...     n_utt_seq=3,\n",
    "    ...     Ts=[2, 4],\n",
    "    ...     base_seed=42\n",
    "    ... )\n",
    "    >>> len(records)\n",
    "    3\n",
    "    >>> records[0][\"log_lik_true_speaker\"]\n",
    "    {2: -1.85, 4: -3.72}\n",
    "    \"\"\"\n",
    "    \n",
    "    # VALIDATE Ts AGAINST SEQUENCE LENGTH\n",
    "    \n",
    "    max_T = len(obs_seq)\n",
    "    \n",
    "    # Check both bounds: T < 1 causes wrong indexing (T-1 = -1 → last element)\n",
    "    #                    T > max_T causes IndexError\n",
    "    invalid_Ts = [T for T in Ts if T < 1 or T > max_T]\n",
    "    if invalid_Ts:\n",
    "        raise ValueError(\n",
    "            f\"All T in Ts must satisfy 1 <= T <= len(obs_seq) ({max_T}). \"\n",
    "            f\"Invalid values: {invalid_Ts}\"\n",
    "        )\n",
    "    \n",
    "    # GENERATE UTTERANCES\n",
    "    \n",
    "    utt_records = []\n",
    "    is_literal = speaker_config[\"speaker_type\"] == \"literal\"\n",
    "    \n",
    "    for utt_idx in range(n_utt_seq):\n",
    "        \n",
    "        # SEED MANAGEMENT\n",
    "        utt_seed = (base_seed + utt_idx) if base_seed is not None else None\n",
    "        \n",
    "        if utt_seed is not None:\n",
    "            np.random.seed(utt_seed)\n",
    "        \n",
    "        # CREATE FRESH SPEAKER\n",
    "        if is_literal:\n",
    "            speaker = LiteralSpeaker(\n",
    "                world=world,\n",
    "                initial_beliefs_theta=speaker_config.get(\"initial_beliefs_theta\")\n",
    "            )\n",
    "        else:\n",
    "            speaker = PragmaticSpeaker_obs(\n",
    "                world=world,\n",
    "                omega=speaker_config[\"omega\"],\n",
    "                psi=speaker_config[\"psi\"],\n",
    "                update_internal=speaker_config[\"update_internal\"],\n",
    "                alpha=speaker_config[\"alpha\"],\n",
    "                beta=speaker_config.get(\"beta\", 0.0),\n",
    "                initial_beliefs_theta=speaker_config.get(\"initial_beliefs_theta\")\n",
    "            )\n",
    "        \n",
    "        # GENERATE UTTERANCES WITH PER-STEP LOG PROBABILITIES\n",
    "        utt_seq = []\n",
    "        log_probs_per_step = []\n",
    "        \n",
    "        for obs in obs_seq:\n",
    "            obs_key = tuple(obs) if not isinstance(obs, tuple) else obs\n",
    "            \n",
    "            # Capture log probs BEFORE speaking (for update_internal=True)\n",
    "            log_probs_for_obs = speaker.utterance_log_prob_obs[obs_key].copy()\n",
    "            \n",
    "            # Generate utterance\n",
    "            utt = speaker.update_and_speak(obs)\n",
    "            utt_seq.append(utt)\n",
    "            \n",
    "            # Look up log probability of chosen utterance\n",
    "            log_p = float(log_probs_for_obs.loc[utt])\n",
    "            \n",
    "            # Handle impossible utterances\n",
    "            if not np.isfinite(log_p):\n",
    "                log_p = -np.inf\n",
    "            \n",
    "            log_probs_per_step.append(log_p)\n",
    "        \n",
    "        # COMPUTE CUMULATIVE LOG-LIKELIHOODS FOR EACH T\n",
    "        log_probs_array = np.array(log_probs_per_step)\n",
    "        cumsum_log_probs = np.cumsum(log_probs_array)\n",
    "        \n",
    "        # Extract for each T (T is 1-indexed, array is 0-indexed)\n",
    "        # Note: Ts validation above ensures T >= 1, so T-1 >= 0\n",
    "        log_lik_true_speaker = {\n",
    "            T: float(cumsum_log_probs[T - 1])\n",
    "            for T in Ts\n",
    "        }\n",
    "        \n",
    "        # STORE RECORD\n",
    "        utt_records.append({\n",
    "            \"utt_idx\": utt_idx,\n",
    "            \"utt_seq\": utt_seq,\n",
    "            \"utt_seed\": utt_seed,\n",
    "            \"log_lik_true_speaker\": log_lik_true_speaker,\n",
    "            \"log_lik_all_speaker\": None\n",
    "        })\n",
    "    \n",
    "    return utt_records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67caf58-6cf6-48ce-85bb-622144ad9a36",
   "metadata": {},
   "source": [
    "# Fitting speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a879f-8791-47cf-85ad-30aba3b2b434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import itertools\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from typing import List, Dict, Union, Optional, Tuple, TypeVar, Iterator, Callable, Any, Literal\n",
    "from joblib import Parallel, delayed, cpu_count\n",
    "from scipy.special import logsumexp\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from rsa_optimal_exp_core import (\n",
    "    World, LiteralSpeaker, PragmaticSpeaker_obs, USE_PRECISE_LOGSPACE\n",
    ")\n",
    "\n",
    "np.seterr(divide='ignore', under='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d50d4d-9f99-48e7-be67-d3d6ef821f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rsa_optimal_exp_sampling_fun import (\n",
    "    sample_observation_sequences_multiT, generate_utterances_for_observations_multiT\n",
    ")\n",
    "\n",
    "TRUE_ALPHA = 4.0\n",
    "TRUE_SPEAKER_CONFIGS = [\n",
    "    {\n",
    "        \"speaker_type\": \"literal\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": TRUE_ALPHA,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632fbf6-ffa4-4509-a800-40e58f0a3e3b",
   "metadata": {},
   "source": [
    "## Single observation and utterance fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7f54c1-431b-4fbd-9888-acb40f708947",
   "metadata": {},
   "source": [
    "### Utterance Sequence Likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2da4d2a-d368-4bc8-8ebe-3bd047a314fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_utt_seq(\n",
    "    world: World,\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    utt_seq: List[str],\n",
    "    speaker_config: Dict[str, Any]\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Compute log p(utt_seq | obs_seq, speaker_config).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    world : World\n",
    "        The World object defining the communication game.\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        Sequence of observations O_1, ..., O_T.\n",
    "    utt_seq : List[str]\n",
    "        Sequence of utterances u_1, ..., u_T.\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration with keys:\n",
    "        - speaker_type : str\n",
    "            \"literal\" or \"pragmatic\" (required)\n",
    "        - omega : str\n",
    "            \"coop\" or \"strat\" (pragmatic only, required)\n",
    "        - psi : str\n",
    "            \"inf\", \"pers+\", or \"pers-\" (pragmatic only, required)\n",
    "        - alpha : float or str\n",
    "            Softmax temperature or \"determ\" (pragmatic only, required)\n",
    "        - update_internal : bool\n",
    "            Whether to update internal listener (pragmatic only, required)\n",
    "        - beta : float or None\n",
    "            Weight on informativeness vs persuasiveness (pragmatic only, default 0.0)\n",
    "            beta=0: pure persuasion, beta=1: pure informativeness\n",
    "            Only used when psi is \"pers+\" or \"pers-\"\n",
    "        - initial_beliefs_theta : np.ndarray or None\n",
    "            Initial prior over theta (default None = uniform)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Log-likelihood of the utterance sequence under the speaker model.\n",
    "    \"\"\"\n",
    "    if len(obs_seq) == 0:\n",
    "        raise ValueError(\"obs_seq is empty.\")\n",
    "        \n",
    "    if len(obs_seq) != len(utt_seq):\n",
    "        raise ValueError(\"obs_seq and utt_seq must have the same length.\")\n",
    "\n",
    "    # Extract config\n",
    "    speaker_type = speaker_config[\"speaker_type\"]\n",
    "    initial_beliefs = speaker_config.get(\"initial_beliefs_theta\")  # default None\n",
    "\n",
    "    # Initialize speaker based on type\n",
    "    if speaker_type == \"literal\":\n",
    "        speaker = LiteralSpeaker(world, initial_beliefs)\n",
    "        update_internal = False  # Literal speaker P(u|O) is static\n",
    "        \n",
    "    elif speaker_type == \"pragmatic\":\n",
    "        # Required parameters (no defaults in original class)\n",
    "        omega = speaker_config[\"omega\"]\n",
    "        psi = speaker_config[\"psi\"]\n",
    "        alpha = speaker_config[\"alpha\"]\n",
    "        update_internal_cfg = speaker_config[\"update_internal\"]\n",
    "        \n",
    "        # Optional parameter (has default in original class)\n",
    "        beta = speaker_config.get(\"beta\")\n",
    "        \n",
    "        speaker = PragmaticSpeaker_obs(\n",
    "            world=world,\n",
    "            omega=omega,\n",
    "            psi=psi,\n",
    "            update_internal=update_internal_cfg,\n",
    "            alpha=alpha,\n",
    "            beta=beta if beta is not None else 0.0,\n",
    "            initial_beliefs_theta=initial_beliefs\n",
    "        )\n",
    "        update_internal = speaker.update_internal\n",
    "    else:\n",
    "        raise ValueError(f\"speaker_type must be 'literal' or 'pragmatic', got '{speaker_type}'\")\n",
    "\n",
    "    # Compute log-likelihood\n",
    "    log_lik = 0.0\n",
    "    \n",
    "    for obs, utt in zip(obs_seq, utt_seq):\n",
    "        obs_key = tuple(obs) if not isinstance(obs, tuple) else obs\n",
    "        \n",
    "        # Validate\n",
    "        if obs_key not in world.observations:\n",
    "            raise ValueError(f\"Observation {obs_key} not supported by the world.\")\n",
    "        if utt not in world.utterances:\n",
    "            raise ValueError(f\"Utterance '{utt}' not in world.utterances\")\n",
    "\n",
    "        # Get log P(u | O)\n",
    "        log_p = speaker.utterance_log_prob_obs.at[utt, obs_key]\n",
    "        \n",
    "        if not np.isfinite(log_p):\n",
    "            return -np.inf\n",
    "        \n",
    "        log_lik += float(log_p)\n",
    "\n",
    "        # Update state for pragmatic speaker if configured\n",
    "        if speaker_type == \"pragmatic\" and update_internal:\n",
    "            speaker.literal_listener.listen_and_update(utt)\n",
    "            speaker.utterance_log_prob_obs = speaker._compute_utterance_log_prob_obs(speaker.alpha)\n",
    "\n",
    "    return log_lik"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26961bbc-c413-41b5-92cd-df5b1b11992a",
   "metadata": {},
   "source": [
    "### Utterance Sequence Likelihood with optimized alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5851bdb3-30e0-4c15-8568-d0620bbeb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_alpha_opt_utt_seq(\n",
    "    world: 'World',\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    utt_seq: List[str],\n",
    "    speaker_config: Dict[str, Any],\n",
    "    alpha_bounds: Tuple[float, float] = (0.001, 50.0),\n",
    "    method: str = \"bounded\",\n",
    "    include_determ: bool = True,\n",
    "    grid_search: bool = False,\n",
    "    grid_points: int = 100,\n",
    "    grid_spacing: str = \"log\"  \n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Find the optimal alpha that maximizes log-likelihood of an utterance sequence.\n",
    "    \n",
    "    This function optimizes over the softmax parameter alpha to find\n",
    "    the value that makes the observed utterance sequence most probable under the\n",
    "    specified speaker model. \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    world : World\n",
    "        The World object defining the communication game.\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        Sequence of observations O_1, ..., O_T.\n",
    "    utt_seq : List[str]\n",
    "        Sequence of utterances u_1, ..., u_T.\n",
    "    speaker_config : Dict[str, Any]\n",
    "        Speaker configuration. Must include:\n",
    "        - speaker_type : str (\"literal\" or \"pragmatic\")\n",
    "        For pragmatic speakers, must also include:\n",
    "        - omega : str (\"coop\" or \"strat\")\n",
    "        - psi : str (\"inf\", \"pers+\", or \"pers-\")\n",
    "        - update_internal : bool\n",
    "        Optional:\n",
    "        - beta : float (default 0.0)\n",
    "        - initial_beliefs_theta : np.ndarray or None (default None)\n",
    "        Note: 'alpha' in speaker_config is ignored; it will be optimized.\n",
    "    alpha_bounds : Tuple[float, float], default (0.001, 50.0)\n",
    "        Lower and upper bounds for continuous alpha optimization.\n",
    "        Lower bound should be > 0 to avoid numerical issues.\n",
    "    method : str, default \"bounded\"\n",
    "        Optimization method passed to scipy.optimize.minimize_scalar.\n",
    "        \"bounded\" is recommended for bounded optimization.\n",
    "    include_determ : bool, default True\n",
    "        Whether to also evaluate alpha=\"determ\" (hard argmax) and compare\n",
    "        against the continuous optimum.\n",
    "    grid_search : bool, default False\n",
    "        If True, use grid search instead of scipy optimization.\n",
    "        Grid search is more robust but slower for fine-grained search.\n",
    "    grid_points : int, default 100\n",
    "        Number of alpha values to evaluate when grid_search=True.\n",
    "    grid_spacing : str, default \"log\"\n",
    "        Spacing method for grid search. Options:\n",
    "        - \"log\": Logarithmic spacing (recommended). Places more points at lower\n",
    "          alpha values where the likelihood function typically changes more rapidly.\n",
    "        - \"linear\": Linear spacing. Evenly spaced points across the range.\n",
    "        \n",
    "        Logarithmic spacing is generally preferred because the softmax function's\n",
    "        behavior changes roughly logarithmically with alpha.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict[str, Any]\n",
    "        Dictionary containing:\n",
    "        - optimal_alpha : float or str or None\n",
    "            The overall best alpha (could be float or \"determ\").\n",
    "            None for literal speakers.\n",
    "        - max_log_likelihood : float\n",
    "            Log-likelihood at the optimal alpha.\n",
    "        - continuous_optimal_alpha : float or None\n",
    "            Best alpha from continuous optimization only.\n",
    "        - continuous_max_log_likelihood : float or None\n",
    "            Log-likelihood at the continuous optimum.\n",
    "        - determ_log_likelihood : float or None\n",
    "            Log-likelihood when alpha=\"determ\" (if include_determ=True).\n",
    "        - optimization_result : scipy.optimize.OptimizeResult or None\n",
    "            Full scipy result object (if grid_search=False).\n",
    "        - grid_results : Dict or None\n",
    "            Grid search details (if grid_search=True), containing:\n",
    "            - alphas: array of tested alpha values\n",
    "            - log_likelihoods: array of corresponding log-likelihoods\n",
    "            - best_idx: index of best alpha\n",
    "            - grid_spacing: spacing method used\n",
    "        - message : str (only for literal speakers)\n",
    "            Explanation that alpha is not applicable.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If obs_seq is empty, lengths don't match, speaker_type is invalid,\n",
    "        or required pragmatic speaker keys are missing.\n",
    "    RuntimeError\n",
    "        If optimization fails unexpectedly.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Validation (add new parameter validation) ---\n",
    "    if grid_spacing not in {\"log\", \"linear\"}:\n",
    "        raise ValueError(\n",
    "            f\"grid_spacing must be 'log' or 'linear', got '{grid_spacing}'\"\n",
    "        )\n",
    "    \n",
    "    # --- [Previous validation code unchanged] ---\n",
    "    if len(obs_seq) == 0:\n",
    "        raise ValueError(\"obs_seq is empty.\")\n",
    "    if len(obs_seq) != len(utt_seq):\n",
    "        raise ValueError(\"obs_seq and utt_seq must have the same length.\")\n",
    "    \n",
    "    speaker_type = speaker_config.get(\"speaker_type\")\n",
    "    \n",
    "    # --- Handle literal speaker ---\n",
    "    if speaker_type == \"literal\":\n",
    "        ll = log_likelihood_utt_seq(world, obs_seq, utt_seq, speaker_config)\n",
    "        return {\n",
    "            \"optimal_alpha\": None,\n",
    "            \"max_log_likelihood\": ll,\n",
    "            \"continuous_optimal_alpha\": None,\n",
    "            \"continuous_max_log_likelihood\": None,\n",
    "            \"determ_log_likelihood\": None,\n",
    "            \"optimization_result\": None,\n",
    "            \"grid_results\": None,\n",
    "            \"message\": \"Literal speaker does not use alpha parameter\"\n",
    "        }\n",
    "    elif speaker_type != \"pragmatic\":\n",
    "        raise ValueError(\n",
    "            f\"speaker_type must be 'literal' or 'pragmatic', got '{speaker_type}'\"\n",
    "        )\n",
    "    \n",
    "    # --- Validate pragmatic speaker config ---\n",
    "    required_keys = [\"omega\", \"psi\", \"update_internal\"]\n",
    "    missing_keys = [k for k in required_keys if k not in speaker_config]\n",
    "    if missing_keys:\n",
    "        raise ValueError(\n",
    "            f\"Missing required keys for pragmatic speaker: {missing_keys}\"\n",
    "        )\n",
    "    \n",
    "    # --- Create base config ---\n",
    "    base_config = {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": speaker_config[\"omega\"],\n",
    "        \"psi\": speaker_config[\"psi\"],\n",
    "        \"update_internal\": speaker_config[\"update_internal\"],\n",
    "        \"beta\": speaker_config.get(\"beta\", 0.0),\n",
    "        \"initial_beliefs_theta\": speaker_config.get(\"initial_beliefs_theta\", None)\n",
    "    }\n",
    "    \n",
    "    # --- Define objective function ---\n",
    "    def neg_log_likelihood(alpha: float) -> float:\n",
    "        config = {**base_config, \"alpha\": float(alpha)}\n",
    "        try:\n",
    "            ll = log_likelihood_utt_seq(world, obs_seq, utt_seq, config)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Error at alpha={alpha}: {e}\")\n",
    "            return np.inf\n",
    "        if ll == -np.inf:\n",
    "            return np.inf\n",
    "        return -ll\n",
    "    \n",
    "    # --- Run optimization ---\n",
    "    if grid_search:\n",
    "        # IMPROVED: Create alpha grid with specified spacing\n",
    "        a_min, a_max = alpha_bounds\n",
    "        \n",
    "        if grid_spacing == \"log\":\n",
    "            # Logarithmic spacing: more points at lower alpha values\n",
    "            alphas = list(np.exp(np.linspace(np.log(a_min), np.log(a_max), grid_points)))\n",
    "        else:  # linear\n",
    "            # Linear spacing: evenly distributed points\n",
    "            alphas = list(np.linspace(a_min, a_max, grid_points))\n",
    "        \n",
    "        # Evaluate log-likelihood at each alpha\n",
    "        log_likelihoods = []\n",
    "        for alpha in alphas:\n",
    "            ll = -neg_log_likelihood(alpha)\n",
    "            log_likelihoods.append(ll)\n",
    "        \n",
    "        log_likelihoods = np.array(log_likelihoods)\n",
    "        best_idx = np.argmax(log_likelihoods)\n",
    "        optimal_alpha_continuous = alphas[best_idx]\n",
    "        max_ll_continuous = log_likelihoods[best_idx]\n",
    "        \n",
    "        grid_results = {\n",
    "            \"alphas\": np.array(alphas),\n",
    "            \"log_likelihoods\": log_likelihoods,\n",
    "            \"best_idx\": best_idx,\n",
    "            \"grid_spacing\": grid_spacing  # Include spacing method in results\n",
    "        }\n",
    "        optimization_result = None\n",
    "        \n",
    "    else:\n",
    "        # Scipy optimization approach \n",
    "        try:\n",
    "            result = minimize_scalar(\n",
    "                neg_log_likelihood,\n",
    "                bounds=alpha_bounds,\n",
    "                method=method\n",
    "            )\n",
    "            if not result.success:\n",
    "                warnings.warn(\n",
    "                    f\"Optimization may not have converged: {result.message}\"\n",
    "                )\n",
    "            optimal_alpha_continuous = result.x\n",
    "            max_ll_continuous = -result.fun if np.isfinite(result.fun) else -np.inf\n",
    "            optimization_result = result\n",
    "        except Exception as e:\n",
    "            raise RuntimeError(f\"Optimization failed: {e}\")\n",
    "        \n",
    "        grid_results = None\n",
    "    \n",
    "    # --- Evaluate deterministic alpha ---\n",
    "    determ_ll = None\n",
    "    if include_determ:\n",
    "        config_determ = {**base_config, \"alpha\": \"determ\"}\n",
    "        try:\n",
    "            determ_ll = log_likelihood_utt_seq(world, obs_seq, utt_seq, config_determ)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to evaluate alpha='determ': {e}\")\n",
    "            determ_ll = -np.inf\n",
    "    \n",
    "    # --- Determine overall best alpha ---\n",
    "    if include_determ and determ_ll is not None and determ_ll > max_ll_continuous:\n",
    "        optimal_alpha = \"determ\"\n",
    "        max_ll = determ_ll\n",
    "    else:\n",
    "        optimal_alpha = optimal_alpha_continuous\n",
    "        max_ll = max_ll_continuous\n",
    "    \n",
    "    return {\n",
    "        \"optimal_alpha\": optimal_alpha,\n",
    "        \"max_log_likelihood\": max_ll,\n",
    "        \"continuous_optimal_alpha\": optimal_alpha_continuous,\n",
    "        \"continuous_max_log_likelihood\": max_ll_continuous,\n",
    "        \"determ_log_likelihood\": determ_ll,\n",
    "        \"optimization_result\": optimization_result,\n",
    "        \"grid_results\": grid_results\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01aa1204-b26c-4634-99e9-8584005dba2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e3ced70-b381-400a-888c-cc43eac77629",
   "metadata": {},
   "source": [
    "## SingleT Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb186865-c2af-4658-b25f-09155f399262",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_literal_log_likelihood_batched(\n",
    "    world: 'World',\n",
    "    df: pd.DataFrame,\n",
    "    initial_beliefs_theta: Optional[np.ndarray] = None\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Batched computation of log P(utt_seq | obs_seq, literal_speaker) for all rows (more memory efficent).\n",
    "    \n",
    "    The literal speaker's P(u|O) table is static, so we create the speaker once\n",
    "    and do batch lookups for all (observation, utterance) pairs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    world : World\n",
    "        The World object defining the communication game.\n",
    "    df : pd.DataFrame\n",
    "        DataFrame with 'obs_seq' and 'utt_seq' columns.\n",
    "    initial_beliefs_theta : Optional[np.ndarray], default None\n",
    "        Initial beliefs over theta.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Array of log-likelihoods, shape (n_rows,).\n",
    "    \"\"\"\n",
    "    if 'obs_seq' not in df.columns or 'utt_seq' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must have 'obs_seq' and 'utt_seq' columns\")\n",
    "    \n",
    "    n_rows = len(df)\n",
    "    if n_rows == 0:\n",
    "        return np.array([])\n",
    "    \n",
    "    # Create literal speaker ONCE to get static P(u|O) table\n",
    "    speaker = LiteralSpeaker(world, initial_beliefs_theta)\n",
    "    utt_log_prob_obs = speaker.utterance_log_prob_obs\n",
    "    \n",
    "    # Build lookup dictionary: (obs_tuple, utt_str) -> log_prob\n",
    "    lookup = {}\n",
    "    for utt in utt_log_prob_obs.index:\n",
    "        for obs in utt_log_prob_obs.columns:\n",
    "            lookup[(obs, utt)] = utt_log_prob_obs.at[utt, obs]\n",
    "    \n",
    "    # Compute log-likelihood for each row\n",
    "    log_likelihoods = []\n",
    "    for _, row in df.iterrows():\n",
    "        obs_seq = row['obs_seq']\n",
    "        utt_seq = row['utt_seq']\n",
    "        \n",
    "        row_ll = 0.0\n",
    "        for obs, utt in zip(obs_seq, utt_seq):\n",
    "            obs_key = tuple(obs) if not isinstance(obs, tuple) else obs\n",
    "            log_p = lookup.get((obs_key, utt), -np.inf)\n",
    "            row_ll += log_p\n",
    "            if np.isinf(row_ll):\n",
    "                break\n",
    "        \n",
    "        log_likelihoods.append(row_ll)\n",
    "    \n",
    "    return np.array(log_likelihoods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6954c8-5852-4b59-8db9-ee42e219eadd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31cbb0e0-fe67-4826-a840-b3d14b67e5da",
   "metadata": {},
   "source": [
    "## MultiT Vectorized Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031696d6-34e6-4b96-b10a-7357327e0baa",
   "metadata": {},
   "source": [
    "### Fitting Literal Speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203e4e87-e985-4ccc-b52b-68439581b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_literal_log_likelihood_multiT(\n",
    "    obs_data: Dict[str, Any],\n",
    "    target_speaker_keys: Optional[List[str]] = None,\n",
    "    target_alpha_keys: Optional[List[Any]] = None,\n",
    "    initial_beliefs_theta: Optional[np.ndarray] = None,\n",
    "    verbose: int = 0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Vectorized computation of log P(utt_seq | obs_seq, literal_speaker) for utterances.\n",
    "    \n",
    "    Evaluates how likely existing utterance sequences (generated by various speakers)\n",
    "    are under a literal speaker model. \n",
    "    \n",
    "    The computation is fully vectorized across ALL thetas, observation sequences,\n",
    "    generating speaker configurations, and utterance sequences in a single pass.\n",
    "    \n",
    "    Mutates obs_data by filling in log_lik_all_speaker[\"literal_fitted\"] for each\n",
    "    utterance record.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_data : Dict[str, Any]\n",
    "        Output from sample_observation_sequences_multiT with utterances generated.\n",
    "        Must have utterances populated via generate_utterances_for_observations_multiT.\n",
    "    target_speaker_keys : Optional[List[str]], default None\n",
    "        Which generating speakers' utterances to evaluate.\n",
    "        E.g., [\"inf_T\", \"persp_T\"] to evaluate only informative and persuasive speakers.\n",
    "        If None, evaluate all speaker keys with existing utterances.\n",
    "        Valid keys: \"literal\", \"inf_T\", \"inf_F\", \"persp_T\", \"persp_F\", \"persm_T\", \"persm_F\"\n",
    "    target_alpha_keys : Optional[List[Any]], default None\n",
    "        Which generating alphas' utterances to evaluate.\n",
    "        E.g., [5.0, 10.0] to evaluate only utterances generated with those alphas.\n",
    "        If None, evaluate all alpha keys with existing utterances.\n",
    "    Note: Not all (speaker_key, alpha_key) combinations exist.\n",
    "        - \"literal\" only has alpha_key 0.0\n",
    "        - Pragmatic speakers have their generated alpha values    \n",
    "        When both target_speaker_keys and target_alpha_keys are specified,\n",
    "        only combinations that exist in the data are processed.\n",
    "    initial_beliefs_theta : Optional[np.ndarray], default None\n",
    "        Initial beliefs over theta for the literal speaker.\n",
    "        Note: Does not affect literal speaker's P(u|O) table (which depends\n",
    "        only on truth values), but accepted for API consistency.\n",
    "    verbose : int, default 0\n",
    "        Verbosity level. If > 0, print summary of what's being processed.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Mutates obs_data in place.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Storage structure (per utterance record):\n",
    "        utt_record[\"log_lik_all_speaker\"][\"literal_fitted\"] = {\n",
    "            \"max_log_lik\": {T: float for T in Ts},\n",
    "            \"optimal_alpha\": {T: 0.0 for T in Ts}  # Always 0.0 for literal\n",
    "        }\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If any observation or utterance sequence has incorrect length (!= max_T).\n",
    "        If any T in Ts is outside the valid range [1, max_T].\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> # Evaluate ALL utterances under literal speaker\n",
    "    >>> compute_literal_log_likelihood_multiT(obs_data, verbose=1)\n",
    "    Processing 30000 utterance sequences (300 unique obs positions) across all configurations\n",
    "    \n",
    "    >>> # Check results\n",
    "    >>> utt_rec = obs_data[\"observations\"][0.5][0][\"utterances\"][\"inf_T\"][5.0][0]\n",
    "    >>> utt_rec[\"log_lik_all_speaker\"][\"literal_fitted\"]\n",
    "    {'max_log_lik': {5: -4.23, 10: -8.91, 15: -13.45, 20: -18.02},\n",
    "     'optimal_alpha': {5: 0.0, 10: 0.0, 15: 0.0, 20: 0.0}}\n",
    "    \"\"\"\n",
    "    \n",
    "    # EXTRACT CONFIGURATION\n",
    "    \n",
    "    world = obs_data[\"world\"]\n",
    "    Ts = obs_data[\"config\"][\"Ts\"]\n",
    "    max_T = obs_data[\"config\"][\"max_T\"]\n",
    "    thetas = obs_data[\"config\"][\"thetas\"]\n",
    "    \n",
    "    # VALIDATE Ts\n",
    "    \n",
    "    invalid_Ts = [T for T in Ts if T < 1 or T > max_T]\n",
    "    if invalid_Ts:\n",
    "        raise ValueError(\n",
    "            f\"All T in Ts must satisfy 1 <= T <= max_T ({max_T}). \"\n",
    "            f\"Invalid values: {invalid_Ts}\"\n",
    "        )\n",
    "    \n",
    "    # CREATE LITERAL SPEAKER AND EXTRACT PROBABILITY TABLE\n",
    "    \n",
    "    speaker = LiteralSpeaker(world, initial_beliefs_theta)\n",
    "    prob_table = speaker.utterance_log_prob_obs.values  # shape: (n_utterances, n_observations)\n",
    "    \n",
    "    # Use existing pandas Index objects\n",
    "    obs_index = speaker.utterance_log_prob_obs.columns\n",
    "    utt_index = speaker.utterance_log_prob_obs.index\n",
    "    \n",
    "    # FLATTEN ALL UTTERANCE SEQUENCES WITH LOCATION TRACKING AND VALIDATION\n",
    "    # Also collect observation sequences by position    \n",
    "    flat_data = []\n",
    "    skipped_combinations = set()\n",
    "    \n",
    "\n",
    "    unique_obs_positions = []  # List of obs_seq (one per unique position)\n",
    "    obs_pos_to_unique_idx = {}  # (theta, obs_list_pos) -> index in unique_obs_positions\n",
    "    \n",
    "    for theta in thetas:\n",
    "        for obs_list_pos, obs_info in enumerate(obs_data[\"observations\"][theta]):\n",
    "            obs_seq = obs_info[\"obs_seq\"]\n",
    "            obs_idx = obs_info[\"obs_idx\"]\n",
    "            \n",
    "            if len(obs_seq) != max_T:\n",
    "                raise ValueError(\n",
    "                    f\"Observation sequence length mismatch: \"\n",
    "                    f\"expected {max_T}, got {len(obs_seq)} \"\n",
    "                    f\"at theta={theta}, obs_idx={obs_idx}\"\n",
    "                )\n",
    "            \n",
    "            if obs_info[\"utterances\"] is None:\n",
    "                continue\n",
    "            \n",
    "            # Register this observation position if not already seen\n",
    "            obs_pos_key = (theta, obs_list_pos)\n",
    "            if obs_pos_key not in obs_pos_to_unique_idx:\n",
    "                obs_pos_to_unique_idx[obs_pos_key] = len(unique_obs_positions)\n",
    "                unique_obs_positions.append(obs_seq)\n",
    "            \n",
    "            for speaker_key, alpha_dict in obs_info[\"utterances\"].items():\n",
    "                if target_speaker_keys is not None and speaker_key not in target_speaker_keys:\n",
    "                    skipped_combinations.add(f\"speaker_key={speaker_key}\")\n",
    "                    continue\n",
    "                \n",
    "                for alpha_key, utt_records in alpha_dict.items():\n",
    "                    if target_alpha_keys is not None and alpha_key not in target_alpha_keys:\n",
    "                        skipped_combinations.add(f\"alpha_key={alpha_key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    for utt_list_idx, utt_rec in enumerate(utt_records):\n",
    "                        utt_seq = utt_rec[\"utt_seq\"]\n",
    "                        \n",
    "                        if len(utt_seq) != max_T:\n",
    "                            raise ValueError(\n",
    "                                f\"Utterance sequence length mismatch: \"\n",
    "                                f\"expected {max_T}, got {len(utt_seq)} \"\n",
    "                                f\"at theta={theta}, obs_idx={obs_idx}, \"\n",
    "                                f\"speaker_key={speaker_key}, alpha_key={alpha_key}, \"\n",
    "                                f\"utt_idx={utt_list_idx}\"\n",
    "                            )\n",
    "                        \n",
    "                        flat_data.append({\n",
    "                            \"utt_seq\": utt_seq,\n",
    "                            \"obs_unique_idx\": obs_pos_to_unique_idx[obs_pos_key],\n",
    "                            \"location\": (theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx)\n",
    "                        })\n",
    "    \n",
    "    # VERBOSE OUTPUT\n",
    "    \n",
    "    n_unique_obs = len(unique_obs_positions)\n",
    "    n_total = len(flat_data)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        filter_desc = []\n",
    "        if target_speaker_keys is not None:\n",
    "            filter_desc.append(f\"speakers: {target_speaker_keys}\")\n",
    "        if target_alpha_keys is not None:\n",
    "            filter_desc.append(f\"alphas: {target_alpha_keys}\")\n",
    "        \n",
    "        if filter_desc:\n",
    "            print(f\"Processing {n_total} utterance sequences ({n_unique_obs} unique obs positions) \"\n",
    "                  f\"for {', '.join(filter_desc)}\")\n",
    "        else:\n",
    "            print(f\"Processing {n_total} utterance sequences ({n_unique_obs} unique obs positions) \"\n",
    "                  f\"across all configurations\")\n",
    "        \n",
    "        if skipped_combinations and verbose > 1:\n",
    "            print(f\"  Skipped: {skipped_combinations}\")\n",
    "    \n",
    "    # EARLY EXIT IF NOTHING TO PROCESS\n",
    "    \n",
    "    if n_total == 0:\n",
    "        if verbose > 0:\n",
    "            print(\"No utterance sequences to process (check filters or ensure utterances exist)\")\n",
    "        return\n",
    "    \n",
    "    # Defensive assertion\n",
    "    # at least one observation position\n",
    "    assert n_unique_obs > 0, \"Internal error: n_total > 0 but no unique obs positions registered\"\n",
    "    \n",
    "    # BUILD OBSERVATION INDEX ARRAY\n",
    "    \n",
    "    # Flatten unique observations only\n",
    "    obs_flat_unique = list(chain.from_iterable(\n",
    "        (tuple(obs) if not isinstance(obs, tuple) else obs for obs in obs_seq)\n",
    "        for obs_seq in unique_obs_positions\n",
    "    ))\n",
    "    \n",
    "    # Batch lookup for unique observations only\n",
    "    obs_indices_flat_unique = obs_index.get_indexer(obs_flat_unique)\n",
    "    \n",
    "    # Validate\n",
    "    if (obs_indices_flat_unique < 0).any():\n",
    "        bad_idx = np.where(obs_indices_flat_unique < 0)[0][0]\n",
    "        raise ValueError(f\"Unknown observation encountered: {obs_flat_unique[bad_idx]}\")\n",
    "    \n",
    "    # Reshape to (n_unique_obs, max_T)\n",
    "    obs_indices_unique = obs_indices_flat_unique.reshape(n_unique_obs, max_T)\n",
    "    \n",
    "    # Build expansion array: which unique obs position each flat_data item uses\n",
    "    unique_obs_idx_per_item = np.array(\n",
    "        [item[\"obs_unique_idx\"] for item in flat_data],\n",
    "        dtype=np.int32\n",
    "    )\n",
    "    \n",
    "    # Expand to full (n_total, max_T) via fancy indexing\n",
    "    obs_indices = obs_indices_unique[unique_obs_idx_per_item]\n",
    "    \n",
    "    # BUILD UTTERANCE INDEX ARRAY\n",
    "    \n",
    "    # Flatten utterances directly from flat_data\n",
    "    utt_flat = list(chain.from_iterable(item[\"utt_seq\"] for item in flat_data))\n",
    "    \n",
    "    # Batch lookup\n",
    "    utt_indices_flat = utt_index.get_indexer(utt_flat)\n",
    "    \n",
    "    # Validate\n",
    "    if (utt_indices_flat < 0).any():\n",
    "        bad_idx = np.where(utt_indices_flat < 0)[0][0]\n",
    "        raise ValueError(f\"Unknown utterance encountered: {utt_flat[bad_idx]}\")\n",
    "    \n",
    "    # Reshape to (n_total, max_T)\n",
    "    utt_indices = utt_indices_flat.reshape(n_total, max_T)\n",
    "    \n",
    "    # VECTORIZED LIKELIHOOD COMPUTATION\n",
    "    \n",
    "    # Single numpy advanced indexing: shape (n_total, max_T)\n",
    "    log_probs = prob_table[utt_indices, obs_indices]\n",
    "    \n",
    "    # Cumulative sum across time dimension\n",
    "    cumsum_log_probs = np.cumsum(log_probs, axis=1)\n",
    "    \n",
    "    # Extract all Ts at once (Optimization D)\n",
    "    T_indices = np.array(Ts, dtype=np.int32) - 1  # Convert to 0-indexed\n",
    "    log_liks_all_T = cumsum_log_probs[:, T_indices]  # shape: (n_total, len(Ts))\n",
    "    \n",
    "    # DISTRIBUTE RESULTS BACK TO NESTED STRUCTURE\n",
    "    \n",
    "    for i, item in enumerate(flat_data):\n",
    "        theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx = item[\"location\"]\n",
    "        \n",
    "        utt_rec = obs_data[\"observations\"][theta][obs_list_pos][\"utterances\"][speaker_key][alpha_key][utt_list_idx]\n",
    "        \n",
    "        if utt_rec[\"log_lik_all_speaker\"] is None:\n",
    "            utt_rec[\"log_lik_all_speaker\"] = {}\n",
    "        \n",
    "        # Build result dicts\n",
    "        utt_rec[\"log_lik_all_speaker\"][\"literal_fitted\"] = {\n",
    "            \"max_log_lik\": {T: float(ll) for T, ll in zip(Ts, log_liks_all_T[i])},\n",
    "            \"optimal_alpha\": {T: 0.0 for T in Ts}\n",
    "        }\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"Completed: stored results in log_lik_all_speaker['literal_fitted']\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c73e91-7c21-4653-a16d-4a2e4ac080ed",
   "metadata": {},
   "source": [
    "### Fitting Pragmatic Speaker with update_internal = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f142b5f7-c30e-4584-8bd3-84ca31a0290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pragmatic_static_log_likelihood_multiT(\n",
    "    obs_data: Dict[str, Any],\n",
    "    fitting_psi: Literal[\"inf\", \"pers+\", \"pers-\"],\n",
    "    target_speaker_keys: Optional[List[str]] = None,\n",
    "    target_alpha_keys: Optional[List[Any]] = None,\n",
    "    method: Literal[\"grid\", \"scipy\"] = \"grid\",\n",
    "    alpha_bounds: Tuple[float, float] = (0.1, 50.0),\n",
    "    grid_spacing: Literal[\"log\", \"linear\"] = \"log\",\n",
    "    n_grid: int = 100,\n",
    "    include_determ: bool = True,\n",
    "    n_jobs: int = 1,\n",
    "    backend: str = \"loky\",\n",
    "    verbose: int = 0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Find optimal alpha and max log-likelihood under a pragmatic speaker \n",
    "    with update_internal=False.\n",
    "    \n",
    "    Mutates obs_data by filling in log_lik_all_speaker[\"{psi}_F_fitted\"] \n",
    "    for each utterance record.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_data : Dict[str, Any]\n",
    "        Output from sample_observation_sequences_multiT with utterances generated.\n",
    "    fitting_psi : {\"inf\", \"pers+\", \"pers-\"}\n",
    "        The psi parameter for the fitting speaker.\n",
    "    target_speaker_keys : Optional[List[str]], default None\n",
    "        Which generating speakers' utterances to evaluate. If None, all.\n",
    "    target_alpha_keys : Optional[List[Any]], default None\n",
    "        Which generating alphas' utterances to evaluate. If None, all.\n",
    "    method : {\"grid\", \"scipy\"}, default \"grid\"\n",
    "        Optimization method.\n",
    "    alpha_bounds : Tuple[float, float], default (0.1, 50.0)\n",
    "        Search range for alpha.\n",
    "    grid_spacing : {\"log\", \"linear\"}, default \"log\"\n",
    "        Grid spacing (only for method=\"grid\").\n",
    "    n_grid : int, default 100\n",
    "        Number of grid points (only for method=\"grid\").\n",
    "    include_determ : bool, default True\n",
    "        Whether to also evaluate alpha=\"determ\".\n",
    "    n_jobs : int, default 1\n",
    "        Number of parallel jobs (only for method=\"scipy\").\n",
    "    backend : str, default \"loky\"\n",
    "        Joblib backend (only for method=\"scipy\").\n",
    "    verbose : int, default 0\n",
    "        Verbosity level.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Mutates obs_data in place.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Storage key: \"{psi}_F_fitted\" (e.g., \"inf_F_fitted\", \"persp_F_fitted\")\n",
    "    \n",
    "    Storage structure:\n",
    "        utt_record[\"log_lik_all_speaker\"][\"{psi}_F_fitted\"] = {\n",
    "            \"max_log_lik\": {T: float for T in Ts},\n",
    "            \"optimal_alpha\": {T: float or \"determ\" for T in Ts}\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Input validation\n",
    "    if fitting_psi not in [\"inf\", \"pers+\", \"pers-\"]:\n",
    "        raise ValueError(f\"fitting_psi must be 'inf', 'pers+', or 'pers-', got '{fitting_psi}'\")\n",
    "    \n",
    "    if method not in [\"grid\", \"scipy\"]:\n",
    "        raise ValueError(f\"method must be 'grid' or 'scipy', got '{method}'\")\n",
    "    \n",
    "    if method == \"grid\" and grid_spacing not in [\"log\", \"linear\"]:\n",
    "        raise ValueError(f\"grid_spacing must be 'log' or 'linear', got '{grid_spacing}'\")\n",
    "    \n",
    "    if backend not in [\"loky\", \"multiprocessing\", \"threading\"]:\n",
    "        raise ValueError(f\"backend must be 'loky', 'multiprocessing', or 'threading'\")\n",
    "    \n",
    "    # Determine storage key\n",
    "    psi_prefix = {\"inf\": \"inf\", \"pers+\": \"persp\", \"pers-\": \"persm\"}[fitting_psi]\n",
    "    fitted_key = f\"{psi_prefix}_F_fitted\"\n",
    "    \n",
    "    # Extract configuration\n",
    "    world = obs_data[\"world\"]\n",
    "    Ts = obs_data[\"config\"][\"Ts\"]\n",
    "    max_T = obs_data[\"config\"][\"max_T\"]\n",
    "    thetas = obs_data[\"config\"][\"thetas\"]\n",
    "    \n",
    "    # Validate Ts\n",
    "    invalid_Ts = [T for T in Ts if T < 1 or T > max_T]\n",
    "    if invalid_Ts:\n",
    "        raise ValueError(f\"All T must satisfy 1 <= T <= {max_T}. Invalid: {invalid_Ts}\")\n",
    "    \n",
    "    # Flatten all utterance sequences with location tracking\n",
    "    flat_data = []\n",
    "    skipped_combinations = set()\n",
    "    \n",
    "    unique_obs_positions = []\n",
    "    obs_pos_to_unique_idx = {}\n",
    "    \n",
    "    for theta in thetas:\n",
    "        for obs_list_pos, obs_info in enumerate(obs_data[\"observations\"][theta]):\n",
    "            obs_seq = obs_info[\"obs_seq\"]\n",
    "            obs_idx = obs_info[\"obs_idx\"]\n",
    "            \n",
    "            if len(obs_seq) != max_T:\n",
    "                raise ValueError(\n",
    "                    f\"Observation sequence length mismatch at theta={theta}, obs_idx={obs_idx}\"\n",
    "                )\n",
    "            \n",
    "            if obs_info[\"utterances\"] is None:\n",
    "                continue\n",
    "            \n",
    "            # Register unique observation position\n",
    "            obs_pos_key = (theta, obs_list_pos)\n",
    "            if obs_pos_key not in obs_pos_to_unique_idx:\n",
    "                obs_pos_to_unique_idx[obs_pos_key] = len(unique_obs_positions)\n",
    "                unique_obs_positions.append(obs_seq)\n",
    "            \n",
    "            for speaker_key, alpha_dict in obs_info[\"utterances\"].items():\n",
    "                if target_speaker_keys is not None and speaker_key not in target_speaker_keys:\n",
    "                    skipped_combinations.add(f\"speaker_key={speaker_key}\")\n",
    "                    continue\n",
    "                \n",
    "                for alpha_key, utt_records in alpha_dict.items():\n",
    "                    if target_alpha_keys is not None and alpha_key not in target_alpha_keys:\n",
    "                        skipped_combinations.add(f\"alpha_key={alpha_key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    for utt_list_idx, utt_rec in enumerate(utt_records):\n",
    "                        utt_seq = utt_rec[\"utt_seq\"]\n",
    "                        \n",
    "                        if len(utt_seq) != max_T:\n",
    "                            raise ValueError(\n",
    "                                f\"Utterance sequence length mismatch at theta={theta}, \"\n",
    "                                f\"obs_idx={obs_idx}, speaker_key={speaker_key}\"\n",
    "                            )\n",
    "                        \n",
    "                        flat_data.append({\n",
    "                            \"utt_seq\": utt_seq,\n",
    "                            \"obs_unique_idx\": obs_pos_to_unique_idx[obs_pos_key],\n",
    "                            \"location\": (theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx)\n",
    "                        })\n",
    "    \n",
    "    # Verbose output\n",
    "    n_unique_obs = len(unique_obs_positions)\n",
    "    n_total = len(flat_data)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        filter_desc = []\n",
    "        if target_speaker_keys is not None:\n",
    "            filter_desc.append(f\"speakers: {target_speaker_keys}\")\n",
    "        if target_alpha_keys is not None:\n",
    "            filter_desc.append(f\"alphas: {target_alpha_keys}\")\n",
    "        \n",
    "        print(f\"Static pragmatic speaker fitting (psi={fitting_psi}, method={method}):\")\n",
    "        print(f\"  Storage key: '{fitted_key}'\")\n",
    "        if filter_desc:\n",
    "            print(f\"  Processing {n_total} utterance sequences ({n_unique_obs} unique obs) \"\n",
    "                  f\"for {', '.join(filter_desc)}\")\n",
    "        else:\n",
    "            print(f\"  Processing {n_total} utterance sequences ({n_unique_obs} unique obs)\")\n",
    "        print(f\"  Ts: {Ts}\")\n",
    "        \n",
    "        if method == \"grid\":\n",
    "            n_alphas = n_grid + (1 if include_determ else 0)\n",
    "            print(f\"  Grid: {n_grid} points, spacing={grid_spacing}, bounds={alpha_bounds}\")\n",
    "        else:\n",
    "            print(f\"  Scipy: {n_total * len(Ts)} optimizations\")\n",
    "    \n",
    "    # Early exit\n",
    "    if n_total == 0:\n",
    "        if verbose > 0:\n",
    "            print(\"No utterance sequences to process\")\n",
    "        return\n",
    "    \n",
    "    # Method-specific computation\n",
    "    if method == \"grid\":\n",
    "        results = _static_grid_search_multiT(\n",
    "            flat_data=flat_data,\n",
    "            unique_obs_positions=unique_obs_positions,\n",
    "            world=world,\n",
    "            psi=fitting_psi,\n",
    "            Ts=Ts,\n",
    "            max_T=max_T,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            grid_spacing=grid_spacing,\n",
    "            n_grid=n_grid,\n",
    "            include_determ=include_determ,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        results = _static_scipy_optimization_multiT(\n",
    "            flat_data=flat_data,\n",
    "            unique_obs_positions=unique_obs_positions,\n",
    "            world=world,\n",
    "            psi=fitting_psi,\n",
    "            Ts=Ts,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            include_determ=include_determ,\n",
    "            n_jobs=n_jobs,\n",
    "            backend=backend,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    \n",
    "    # Distribute results\n",
    "    for i, item in enumerate(flat_data):\n",
    "        theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx = item[\"location\"]\n",
    "        \n",
    "        utt_rec = obs_data[\"observations\"][theta][obs_list_pos][\"utterances\"][speaker_key][alpha_key][utt_list_idx]\n",
    "        \n",
    "        if utt_rec[\"log_lik_all_speaker\"] is None:\n",
    "            utt_rec[\"log_lik_all_speaker\"] = {}\n",
    "        \n",
    "        utt_rec[\"log_lik_all_speaker\"][fitted_key] = {\n",
    "            \"max_log_lik\": results[\"max_log_lik\"][i],\n",
    "            \"optimal_alpha\": results[\"optimal_alpha\"][i]\n",
    "        }\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"Completed: stored results in log_lik_all_speaker['{fitted_key}']\")\n",
    "\n",
    "\n",
    "def _static_grid_search_multiT(\n",
    "    flat_data: List[Dict[str, Any]],\n",
    "    unique_obs_positions: List[List[Tuple[int, ...]]],\n",
    "    world: 'World',\n",
    "    psi: str,\n",
    "    Ts: List[int],\n",
    "    max_T: int,\n",
    "    alpha_bounds: Tuple[float, float],\n",
    "    grid_spacing: str,\n",
    "    n_grid: int,\n",
    "    include_determ: bool,\n",
    "    verbose: int\n",
    ") -> Dict[str, List[Dict[int, Any]]]:\n",
    "    \"\"\"\n",
    "    Grid search for optimal alpha with static speaker.\n",
    "    \n",
    "    Optimization: Utilities are computed once (alpha-independent).\n",
    "    Memory-efficient: Loops over alphas instead of 4D vectorization.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with \"max_log_lik\" and \"optimal_alpha\" lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = len(flat_data)\n",
    "    n_unique_obs = len(unique_obs_positions)\n",
    "    n_Ts = len(Ts)\n",
    "    T_indices = np.array(Ts, dtype=np.int32) - 1\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 1: Create ONE speaker and extract utility table\n",
    "    # =========================================================================\n",
    "    \n",
    "    ref_speaker = PragmaticSpeaker_obs(\n",
    "        world=world,\n",
    "        omega=\"strat\",\n",
    "        psi=psi,\n",
    "        update_internal=False,\n",
    "        alpha=1.0,\n",
    "        beta=0.0,\n",
    "        initial_beliefs_theta=None\n",
    "    )\n",
    "    \n",
    "    # Utility table: shape (n_utterances, n_observations)\n",
    "    utility_table = ref_speaker.utility.values\n",
    "    n_utterances, n_obs_total = utility_table.shape\n",
    "    \n",
    "    # Index mappings\n",
    "    obs_index = ref_speaker.utility.columns\n",
    "    utt_index = ref_speaker.utility.index\n",
    "    utterances = list(utt_index)\n",
    "    utt_to_idx = {u: i for i, u in enumerate(utterances)}\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print(f\"  utility_table: {utility_table.shape}, {utility_table.nbytes/1024/1024:.1f} MB\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 2: Build observation index arrays\n",
    "    # =========================================================================\n",
    "    \n",
    "    obs_flat_unique = list(chain.from_iterable(\n",
    "        (tuple(obs) if not isinstance(obs, tuple) else obs for obs in obs_seq)\n",
    "        for obs_seq in unique_obs_positions\n",
    "    ))\n",
    "    \n",
    "    obs_indices_flat_unique = obs_index.get_indexer(obs_flat_unique)\n",
    "    if (obs_indices_flat_unique < 0).any():\n",
    "        bad_idx = np.where(obs_indices_flat_unique < 0)[0][0]\n",
    "        raise ValueError(f\"Unknown observation: {obs_flat_unique[bad_idx]}\")\n",
    "    \n",
    "    obs_indices_unique = obs_indices_flat_unique.reshape(n_unique_obs, max_T)\n",
    "    \n",
    "    unique_obs_idx_per_item = np.array(\n",
    "        [item[\"obs_unique_idx\"] for item in flat_data], dtype=np.int32\n",
    "    )\n",
    "    obs_indices = obs_indices_unique[unique_obs_idx_per_item]  # (n_total, max_T)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 3: Build utterance index array\n",
    "    # =========================================================================\n",
    "    \n",
    "    utt_flat = list(chain.from_iterable(item[\"utt_seq\"] for item in flat_data))\n",
    "    utt_indices_flat = np.array([utt_to_idx[u] for u in utt_flat], dtype=np.int32)\n",
    "    utt_indices = utt_indices_flat.reshape(n_total, max_T)  # (n_total, max_T)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 4: Extract utilities for all (item, time_step) pairs\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Utilities for observed (utterance, observation) pairs: (n_total, max_T)\n",
    "    observed_utilities = utility_table[utt_indices, obs_indices]\n",
    "    \n",
    "    # Utilities for ALL utterances at each position: (n_total, max_T, n_utterances)\n",
    "    # Computed once, reused for all alphas\n",
    "    all_utilities_per_step = utility_table[:, obs_indices].transpose(1, 2, 0)\n",
    "    \n",
    "    if verbose > 1:\n",
    "        print(f\"  all_utilities_per_step: {all_utilities_per_step.shape}, \"\n",
    "              f\"{all_utilities_per_step.nbytes/1024/1024:.1f} MB\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 5: Create alpha grid\n",
    "    # =========================================================================\n",
    "    \n",
    "    a_min, a_max = alpha_bounds\n",
    "    if grid_spacing == \"log\":\n",
    "        alphas = list(np.exp(np.linspace(np.log(a_min), np.log(a_max), n_grid)))\n",
    "    else:\n",
    "        alphas = list(np.linspace(a_min, a_max, n_grid))\n",
    "    \n",
    "    if include_determ:\n",
    "        alphas.append(\"determ\")\n",
    "    \n",
    "    n_alphas = len(alphas)\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 6: Compute log P(u|O,α) for each alpha (loop for memory efficiency)\n",
    "    # =========================================================================\n",
    "    \n",
    "    # Output array: (n_alphas, n_total, n_Ts)\n",
    "    all_lls = np.zeros((n_alphas, n_total, n_Ts))\n",
    "    \n",
    "    for alpha_idx, alpha in enumerate(alphas):\n",
    "        if verbose > 1 and alpha_idx % 20 == 0:\n",
    "            print(f\"  Processing alpha {alpha_idx+1}/{n_alphas}\")\n",
    "        \n",
    "        if alpha == \"determ\":\n",
    "            # Deterministic: uniform over max-utility utterances\n",
    "            max_utilities = np.max(all_utilities_per_step, axis=2)  # (n_total, max_T)\n",
    "            is_max = np.isclose(observed_utilities, max_utilities)\n",
    "            is_max_all = np.isclose(all_utilities_per_step, max_utilities[:, :, np.newaxis])\n",
    "            n_ties = np.sum(is_max_all, axis=2)\n",
    "            log_probs = np.where(is_max, -np.log(n_ties), -np.inf)\n",
    "        else:\n",
    "            # Softmax: P(u|O,α) = exp(α·U(u)) / Σ exp(α·U(u'))\n",
    "            scaled_observed = alpha * observed_utilities  # (n_total, max_T)\n",
    "            scaled_all = alpha * all_utilities_per_step   # (n_total, max_T, n_utterances)\n",
    "            log_normalizers = logsumexp(scaled_all, axis=2)  # (n_total, max_T)\n",
    "            log_probs = scaled_observed - log_normalizers\n",
    "        \n",
    "        # Cumulative sum and extract for Ts\n",
    "        cumsum_log_probs = np.cumsum(log_probs, axis=1)  # (n_total, max_T)\n",
    "        log_liks = cumsum_log_probs[:, T_indices]  # (n_total, n_Ts)\n",
    "        \n",
    "        # Handle -inf propagation\n",
    "        has_neginf = np.isneginf(log_probs)\n",
    "        for t_idx, T in enumerate(Ts):\n",
    "            has_neginf_up_to_T = np.any(has_neginf[:, :T], axis=1)\n",
    "            log_liks[has_neginf_up_to_T, t_idx] = -np.inf\n",
    "        \n",
    "        all_lls[alpha_idx] = log_liks\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 7: Find optimal alpha for each (sequence, T)\n",
    "    # =========================================================================\n",
    "    \n",
    "    best_alpha_indices = np.argmax(all_lls, axis=0)  # (n_total, n_Ts)\n",
    "    \n",
    "    row_indices = np.arange(n_total)[:, np.newaxis]\n",
    "    T_col_indices = np.arange(n_Ts)[np.newaxis, :]\n",
    "    max_lls = all_lls[best_alpha_indices, row_indices, T_col_indices]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # PHASE 8: Convert to list of dicts\n",
    "    # =========================================================================\n",
    "    \n",
    "    alphas_array = np.array(alphas, dtype=object)\n",
    "    \n",
    "    max_log_lik_list = []\n",
    "    optimal_alpha_list = []\n",
    "    \n",
    "    for i in range(n_total):\n",
    "        max_log_lik_list.append({\n",
    "            T: float(max_lls[i, t_idx]) for t_idx, T in enumerate(Ts)\n",
    "        })\n",
    "        optimal_alpha_list.append({\n",
    "            T: alphas_array[best_alpha_indices[i, t_idx]] for t_idx, T in enumerate(Ts)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"max_log_lik\": max_log_lik_list,\n",
    "        \"optimal_alpha\": optimal_alpha_list\n",
    "    }\n",
    "\n",
    "\n",
    "def _static_scipy_optimization_multiT(\n",
    "    flat_data: List[Dict[str, Any]],\n",
    "    unique_obs_positions: List[List[Tuple[int, ...]]],\n",
    "    world: 'World',\n",
    "    psi: str,\n",
    "    Ts: List[int],\n",
    "    alpha_bounds: Tuple[float, float],\n",
    "    include_determ: bool,\n",
    "    n_jobs: int,\n",
    "    backend: str,\n",
    "    verbose: int\n",
    ") -> Dict[str, List[Dict[int, Any]]]:\n",
    "    \"\"\"\n",
    "    Scipy optimization for optimal alpha with static speaker.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with \"max_log_lik\" and \"optimal_alpha\" lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = len(flat_data)\n",
    "    \n",
    "    # Reconstruct obs_seq for each item\n",
    "    obs_seqs = [unique_obs_positions[item[\"obs_unique_idx\"]] for item in flat_data]\n",
    "    \n",
    "    # Build tasks: one per (sequence, T)\n",
    "    tasks = []\n",
    "    for item_idx, item in enumerate(flat_data):\n",
    "        obs_seq = obs_seqs[item_idx]\n",
    "        for T in Ts:\n",
    "            tasks.append({\n",
    "                \"item_idx\": item_idx,\n",
    "                \"T\": T,\n",
    "                \"obs_seq\": obs_seq[:T],\n",
    "                \"utt_seq\": item[\"utt_seq\"][:T]\n",
    "            })\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"  Running {len(tasks)} scipy optimizations...\")\n",
    "    \n",
    "    # Worker function\n",
    "    def optimize_single(task):\n",
    "        base_config = {\n",
    "            \"speaker_type\": \"pragmatic\",\n",
    "            \"omega\": \"strat\",\n",
    "            \"psi\": psi,\n",
    "            \"update_internal\": False,\n",
    "            \"beta\": 0.0,\n",
    "            \"initial_beliefs_theta\": None\n",
    "        }\n",
    "        \n",
    "        result = log_likelihood_alpha_opt_utt_seq(\n",
    "            world=world,\n",
    "            obs_seq=task[\"obs_seq\"],\n",
    "            utt_seq=task[\"utt_seq\"],\n",
    "            speaker_config=base_config,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            grid_search=False,\n",
    "            include_determ=include_determ\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"item_idx\": task[\"item_idx\"],\n",
    "            \"T\": task[\"T\"],\n",
    "            \"optimal_alpha\": result[\"optimal_alpha\"],\n",
    "            \"max_log_lik\": result[\"max_log_likelihood\"]\n",
    "        }\n",
    "    \n",
    "    # Execute\n",
    "    if n_jobs == 1:\n",
    "        results = [optimize_single(task) for task in tasks]\n",
    "    else:\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend, verbose=verbose)(\n",
    "            delayed(optimize_single)(task) for task in tasks\n",
    "        )\n",
    "    \n",
    "    # Reorganize by item_idx\n",
    "    max_log_lik_list = [{} for _ in range(n_total)]\n",
    "    optimal_alpha_list = [{} for _ in range(n_total)]\n",
    "    \n",
    "    for res in results:\n",
    "        item_idx = res[\"item_idx\"]\n",
    "        T = res[\"T\"]\n",
    "        max_log_lik_list[item_idx][T] = res[\"max_log_lik\"]\n",
    "        optimal_alpha_list[item_idx][T] = res[\"optimal_alpha\"]\n",
    "    \n",
    "    return {\n",
    "        \"max_log_lik\": max_log_lik_list,\n",
    "        \"optimal_alpha\": optimal_alpha_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14a2d2a-1384-42b9-8e18-c3efe72d2abf",
   "metadata": {},
   "source": [
    "### Fitting Pragmatic Speaker with update_internal = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2d753c-46fe-4127-8bcd-c4aa20fb5691",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pragmatic_dynamic_log_likelihood_multiT(\n",
    "    obs_data: Dict[str, Any],\n",
    "    fitting_psi: Literal[\"inf\", \"pers+\", \"pers-\"],\n",
    "    fitting_Ts: Optional[List[int]] = None,\n",
    "    target_speaker_keys: Optional[List[str]] = None,\n",
    "    target_alpha_keys: Optional[List[Any]] = None,\n",
    "    method: Literal[\"grid\", \"scipy\"] = \"grid\",\n",
    "    alpha_bounds: Tuple[float, float] = (0.1, 50.0),\n",
    "    grid_spacing: Literal[\"log\", \"linear\"] = \"log\",\n",
    "    n_grid: int = 100,\n",
    "    include_determ: bool = True,\n",
    "    n_jobs: int = 1,\n",
    "    backend: str = \"loky\",\n",
    "    verbose: int = 0\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Find optimal alpha and max log-likelihood under a pragmatic speaker \n",
    "    with update_internal=True.\n",
    "    \n",
    "    Mutates obs_data by filling in log_lik_all_speaker[\"{psi}_T_fitted\"] \n",
    "    for each utterance record.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_data : Dict[str, Any]\n",
    "        Output from sample_observation_sequences_multiT with utterances generated.\n",
    "    fitting_psi : {\"inf\", \"pers+\", \"pers-\"}\n",
    "        The psi parameter for the fitting speaker.\n",
    "    fitting_Ts : Optional[List[int]], default None\n",
    "        Subset of Ts to compute likelihoods for. If None, use all Ts.\n",
    "    target_speaker_keys : Optional[List[str]], default None\n",
    "        Which generating speakers' utterances to evaluate. If None, all.\n",
    "    target_alpha_keys : Optional[List[Any]], default None\n",
    "        Which generating alphas' utterances to evaluate. If None, all.\n",
    "    method : {\"grid\", \"scipy\"}, default \"grid\"\n",
    "        Optimization method.\n",
    "    alpha_bounds : Tuple[float, float], default (0.1, 50.0)\n",
    "        Search range for alpha.\n",
    "    grid_spacing : {\"log\", \"linear\"}, default \"log\"\n",
    "        Grid spacing (only for method=\"grid\").\n",
    "    n_grid : int, default 100\n",
    "        Number of grid points (only for method=\"grid\").\n",
    "    include_determ : bool, default True\n",
    "        Whether to also evaluate alpha=\"determ\".\n",
    "    n_jobs : int, default 1\n",
    "        Number of parallel jobs.\n",
    "    backend : str, default \"loky\"\n",
    "        Joblib backend.\n",
    "    verbose : int, default 0\n",
    "        Verbosity level.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "        Mutates obs_data in place.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Storage key: \"{psi}_T_fitted\" (e.g., \"inf_T_fitted\", \"persp_T_fitted\")\n",
    "    \n",
    "    Storage structure:\n",
    "        utt_record[\"log_lik_all_speaker\"][\"{psi}_T_fitted\"] = {\n",
    "            \"max_log_lik\": {T: float for T in fitting_Ts},\n",
    "            \"optimal_alpha\": {T: float or \"determ\" for T in fitting_Ts}\n",
    "        }\n",
    "    \"\"\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # INPUT VALIDATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    if fitting_psi not in [\"inf\", \"pers+\", \"pers-\"]:\n",
    "        raise ValueError(f\"fitting_psi must be 'inf', 'pers+', or 'pers-', got '{fitting_psi}'\")\n",
    "    \n",
    "    if method not in [\"grid\", \"scipy\"]:\n",
    "        raise ValueError(f\"method must be 'grid' or 'scipy', got '{method}'\")\n",
    "    \n",
    "    if method == \"grid\" and grid_spacing not in [\"log\", \"linear\"]:\n",
    "        raise ValueError(f\"grid_spacing must be 'log' or 'linear', got '{grid_spacing}'\")\n",
    "    \n",
    "    if backend not in [\"loky\", \"multiprocessing\", \"threading\"]:\n",
    "        raise ValueError(f\"backend must be 'loky', 'multiprocessing', or 'threading'\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DETERMINE STORAGE KEY\n",
    "    # =========================================================================\n",
    "    \n",
    "    psi_prefix = {\"inf\": \"inf\", \"pers+\": \"persp\", \"pers-\": \"persm\"}[fitting_psi]\n",
    "    fitted_key = f\"{psi_prefix}_T_fitted\"\n",
    "    \n",
    "    # =========================================================================\n",
    "    # EXTRACT CONFIGURATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    world = obs_data[\"world\"]\n",
    "    config_Ts = obs_data[\"config\"][\"Ts\"]\n",
    "    max_T = obs_data[\"config\"][\"max_T\"]\n",
    "    thetas = obs_data[\"config\"][\"thetas\"]\n",
    "    \n",
    "    # =========================================================================\n",
    "    # VALIDATE AND PROCESS fitting_Ts\n",
    "    # =========================================================================\n",
    "    \n",
    "    if fitting_Ts is None:\n",
    "        Ts = config_Ts\n",
    "    else:\n",
    "        if not isinstance(fitting_Ts, (list, np.ndarray)):\n",
    "            raise TypeError(\"fitting_Ts must be a list of integers or None\")\n",
    "        \n",
    "        fitting_Ts = list(fitting_Ts)\n",
    "        \n",
    "        if len(fitting_Ts) == 0:\n",
    "            raise ValueError(\"fitting_Ts cannot be empty\")\n",
    "        \n",
    "        for T in fitting_Ts:\n",
    "            if not isinstance(T, (int, np.integer)):\n",
    "                raise ValueError(f\"All values in fitting_Ts must be integers\")\n",
    "        \n",
    "        fitting_Ts = sorted(set(int(T) for T in fitting_Ts))\n",
    "        \n",
    "        invalid_Ts = [T for T in fitting_Ts if T not in config_Ts]\n",
    "        if invalid_Ts:\n",
    "            raise ValueError(\n",
    "                f\"fitting_Ts contains values not in config Ts. \"\n",
    "                f\"Invalid: {invalid_Ts}. Available: {config_Ts}\"\n",
    "            )\n",
    "        \n",
    "        Ts = fitting_Ts\n",
    "    \n",
    "    invalid_Ts = [T for T in Ts if T < 1 or T > max_T]\n",
    "    if invalid_Ts:\n",
    "        raise ValueError(f\"All T must satisfy 1 <= T <= {max_T}. Invalid: {invalid_Ts}\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # FLATTEN ALL UTTERANCE SEQUENCES WITH LOCATION TRACKING\n",
    "    # =========================================================================\n",
    "    \n",
    "    flat_data = []\n",
    "    skipped_combinations = set()\n",
    "    \n",
    "    for theta in thetas:\n",
    "        for obs_list_pos, obs_info in enumerate(obs_data[\"observations\"][theta]):\n",
    "            obs_seq = obs_info[\"obs_seq\"]\n",
    "            obs_idx = obs_info[\"obs_idx\"]\n",
    "            \n",
    "            if len(obs_seq) != max_T:\n",
    "                raise ValueError(\n",
    "                    f\"Observation sequence length mismatch at theta={theta}, obs_idx={obs_idx}\"\n",
    "                )\n",
    "            \n",
    "            if obs_info[\"utterances\"] is None:\n",
    "                continue\n",
    "            \n",
    "            for speaker_key, alpha_dict in obs_info[\"utterances\"].items():\n",
    "                if target_speaker_keys is not None and speaker_key not in target_speaker_keys:\n",
    "                    skipped_combinations.add(f\"speaker_key={speaker_key}\")\n",
    "                    continue\n",
    "                \n",
    "                for alpha_key, utt_records in alpha_dict.items():\n",
    "                    if target_alpha_keys is not None and alpha_key not in target_alpha_keys:\n",
    "                        skipped_combinations.add(f\"alpha_key={alpha_key}\")\n",
    "                        continue\n",
    "                    \n",
    "                    for utt_list_idx, utt_rec in enumerate(utt_records):\n",
    "                        utt_seq = utt_rec[\"utt_seq\"]\n",
    "                        \n",
    "                        if len(utt_seq) != max_T:\n",
    "                            raise ValueError(\n",
    "                                f\"Utterance sequence length mismatch at theta={theta}, \"\n",
    "                                f\"obs_idx={obs_idx}, speaker_key={speaker_key}\"\n",
    "                            )\n",
    "                        \n",
    "                        flat_data.append({\n",
    "                            \"obs_seq\": obs_seq,\n",
    "                            \"utt_seq\": utt_seq,\n",
    "                            \"location\": (theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx)\n",
    "                        })\n",
    "    \n",
    "    # =========================================================================\n",
    "    # VERBOSE OUTPUT\n",
    "    # =========================================================================\n",
    "    \n",
    "    n_total = len(flat_data)\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"Dynamic pragmatic speaker fitting (psi={fitting_psi}, method={method}):\")\n",
    "        print(f\"  Storage key: '{fitted_key}'\")\n",
    "        print(f\"  Processing {n_total} utterance sequences\")\n",
    "        print(f\"  Ts: {Ts}\")\n",
    "        \n",
    "        if method == \"grid\":\n",
    "            n_alphas = n_grid + (1 if include_determ else 0)\n",
    "            print(f\"  Grid: {n_grid} points, spacing={grid_spacing}, bounds={alpha_bounds}\")\n",
    "        else:\n",
    "            print(f\"  Scipy: {n_total * len(Ts)} optimizations\")\n",
    "        \n",
    "        if n_jobs != 1:\n",
    "\n",
    "            n_workers = (cpu_count() if n_jobs == -1 \n",
    "                        else max(1, cpu_count() + 1 + n_jobs) if n_jobs < 0 \n",
    "                        else max(1, n_jobs))\n",
    "            print(f\"  Parallel: {n_workers} workers\")\n",
    "    \n",
    "    # =========================================================================\n",
    "    # EARLY EXIT\n",
    "    # =========================================================================\n",
    "    \n",
    "    if n_total == 0:\n",
    "        if verbose > 0:\n",
    "            print(\"No utterance sequences to process\")\n",
    "        return\n",
    "    \n",
    "    # =========================================================================\n",
    "    # METHOD-SPECIFIC COMPUTATION\n",
    "    # =========================================================================\n",
    "    \n",
    "    if method == \"grid\":\n",
    "        results = _dynamic_grid_search_multiT(\n",
    "            flat_data=flat_data,\n",
    "            world=world,\n",
    "            psi=fitting_psi,\n",
    "            Ts=Ts,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            grid_spacing=grid_spacing,\n",
    "            n_grid=n_grid,\n",
    "            include_determ=include_determ,\n",
    "            n_jobs=n_jobs,\n",
    "            backend=backend,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    else:\n",
    "        results = _dynamic_scipy_optimization_multiT(\n",
    "            flat_data=flat_data,\n",
    "            world=world,\n",
    "            psi=fitting_psi,\n",
    "            Ts=Ts,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            include_determ=include_determ,\n",
    "            n_jobs=n_jobs,\n",
    "            backend=backend,\n",
    "            verbose=verbose\n",
    "        )\n",
    "    \n",
    "    # =========================================================================\n",
    "    # DISTRIBUTE RESULTS BACK TO NESTED STRUCTURE\n",
    "    # =========================================================================\n",
    "    \n",
    "    for i, item in enumerate(flat_data):\n",
    "        theta, obs_list_pos, speaker_key, alpha_key, utt_list_idx = item[\"location\"]\n",
    "        \n",
    "        utt_rec = obs_data[\"observations\"][theta][obs_list_pos][\"utterances\"][speaker_key][alpha_key][utt_list_idx]\n",
    "        \n",
    "        if utt_rec[\"log_lik_all_speaker\"] is None:\n",
    "            utt_rec[\"log_lik_all_speaker\"] = {}\n",
    "        \n",
    "        if fitted_key in utt_rec[\"log_lik_all_speaker\"]:\n",
    "            existing = utt_rec[\"log_lik_all_speaker\"][fitted_key]\n",
    "            existing[\"max_log_lik\"].update(results[\"max_log_lik\"][i])\n",
    "            existing[\"optimal_alpha\"].update(results[\"optimal_alpha\"][i])\n",
    "        else:\n",
    "            utt_rec[\"log_lik_all_speaker\"][fitted_key] = {\n",
    "                \"max_log_lik\": results[\"max_log_lik\"][i],\n",
    "                \"optimal_alpha\": results[\"optimal_alpha\"][i]\n",
    "            }\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"Completed: stored results in log_lik_all_speaker['{fitted_key}']\")\n",
    "\n",
    "\n",
    "def _dynamic_evaluate_sequence_all_alphas(\n",
    "    obs_seq: List[Tuple[int, ...]],\n",
    "    utt_seq: List[str],\n",
    "    world: 'World',\n",
    "    psi: str,\n",
    "    alphas: List[Any],\n",
    "    Ts: List[int]\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Evaluate log-likelihoods for one sequence across all alphas and Ts.\n",
    "    \n",
    "    For update_internal=True, the listener's beliefs evolve based on observed\n",
    "    utterances. This evolution is alpha-independent, so we:\n",
    "    1. Walk through the sequence collecting utilities at each step\n",
    "    2. Apply softmax for all alphas at once\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    obs_seq : List[Tuple[int, ...]]\n",
    "        Observation sequence.\n",
    "    utt_seq : List[str]\n",
    "        Utterance sequence.\n",
    "    world : World\n",
    "        The World object.\n",
    "    psi : str\n",
    "        Speaker goal: \"inf\", \"pers+\", or \"pers-\".\n",
    "    alphas : List[Any]\n",
    "        List of alpha values (floats and/or \"determ\").\n",
    "    Ts : List[int]\n",
    "        Sequence lengths to compute likelihoods for.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Shape (n_alphas, n_Ts) log-likelihoods.\n",
    "    \"\"\"\n",
    "    \n",
    "    steps_needed = max(Ts)\n",
    "    n_alphas = len(alphas)\n",
    "    n_Ts = len(Ts)\n",
    "    T_indices = np.array(Ts) - 1\n",
    "    \n",
    "    # =========================================================================\n",
    "    # SEPARATE NUMERIC ALPHAS FROM \"determ\"\n",
    "    # =========================================================================\n",
    "    \n",
    "    numeric_indices = []\n",
    "    numeric_alphas = []\n",
    "    determ_idx = None\n",
    "    \n",
    "    for i, alpha in enumerate(alphas):\n",
    "        if alpha == \"determ\":\n",
    "            determ_idx = i\n",
    "        else:\n",
    "            numeric_indices.append(i)\n",
    "            numeric_alphas.append(float(alpha))\n",
    "    \n",
    "    has_numeric = len(numeric_alphas) > 0\n",
    "    has_determ = determ_idx is not None\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CREATE SPEAKER (alpha value doesn't matter for utility extraction)\n",
    "    # =========================================================================\n",
    "    \n",
    "    speaker = PragmaticSpeaker_obs(\n",
    "        world=world,\n",
    "        omega=\"strat\",\n",
    "        psi=psi,\n",
    "        update_internal=True,\n",
    "        alpha=1.0,\n",
    "        beta=0.0,\n",
    "        initial_beliefs_theta=None\n",
    "    )\n",
    "    \n",
    "    utterances = list(speaker.utility.index)\n",
    "    utt_to_idx = {u: i for i, u in enumerate(utterances)}\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COLLECT UTILITIES AT EACH TIME STEP\n",
    "    # =========================================================================\n",
    "    # Loop is unavoidable: listener state at t depends on utterances u_0...u_{t-1}\n",
    "    \n",
    "    utilities_list = []\n",
    "    \n",
    "    for t in range(steps_needed):\n",
    "        obs = obs_seq[t]\n",
    "        obs_key = tuple(obs) if not isinstance(obs, tuple) else obs\n",
    "        \n",
    "        # Extract utilities directly from speaker.utility DataFrame\n",
    "        utilities = speaker.utility[obs_key].values.copy()\n",
    "        utilities_list.append(utilities)\n",
    "        \n",
    "        # Update listener with observed utterance\n",
    "        utt = utt_seq[t]\n",
    "        speaker.literal_listener.listen_and_update(utt)\n",
    "        \n",
    "        # Recompute utility table for new listener state\n",
    "        speaker.utterance_log_prob_obs = speaker._compute_utterance_log_prob_obs(speaker.alpha)\n",
    "    \n",
    "    # Stack: shape (steps_needed, n_utterances)\n",
    "    utilities_matrix = np.stack(utilities_list)\n",
    "    \n",
    "    # Observed utterance indices: shape (steps_needed,)\n",
    "    utt_indices = np.array([utt_to_idx[utt_seq[t]] for t in range(steps_needed)])\n",
    "    \n",
    "    # =========================================================================\n",
    "    # COMPUTE LOG PROBABILITIES FOR ALL ALPHAS AND TIME STEPS\n",
    "    # =========================================================================\n",
    "    \n",
    "    all_log_probs = np.zeros((n_alphas, steps_needed))\n",
    "    \n",
    "    if has_numeric:\n",
    "        alphas_arr = np.array(numeric_alphas)\n",
    "        \n",
    "        # Scale utilities: α * U(u, O)\n",
    "        # Shape: (n_numeric, steps_needed, n_utterances)\n",
    "        scaled_utilities = (alphas_arr[:, np.newaxis, np.newaxis] * \n",
    "                          utilities_matrix[np.newaxis, :, :])\n",
    "        \n",
    "        # Log normalizers: logsumexp over utterances\n",
    "        # Shape: (n_numeric, steps_needed)\n",
    "        log_normalizers = logsumexp(scaled_utilities, axis=2)\n",
    "        \n",
    "        # Scaled utilities for observed utterances\n",
    "        # Shape: (n_numeric, steps_needed)\n",
    "        observed_scaled = scaled_utilities[:, np.arange(steps_needed), utt_indices]\n",
    "        \n",
    "        # Log P(u|O, α) = α*U(u) - logsumexp(α*U)\n",
    "        log_probs_numeric = observed_scaled - log_normalizers\n",
    "        \n",
    "        all_log_probs[np.array(numeric_indices), :] = log_probs_numeric\n",
    "    \n",
    "    # =========================================================================\n",
    "    # HANDLE DETERMINISTIC ALPHA\n",
    "    # =========================================================================\n",
    "    \n",
    "    if has_determ:\n",
    "        # Max utility at each step\n",
    "        max_utilities = np.max(utilities_matrix, axis=1)\n",
    "        \n",
    "        # Utility of observed utterance at each step\n",
    "        observed_utilities = utilities_matrix[np.arange(steps_needed), utt_indices]\n",
    "        \n",
    "        # Check if observed is among maxima\n",
    "        is_max = np.isclose(observed_utilities, max_utilities)\n",
    "        \n",
    "        # Count ties\n",
    "        is_max_all = np.isclose(utilities_matrix, max_utilities[:, np.newaxis])\n",
    "        n_ties = np.sum(is_max_all, axis=1)\n",
    "        \n",
    "        # Log prob: -log(n_ties) if max, -inf otherwise\n",
    "        determ_log_probs = np.where(is_max, -np.log(n_ties), -np.inf)\n",
    "        all_log_probs[determ_idx, :] = determ_log_probs\n",
    "    \n",
    "    # =========================================================================\n",
    "    # CUMULATIVE SUM AND EXTRACT FOR EACH T\n",
    "    # =========================================================================\n",
    "    \n",
    "    cumsum_log_probs = np.cumsum(all_log_probs, axis=1)\n",
    "    all_lls = cumsum_log_probs[:, T_indices]\n",
    "    \n",
    "    # Handle -inf propagation\n",
    "    has_neginf = np.isneginf(all_log_probs)\n",
    "    for t_idx, T in enumerate(Ts):\n",
    "        has_neginf_up_to_T = np.any(has_neginf[:, :T], axis=1)\n",
    "        all_lls[has_neginf_up_to_T, t_idx] = -np.inf\n",
    "    \n",
    "    return all_lls\n",
    "\n",
    "\n",
    "def _dynamic_grid_search_multiT(\n",
    "    flat_data: List[Dict[str, Any]],\n",
    "    world: 'World',\n",
    "    psi: str,\n",
    "    Ts: List[int],\n",
    "    alpha_bounds: Tuple[float, float],\n",
    "    grid_spacing: str,\n",
    "    n_grid: int,\n",
    "    include_determ: bool,\n",
    "    n_jobs: int,\n",
    "    backend: str,\n",
    "    verbose: int\n",
    ") -> Dict[str, List[Dict[int, Any]]]:\n",
    "    \"\"\"\n",
    "    Grid search for optimal alpha with dynamic speaker.\n",
    "    \n",
    "    Parallelizes over sequences.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with \"max_log_lik\" and \"optimal_alpha\" lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = len(flat_data)\n",
    "    n_Ts = len(Ts)\n",
    "    \n",
    "    # Create alpha grid\n",
    "    a_min, a_max = alpha_bounds\n",
    "    if grid_spacing == \"log\":\n",
    "        alphas = list(np.exp(np.linspace(np.log(a_min), np.log(a_max), n_grid)))\n",
    "    else:\n",
    "        alphas = list(np.linspace(a_min, a_max, n_grid))\n",
    "    \n",
    "    if include_determ:\n",
    "        alphas.append(\"determ\")\n",
    "    \n",
    "    alphas_array = np.array(alphas, dtype=object)\n",
    "    \n",
    "    # Worker function\n",
    "    def evaluate_single(item):\n",
    "        return _dynamic_evaluate_sequence_all_alphas(\n",
    "            obs_seq=item[\"obs_seq\"],\n",
    "            utt_seq=item[\"utt_seq\"],\n",
    "            world=world,\n",
    "            psi=psi,\n",
    "            alphas=alphas,\n",
    "            Ts=Ts\n",
    "        )\n",
    "    \n",
    "    # Execute\n",
    "    if n_jobs == 1:\n",
    "        all_results = [evaluate_single(item) for item in flat_data]\n",
    "    else:\n",
    "        all_results = Parallel(n_jobs=n_jobs, backend=backend, verbose=verbose)(\n",
    "            delayed(evaluate_single)(item) for item in flat_data\n",
    "        )\n",
    "    \n",
    "    # Stack: shape (n_total, n_alphas, n_Ts)\n",
    "    all_lls = np.array(all_results)\n",
    "    \n",
    "    # Find optimal alpha for each (sequence, T)\n",
    "    # Transpose to (n_alphas, n_total, n_Ts) for argmax\n",
    "    all_lls_T = all_lls.transpose(1, 0, 2)\n",
    "    best_alpha_indices = np.argmax(all_lls_T, axis=0)\n",
    "    \n",
    "    # Extract max log-likelihoods\n",
    "    row_idx = np.arange(n_total)[:, np.newaxis]\n",
    "    T_idx = np.arange(n_Ts)[np.newaxis, :]\n",
    "    max_lls = all_lls_T[best_alpha_indices, row_idx, T_idx]\n",
    "    \n",
    "    # Convert to list of dicts\n",
    "    max_log_lik_list = []\n",
    "    optimal_alpha_list = []\n",
    "    \n",
    "    for i in range(n_total):\n",
    "        max_log_lik_list.append({\n",
    "            T: float(max_lls[i, t_idx]) for t_idx, T in enumerate(Ts)\n",
    "        })\n",
    "        optimal_alpha_list.append({\n",
    "            T: alphas_array[best_alpha_indices[i, t_idx]] for t_idx, T in enumerate(Ts)\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        \"max_log_lik\": max_log_lik_list,\n",
    "        \"optimal_alpha\": optimal_alpha_list\n",
    "    }\n",
    "\n",
    "\n",
    "def _dynamic_scipy_optimization_multiT(\n",
    "    flat_data: List[Dict[str, Any]],\n",
    "    world: 'World',\n",
    "    psi: str,\n",
    "    Ts: List[int],\n",
    "    alpha_bounds: Tuple[float, float],\n",
    "    include_determ: bool,\n",
    "    n_jobs: int,\n",
    "    backend: str,\n",
    "    verbose: int\n",
    ") -> Dict[str, List[Dict[int, Any]]]:\n",
    "    \"\"\"\n",
    "    Scipy optimization for optimal alpha with dynamic speaker.\n",
    "    \n",
    "    Runs separate optimization for each (sequence, T) pair.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Dict with \"max_log_lik\" and \"optimal_alpha\" lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_total = len(flat_data)\n",
    "    \n",
    "    # Build tasks: one per (sequence, T)\n",
    "    tasks = []\n",
    "    for item_idx, item in enumerate(flat_data):\n",
    "        for T in Ts:\n",
    "            tasks.append({\n",
    "                \"item_idx\": item_idx,\n",
    "                \"T\": T,\n",
    "                \"obs_seq\": item[\"obs_seq\"][:T],\n",
    "                \"utt_seq\": item[\"utt_seq\"][:T]\n",
    "            })\n",
    "    \n",
    "    if verbose > 0:\n",
    "        print(f\"  Running {len(tasks)} scipy optimizations...\")\n",
    "    \n",
    "    # Worker function\n",
    "    def optimize_single(task):\n",
    "        base_config = {\n",
    "            \"speaker_type\": \"pragmatic\",\n",
    "            \"omega\": \"strat\",\n",
    "            \"psi\": psi,\n",
    "            \"update_internal\": True,\n",
    "            \"beta\": 0.0,\n",
    "            \"initial_beliefs_theta\": None\n",
    "        }\n",
    "        \n",
    "        result = log_likelihood_alpha_opt_utt_seq(\n",
    "            world=world,\n",
    "            obs_seq=task[\"obs_seq\"],\n",
    "            utt_seq=task[\"utt_seq\"],\n",
    "            speaker_config=base_config,\n",
    "            alpha_bounds=alpha_bounds,\n",
    "            grid_search=False,\n",
    "            include_determ=include_determ\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"item_idx\": task[\"item_idx\"],\n",
    "            \"T\": task[\"T\"],\n",
    "            \"optimal_alpha\": result[\"optimal_alpha\"],\n",
    "            \"max_log_lik\": result[\"max_log_likelihood\"]\n",
    "        }\n",
    "    \n",
    "    # Execute\n",
    "    if n_jobs == 1:\n",
    "        results = [optimize_single(task) for task in tasks]\n",
    "    else:\n",
    "        results = Parallel(n_jobs=n_jobs, backend=backend, verbose=verbose)(\n",
    "            delayed(optimize_single)(task) for task in tasks\n",
    "        )\n",
    "    \n",
    "    # Reorganize by item_idx\n",
    "    max_log_lik_list = [{} for _ in range(n_total)]\n",
    "    optimal_alpha_list = [{} for _ in range(n_total)]\n",
    "    \n",
    "    for res in results:\n",
    "        item_idx = res[\"item_idx\"]\n",
    "        T = res[\"T\"]\n",
    "        max_log_lik_list[item_idx][T] = res[\"max_log_lik\"]\n",
    "        optimal_alpha_list[item_idx][T] = res[\"optimal_alpha\"]\n",
    "    \n",
    "    return {\n",
    "        \"max_log_lik\": max_log_lik_list,\n",
    "        \"optimal_alpha\": optimal_alpha_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e6a544-9089-434d-bc82-0f5c7b3a4f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_DEMO =True\n",
    "if RUN_DEMO:\n",
    "    test_1_multiT = sample_observation_sequences_multiT(\n",
    "        n=5, m=5,\n",
    "        thetas=[0.3, 0.5],#[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "        Ts=[10, 11, 12, 13, 14, 15],\n",
    "        n_obs_seq=5,\n",
    "        random_seed=21,\n",
    "        compute_obs_likelihood=\"all\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    for true_speaker_config in TRUE_SPEAKER_CONFIGS:\n",
    "        \n",
    "        generate_utterances_for_observations_multiT(\n",
    "            obs_data=test_1_multiT,\n",
    "            speaker_config=true_speaker_config,\n",
    "            n_utt_seq=5,\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )\n",
    "    \n",
    "    fitting_psis = [\"inf\", \"pers+\", \"pers-\"]\n",
    "    \n",
    "    for fitting_psi in fitting_psis:\n",
    "        compute_pragmatic_dynamic_log_likelihood_multiT(\n",
    "            obs_data=test_1_multiT,\n",
    "            fitting_psi=fitting_psi,\n",
    "            method=\"scipy\",\n",
    "            n_jobs=-1,\n",
    "            verbose=2\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1230be-61e1-4ada-9ea7-da8ce7b3f565",
   "metadata": {},
   "source": [
    "# Test coarse screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d0c7ccd-083e-4226-afa7-646be9edad23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8fbab2a-4482-46d5-a249-025b7ea46db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9f244c-ae4a-4fae-933e-b00b95165abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: literal, alpha: 0.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: inf_F, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: inf_T, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: persp_F, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: persp_T, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: persm_F, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Generating utterances: 2 obs_seq × 45 utt_seq\n",
      "  Speaker: persm_T, alpha: 4.0, Ts: [5, 7, 10, 13, 15]\n",
      "  Workers: 1\n",
      "Processing 630 utterance sequences (2 unique obs positions) across all configurations\n",
      "Completed: stored results in log_lik_all_speaker['literal_fitted']\n",
      "Static pragmatic speaker fitting (psi=inf, method=grid):\n",
      "  Storage key: 'inf_F_fitted'\n",
      "  Processing 630 utterance sequences (2 unique obs)\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "  utility_table: (32, 252), 0.1 MB\n",
      "  all_utilities_per_step: (630, 15, 32), 2.3 MB\n",
      "  Processing alpha 1/101\n",
      "  Processing alpha 21/101\n",
      "  Processing alpha 41/101\n",
      "  Processing alpha 61/101\n",
      "  Processing alpha 81/101\n",
      "  Processing alpha 101/101\n",
      "Completed: stored results in log_lik_all_speaker['inf_F_fitted']\n",
      "Dynamic pragmatic speaker fitting (psi=inf, method=grid):\n",
      "  Storage key: 'inf_T_fitted'\n",
      "  Processing 630 utterance sequences\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "Completed: stored results in log_lik_all_speaker['inf_T_fitted']\n",
      "Static pragmatic speaker fitting (psi=pers+, method=grid):\n",
      "  Storage key: 'persp_F_fitted'\n",
      "  Processing 630 utterance sequences (2 unique obs)\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "  utility_table: (32, 252), 0.1 MB\n",
      "  all_utilities_per_step: (630, 15, 32), 2.3 MB\n",
      "  Processing alpha 1/101\n",
      "  Processing alpha 21/101\n",
      "  Processing alpha 41/101\n",
      "  Processing alpha 61/101\n",
      "  Processing alpha 81/101\n",
      "  Processing alpha 101/101\n",
      "Completed: stored results in log_lik_all_speaker['persp_F_fitted']\n",
      "Dynamic pragmatic speaker fitting (psi=pers+, method=grid):\n",
      "  Storage key: 'persp_T_fitted'\n",
      "  Processing 630 utterance sequences\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "Completed: stored results in log_lik_all_speaker['persp_T_fitted']\n",
      "Static pragmatic speaker fitting (psi=pers-, method=grid):\n",
      "  Storage key: 'persm_F_fitted'\n",
      "  Processing 630 utterance sequences (2 unique obs)\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "  utility_table: (32, 252), 0.1 MB\n",
      "  all_utilities_per_step: (630, 15, 32), 2.3 MB\n",
      "  Processing alpha 1/101\n",
      "  Processing alpha 21/101\n",
      "  Processing alpha 41/101\n",
      "  Processing alpha 61/101\n",
      "  Processing alpha 81/101\n",
      "  Processing alpha 101/101\n",
      "Completed: stored results in log_lik_all_speaker['persm_F_fitted']\n",
      "Dynamic pragmatic speaker fitting (psi=pers-, method=grid):\n",
      "  Storage key: 'persm_T_fitted'\n",
      "  Processing 630 utterance sequences\n",
      "  Ts: [5, 7, 10, 13, 15]\n",
      "  Grid: 100 points, spacing=log, bounds=(0.1, 50.0)\n",
      "Completed: stored results in log_lik_all_speaker['persm_T_fitted']\n"
     ]
    }
   ],
   "source": [
    "from rsa_optimal_exp_sampling_fun import (\n",
    "    sample_observation_sequences_multiT, generate_utterances_for_observations_multiT\n",
    ")\n",
    "\n",
    "from rsa_optimal_exp_fitting import (\n",
    "    compute_literal_log_likelihood_multiT, \n",
    "    compute_pragmatic_static_log_likelihood_multiT,\n",
    "    compute_pragmatic_dynamic_log_likelihood_multiT\n",
    ")\n",
    "\n",
    "\n",
    "N=5\n",
    "M=5\n",
    "Ts = [5, 7, 10, 13, 15]\n",
    "THETAs = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "SEED = 21\n",
    "N_OBS_SEQ = 1\n",
    "N_UTT_SEQ = 45\n",
    "VERBOSE = 3\n",
    "\n",
    "TRUE_SPEAKER_CONFIGS = [\n",
    "    {\n",
    "        \"speaker_type\": \"literal\"\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"inf\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers+\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": False\n",
    "    },\n",
    "    {\n",
    "        \"speaker_type\": \"pragmatic\",\n",
    "        \"omega\": \"strat\",\n",
    "        \"psi\": \"pers-\",\n",
    "        \"alpha\": 4.0,\n",
    "        \"update_internal\": True\n",
    "    },\n",
    "]\n",
    "\n",
    "FITTING_SPEAKER_PSIS = [\"inf\", \"pers+\", \"pers-\"]\n",
    "\n",
    "\n",
    "raw_data = sample_observation_sequences_multiT(\n",
    "    n=N, m=M,\n",
    "    thetas=THETAs,\n",
    "    Ts=Ts,\n",
    "    n_obs_seq=N_OBS_SEQ,\n",
    "    random_seed=SEED,\n",
    "    compute_obs_likelihood=\"all\"\n",
    ")\n",
    "\n",
    "\n",
    "for true_speaker_config in TRUE_SPEAKER_CONFIGS:\n",
    "    generate_utterances_for_observations_multiT(\n",
    "        obs_data=raw_data,\n",
    "        speaker_config=true_speaker_config,\n",
    "        n_utt_seq=N_UTT_SEQ,\n",
    "        n_jobs=-1,\n",
    "        verbose=VERBOSE\n",
    "    )\n",
    "\n",
    "\n",
    "compute_literal_log_likelihood_multiT(\n",
    "    obs_data=raw_data,\n",
    "    verbose=VERBOSE\n",
    ")\n",
    "\n",
    "\n",
    "for fitting_speaker_psi in FITTING_SPEAKER_PSIS:\n",
    "    \n",
    "    compute_pragmatic_static_log_likelihood_multiT(\n",
    "        obs_data=raw_data,\n",
    "        fitting_psi=fitting_speaker_psi,\n",
    "        n_jobs=1,\n",
    "        verbose=VERBOSE\n",
    "    )\n",
    "\n",
    "    compute_pragmatic_dynamic_log_likelihood_multiT(\n",
    "        obs_data=raw_data,\n",
    "        fitting_psi=fitting_speaker_psi,\n",
    "        n_jobs=-1,\n",
    "        verbose=VERBOSE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f661532d-22c8-4e15-b6a4-193fdc716e81",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa20300b-ba7b-4630-bbb7-cb55efaeaa52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved obs_data to /home/users/fangke/prag_net/optimal_design/obs_seqs_N5M6T15.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "# Define the output directory and file path\n",
    "output_dir = \"/home/users/fangke/prag_net/optimal_design\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save the dictionary using pickle\n",
    "pickle_file = os.path.join(output_dir, \"N5M5T15.pkl\")\n",
    "\n",
    "with open(pickle_file, 'wb') as f:\n",
    "    pickle.dump(raw_data, f)\n",
    "\n",
    "print(f\"Saved obs_data to {pickle_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5083e04-39c7-4297-9bc5-9454e7b9ff4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['utt_idx', 'utt_seq', 'utt_seed', 'log_lik_true_speaker', 'log_lik_all_speaker'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"observations\"][0.2][0][\"utterances\"][\"persp_T\"][4.0][1][\"log_lik_all_speaker\"][\"persm_T_fitted\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ef34db1-6b30-4003-a627-2488be42efce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0.1, 0.2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data[\"observations\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8d6ce01-f3fe-4e56-a483-5651675c9ef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "  RSA Experiment Stimuli Generator (Randomized Arrangements)\n",
      "  Configuration: 5 patients, 1 session\n",
      "  Using Twemoji images from CDN\n",
      "============================================================\n",
      "\n",
      "  Emojis:\n",
      "    Effective:   😃 (Twemoji: 1f603)\n",
      "    Ineffective: 🤒 (Twemoji: 1f912)\n",
      "\n",
      "  For 5 patients, generating ALL possible arrangements:\n",
      "    This creates C(5,k) images for each k effective patients\n",
      "Loading emoji images from Twemoji CDN...\n",
      "  😃 (effective)... ✓\n",
      "  🤒 (ineffective)... ✓\n",
      "\n",
      "Generating all arrangement stimuli in 'stimuli_emoji_n5m1/'...\n",
      "------------------------------------------------------------\n",
      "\n",
      "0/5 effective: 1 arrangement(s)\n",
      "  v0: 🤒🤒🤒🤒🤒 (effective at positions: none) → effective_0_v0.png\n",
      "\n",
      "1/5 effective: 5 arrangement(s)\n",
      "  v0: 😃🤒🤒🤒🤒 (effective at positions: 0) → effective_1_v0.png\n",
      "  v1: 🤒😃🤒🤒🤒 (effective at positions: 1) → effective_1_v1.png\n",
      "  v2: 🤒🤒😃🤒🤒 (effective at positions: 2) → effective_1_v2.png\n",
      "  v3: 🤒🤒🤒😃🤒 (effective at positions: 3) → effective_1_v3.png\n",
      "  v4: 🤒🤒🤒🤒😃 (effective at positions: 4) → effective_1_v4.png\n",
      "\n",
      "2/5 effective: 10 arrangement(s)\n",
      "  v0: 😃😃🤒🤒🤒 (effective at positions: 0,1) → effective_2_v0.png\n",
      "  v1: 😃🤒😃🤒🤒 (effective at positions: 0,2) → effective_2_v1.png\n",
      "  v2: 😃🤒🤒😃🤒 (effective at positions: 0,3) → effective_2_v2.png\n",
      "  v3: 😃🤒🤒🤒😃 (effective at positions: 0,4) → effective_2_v3.png\n",
      "  v4: 🤒😃😃🤒🤒 (effective at positions: 1,2) → effective_2_v4.png\n",
      "  v5: 🤒😃🤒😃🤒 (effective at positions: 1,3) → effective_2_v5.png\n",
      "  v6: 🤒😃🤒🤒😃 (effective at positions: 1,4) → effective_2_v6.png\n",
      "  v7: 🤒🤒😃😃🤒 (effective at positions: 2,3) → effective_2_v7.png\n",
      "  v8: 🤒🤒😃🤒😃 (effective at positions: 2,4) → effective_2_v8.png\n",
      "  v9: 🤒🤒🤒😃😃 (effective at positions: 3,4) → effective_2_v9.png\n",
      "\n",
      "3/5 effective: 10 arrangement(s)\n",
      "  v0: 😃😃😃🤒🤒 (effective at positions: 0,1,2) → effective_3_v0.png\n",
      "  v1: 😃😃🤒😃🤒 (effective at positions: 0,1,3) → effective_3_v1.png\n",
      "  v2: 😃😃🤒🤒😃 (effective at positions: 0,1,4) → effective_3_v2.png\n",
      "  v3: 😃🤒😃😃🤒 (effective at positions: 0,2,3) → effective_3_v3.png\n",
      "  v4: 😃🤒😃🤒😃 (effective at positions: 0,2,4) → effective_3_v4.png\n",
      "  v5: 😃🤒🤒😃😃 (effective at positions: 0,3,4) → effective_3_v5.png\n",
      "  v6: 🤒😃😃😃🤒 (effective at positions: 1,2,3) → effective_3_v6.png\n",
      "  v7: 🤒😃😃🤒😃 (effective at positions: 1,2,4) → effective_3_v7.png\n",
      "  v8: 🤒😃🤒😃😃 (effective at positions: 1,3,4) → effective_3_v8.png\n",
      "  v9: 🤒🤒😃😃😃 (effective at positions: 2,3,4) → effective_3_v9.png\n",
      "\n",
      "4/5 effective: 5 arrangement(s)\n",
      "  v0: 😃😃😃😃🤒 (effective at positions: 0,1,2,3) → effective_4_v0.png\n",
      "  v1: 😃😃😃🤒😃 (effective at positions: 0,1,2,4) → effective_4_v1.png\n",
      "  v2: 😃😃🤒😃😃 (effective at positions: 0,1,3,4) → effective_4_v2.png\n",
      "  v3: 😃🤒😃😃😃 (effective at positions: 0,2,3,4) → effective_4_v3.png\n",
      "  v4: 🤒😃😃😃😃 (effective at positions: 1,2,3,4) → effective_4_v4.png\n",
      "\n",
      "5/5 effective: 1 arrangement(s)\n",
      "  v0: 😃😃😃😃😃 (effective at positions: 0,1,2,3,4) → effective_5_v0.png\n",
      "\n",
      "------------------------------------------------------------\n",
      "✓ Done! Generated 32 images.\n",
      "\n",
      "Summary of arrangements per effectiveness level:\n",
      "  0 effective: 1 variants\n",
      "  1 effective: 5 variants\n",
      "  2 effective: 10 variants\n",
      "  3 effective: 10 variants\n",
      "  4 effective: 5 variants\n",
      "  5 effective: 1 variants\n",
      "\n",
      "============================================================\n",
      "JavaScript configuration for stimuli.js:\n",
      "============================================================\n",
      "\n",
      "// Arrangement data: maps numEffective to list of variant indices\n",
      "const ARRANGEMENT_COUNTS = {\n",
      "    0: 1,   // C(5,0) = 1\n",
      "    1: 5,   // C(5,1) = 5\n",
      "    2: 10,  // C(5,2) = 10\n",
      "    3: 10,  // C(5,3) = 10\n",
      "    4: 5,   // C(5,4) = 5\n",
      "    5: 1    // C(5,5) = 1\n",
      "};\n",
      "\n",
      "// Total: 32 images\n",
      "\n",
      "\n",
      "// Detailed arrangement mappings (positions of effective patients):\n",
      "const ARRANGEMENTS = {\n",
      "    0: [[]],\n",
      "    1: [[0], [1], [2], [3], [4]],\n",
      "    2: [[0, 1], [0, 2], [0, 3], [0, 4], [1, 2], [1, 3], [1, 4], [2, 3], [2, 4], [3, 4]],\n",
      "    3: [[0, 1, 2], [0, 1, 3], [0, 1, 4], [0, 2, 3], [0, 2, 4], [0, 3, 4], [1, 2, 3], [1, 2, 4], [1, 3, 4], [2, 3, 4]],\n",
      "    4: [[0, 1, 2, 3], [0, 1, 2, 4], [0, 1, 3, 4], [0, 2, 3, 4], [1, 2, 3, 4]],\n",
      "    5: [[0, 1, 2, 3, 4]],\n",
      "};\n",
      "\n",
      "============================================================\n",
      "  Cache location: /Users/kangke/.cache/rsa_stimuli/emoji\n",
      "  Done! Images created with Twemoji and randomized arrangements.\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Stimuli Generator for RSA Human Experiment (N=5, M=1 Version)\n",
    "UPDATED: Generates ALL possible arrangements for each effectiveness level.\n",
    "Uses Twemoji images from CDN for consistent, high-quality emoji display.\n",
    "\n",
    "For 5 patients with k effective:\n",
    "- k=0: 1 arrangement  (C(5,0) = 1)\n",
    "- k=1: 5 arrangements (C(5,1) = 5)\n",
    "- k=2: 10 arrangements (C(5,2) = 10)\n",
    "- k=3: 10 arrangements (C(5,3) = 10)\n",
    "- k=4: 5 arrangements (C(5,4) = 5)\n",
    "- k=5: 1 arrangement  (C(5,5) = 1)\n",
    "Total: 32 images\n",
    "\n",
    "Output folder: stimuli_emoji_n5m1/\n",
    "Naming: effective_{k}_v{variant}.png (e.g., effective_3_v0.png through effective_3_v9.png)\n",
    "\"\"\"\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "from itertools import combinations\n",
    "import urllib.request\n",
    "\n",
    "# Configuration\n",
    "N_PATIENTS = 5\n",
    "\n",
    "# Emoji Unicode code points for Twemoji URLs\n",
    "EMOJI_CONFIG = {\n",
    "    \"effective\": {\n",
    "        \"char\": \"😃\",\n",
    "        \"codepoint\": \"1f603\",  # Smiling face with open mouth\n",
    "    },\n",
    "    \"ineffective\": {\n",
    "        \"char\": \"🤒\",\n",
    "        \"codepoint\": \"1f912\",  # Face with thermometer\n",
    "    }\n",
    "}\n",
    "\n",
    "TWEMOJI_URL = \"https://cdn.jsdelivr.net/gh/twitter/twemoji@latest/assets/72x72/{codepoint}.png\"\n",
    "\n",
    "\n",
    "def get_cache_dir() -> Path:\n",
    "    \"\"\"Get or create the emoji cache directory.\"\"\"\n",
    "    cache_dir = Path.home() / \".cache\" / \"rsa_stimuli\" / \"emoji\"\n",
    "    cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "def download_emoji(codepoint: str, cache_dir: Path) -> Path:\n",
    "    \"\"\"Download an emoji PNG from Twemoji CDN.\"\"\"\n",
    "    cache_path = cache_dir / f\"{codepoint}.png\"\n",
    "    \n",
    "    if cache_path.exists():\n",
    "        return cache_path\n",
    "    \n",
    "    url = TWEMOJI_URL.format(codepoint=codepoint)\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, cache_path)\n",
    "        return cache_path\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to download emoji {codepoint}: {e}\\nURL: {url}\")\n",
    "\n",
    "\n",
    "def load_emoji_images(size: int = 72) -> Dict[str, Image.Image]:\n",
    "    \"\"\"Download and load all required emoji images.\"\"\"\n",
    "    cache_dir = get_cache_dir()\n",
    "    emojis = {}\n",
    "    \n",
    "    print(\"Loading emoji images from Twemoji CDN...\")\n",
    "    for name, config in EMOJI_CONFIG.items():\n",
    "        print(f\"  {config['char']} ({name})...\", end=\" \")\n",
    "        png_path = download_emoji(config[\"codepoint\"], cache_dir)\n",
    "        img = Image.open(png_path).convert(\"RGBA\")\n",
    "        img = img.resize((size, size), Image.Resampling.LANCZOS)\n",
    "        emojis[name] = img\n",
    "        print(\"✓\")\n",
    "    \n",
    "    return emojis\n",
    "\n",
    "\n",
    "def get_all_arrangements(num_effective: int) -> List[Tuple[int, ...]]:\n",
    "    \"\"\"\n",
    "    Get all possible arrangements of effective/ineffective patients.\n",
    "    \n",
    "    Returns a list of tuples, where each tuple contains the positions (0-4)\n",
    "    of effective patients.\n",
    "    \n",
    "    Example for num_effective=2:\n",
    "    [(0,1), (0,2), (0,3), (0,4), (1,2), (1,3), (1,4), (2,3), (2,4), (3,4)]\n",
    "    \"\"\"\n",
    "    return list(combinations(range(N_PATIENTS), num_effective))\n",
    "\n",
    "\n",
    "def create_stimuli_image(\n",
    "    effective_positions: Tuple[int, ...],\n",
    "    emoji_images: Dict[str, Image.Image],\n",
    "    emoji_size: int = 72,\n",
    "    padding: int = 10,\n",
    "    output_path: str = None\n",
    ") -> Image.Image:\n",
    "    \"\"\"\n",
    "    Create a row of 5 emojis with specified positions being effective.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    effective_positions : Tuple[int, ...]\n",
    "        Tuple of positions (0-4) that should show effective (happy) faces.\n",
    "        All other positions show ineffective (sick) faces.\n",
    "    emoji_images : Dict[str, Image.Image]\n",
    "        Pre-loaded emoji images\n",
    "    emoji_size : int\n",
    "        Size of each emoji\n",
    "    padding : int\n",
    "        Padding between emojis\n",
    "    output_path : str\n",
    "        If provided, save the image to this path\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Image.Image\n",
    "        The PIL Image object\n",
    "    \"\"\"\n",
    "    # Calculate dimensions\n",
    "    width = N_PATIENTS * emoji_size + (N_PATIENTS + 1) * padding\n",
    "    height = emoji_size + 2 * padding\n",
    "    \n",
    "    # Create image with white background\n",
    "    img = Image.new('RGBA', (width, height), color='#FFFFFF')\n",
    "    \n",
    "    # Place emojis based on effective_positions\n",
    "    effective_set = set(effective_positions)\n",
    "    for i in range(N_PATIENTS):\n",
    "        emoji_key = \"effective\" if i in effective_set else \"ineffective\"\n",
    "        emoji_img = emoji_images[emoji_key]\n",
    "        \n",
    "        x = padding + i * (emoji_size + padding)\n",
    "        y = padding\n",
    "        \n",
    "        img.paste(emoji_img, (x, y), emoji_img)\n",
    "    \n",
    "    # Convert to RGB\n",
    "    img_rgb = Image.new('RGB', img.size, '#FFFFFF')\n",
    "    img_rgb.paste(img, mask=img.split()[3] if img.mode == 'RGBA' else None)\n",
    "    \n",
    "    if output_path:\n",
    "        img_rgb.save(output_path, quality=95)\n",
    "    \n",
    "    return img_rgb\n",
    "\n",
    "\n",
    "def positions_to_visual(effective_positions: Tuple[int, ...]) -> str:\n",
    "    \"\"\"Convert positions to emoji visual representation.\"\"\"\n",
    "    result = []\n",
    "    effective_set = set(effective_positions)\n",
    "    for i in range(N_PATIENTS):\n",
    "        result.append(\"😃\" if i in effective_set else \"🤒\")\n",
    "    return \"\".join(result)\n",
    "\n",
    "\n",
    "def generate_all_stimuli(output_dir: str = \"stimuli_emoji_n5m1\", emoji_size: int = 72) -> None:\n",
    "    \"\"\"Generate stimuli images for all possible arrangements.\"\"\"\n",
    "    output_path = Path(output_dir)\n",
    "    output_path.mkdir(exist_ok=True)\n",
    "    \n",
    "    emoji_images = load_emoji_images(size=emoji_size)\n",
    "    \n",
    "    print(f\"\\nGenerating all arrangement stimuli in '{output_dir}/'...\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    total_images = 0\n",
    "    arrangement_counts = {}\n",
    "    \n",
    "    for num_effective in range(N_PATIENTS + 1):\n",
    "        arrangements = get_all_arrangements(num_effective)\n",
    "        arrangement_counts[num_effective] = len(arrangements)\n",
    "        \n",
    "        print(f\"\\n{num_effective}/5 effective: {len(arrangements)} arrangement(s)\")\n",
    "        \n",
    "        for variant_idx, positions in enumerate(arrangements):\n",
    "            filename = output_path / f\"effective_{num_effective}_v{variant_idx}.png\"\n",
    "            \n",
    "            create_stimuli_image(\n",
    "                effective_positions=positions,\n",
    "                emoji_images=emoji_images,\n",
    "                emoji_size=emoji_size,\n",
    "                output_path=str(filename)\n",
    "            )\n",
    "            \n",
    "            visual = positions_to_visual(positions)\n",
    "            positions_str = \",\".join(map(str, positions)) if positions else \"none\"\n",
    "            print(f\"  v{variant_idx}: {visual} (effective at positions: {positions_str}) → {filename.name}\")\n",
    "            total_images += 1\n",
    "    \n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"✓ Done! Generated {total_images} images.\")\n",
    "    print(\"\\nSummary of arrangements per effectiveness level:\")\n",
    "    for k, count in arrangement_counts.items():\n",
    "        print(f\"  {k} effective: {count} variants\")\n",
    "\n",
    "\n",
    "def generate_arrangement_map() -> Dict[int, List[Tuple[int, ...]]]:\n",
    "    \"\"\"\n",
    "    Generate a mapping from num_effective to all possible arrangements.\n",
    "    Useful for verification and JavaScript code generation.\n",
    "    \"\"\"\n",
    "    return {k: get_all_arrangements(k) for k in range(N_PATIENTS + 1)}\n",
    "\n",
    "\n",
    "def print_javascript_config():\n",
    "    \"\"\"Print JavaScript configuration for stimuli.js\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"JavaScript configuration for stimuli.js:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\"\"\n",
    "// Arrangement data: maps numEffective to list of variant indices\n",
    "const ARRANGEMENT_COUNTS = {\n",
    "    0: 1,   // C(5,0) = 1\n",
    "    1: 5,   // C(5,1) = 5\n",
    "    2: 10,  // C(5,2) = 10\n",
    "    3: 10,  // C(5,3) = 10\n",
    "    4: 5,   // C(5,4) = 5\n",
    "    5: 1    // C(5,5) = 1\n",
    "};\n",
    "\n",
    "// Total: 32 images\n",
    "\"\"\")\n",
    "    \n",
    "    print(\"\\n// Detailed arrangement mappings (positions of effective patients):\")\n",
    "    print(\"const ARRANGEMENTS = {\")\n",
    "    for k in range(N_PATIENTS + 1):\n",
    "        arrangements = get_all_arrangements(k)\n",
    "        arr_str = \", \".join([str(list(a)) for a in arrangements])\n",
    "        print(f\"    {k}: [{arr_str}],\")\n",
    "    print(\"};\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main entry point.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"  RSA Experiment Stimuli Generator (Randomized Arrangements)\")\n",
    "    print(f\"  Configuration: {N_PATIENTS} patients, 1 session\")\n",
    "    print(\"  Using Twemoji images from CDN\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\n  Emojis:\")\n",
    "    print(f\"    Effective:   😃 (Twemoji: {EMOJI_CONFIG['effective']['codepoint']})\")\n",
    "    print(f\"    Ineffective: 🤒 (Twemoji: {EMOJI_CONFIG['ineffective']['codepoint']})\")\n",
    "    \n",
    "    print(f\"\\n  For {N_PATIENTS} patients, generating ALL possible arrangements:\")\n",
    "    print(f\"    This creates C(5,k) images for each k effective patients\")\n",
    "    \n",
    "    generate_all_stimuli(output_dir=\"stimuli_emoji_n5m1\")\n",
    "    \n",
    "    print_javascript_config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"  Cache location: \" + str(get_cache_dir()))\n",
    "    print(\"  Done! Images created with Twemoji and randomized arrangements.\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "programming-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
