{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63ac539",
   "metadata": {},
   "source": [
    "# RSA Experiment Data Processing\n",
    "\n",
    "This notebook processes individual participant CSV files from DataPipe/OSF \n",
    "and creates a single wide-format dataframe with one row per participant.\n",
    "\n",
    "**Works with both pilot data and main study data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429595b",
   "metadata": {},
   "source": [
    "## Cell 1: Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1a4101b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def parse_json_safe(s):\n",
    "    \"\"\"Safely parse JSON string, return None if fails.\"\"\"\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    try:\n",
    "        return json.loads(s)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def calculate_duration_minutes(start_time, end_time):\n",
    "    \"\"\"Calculate duration in minutes between two ISO timestamps.\"\"\"\n",
    "    try:\n",
    "        start = datetime.fromisoformat(start_time.replace('Z', '+00:00'))\n",
    "        end = datetime.fromisoformat(end_time.replace('Z', '+00:00'))\n",
    "        return (end - start).total_seconds() / 60\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def scenario_to_short(scenario):\n",
    "    \"\"\"Convert scenario name to short form.\"\"\"\n",
    "    mapping = {\n",
    "        'informative': 'inf',\n",
    "        'pers_plus': 'persp',\n",
    "        'pers_minus': 'persm'\n",
    "    }\n",
    "    return mapping.get(scenario, scenario)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b674f",
   "metadata": {},
   "source": [
    "## Cell 2: Main Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011460cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_participant(filepath):\n",
    "    \"\"\"\n",
    "    Process a single participant's CSV file and extract key data.\n",
    "    \n",
    "    Works with both pilot data (attention checks have stimulus info)\n",
    "    and main study data (attention checks have no stimulus).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    filepath : str or Path\n",
    "        Path to the participant's CSV file\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    dict or None\n",
    "        Dictionary with all relevant data for this participant\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "    except Exception as e:\n",
    "        print(f\"  Error reading {filepath}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    if len(df) == 0:\n",
    "        print(f\"  Empty file: {filepath}\")\n",
    "        return None\n",
    "    \n",
    "    result = {}\n",
    "    first_row = df.iloc[0]\n",
    "    \n",
    "    # === METADATA ===\n",
    "    result['subject_id'] = first_row.get('subject_id', '')\n",
    "    result['prolific_pid'] = first_row.get('prolific_pid', '')\n",
    "    result['study_id'] = first_row.get('study_id', '')\n",
    "    result['session_id'] = first_row.get('session_id', '')\n",
    "    result['experiment_version'] = first_row.get('experiment_version', '')\n",
    "    result['start_time'] = first_row.get('start_time', '')\n",
    "    result['completion_status'] = first_row.get('completion_status', '')\n",
    "    result['completion_time'] = first_row.get('completion_time', '')\n",
    "    result['terminated_early'] = first_row.get('terminated_early', False)\n",
    "    result['termination_reason'] = first_row.get('termination_reason', '')\n",
    "    result['duration_minutes'] = calculate_duration_minutes(\n",
    "        str(result['start_time']), str(result['completion_time']))\n",
    "    result['total_time_elapsed_ms'] = df['time_elapsed'].max()\n",
    "    \n",
    "    # === BLOCK ORDER ===\n",
    "    scenarios_by_block = {}\n",
    "    role_comp = df[df['task'] == 'role_comprehension'].sort_values('block')\n",
    "    for _, row in role_comp.iterrows():\n",
    "        block = int(row.get('block', -1))\n",
    "        scenario = row.get('scenario', '')\n",
    "        if block >= 0 and scenario:\n",
    "            scenarios_by_block[block] = scenario\n",
    "    \n",
    "    speaker = df[df['task'] == 'speaker']\n",
    "    if len(speaker) > 0:\n",
    "        for block in speaker['block'].dropna().unique():\n",
    "            block_int = int(block)\n",
    "            if block_int not in scenarios_by_block:\n",
    "                scenarios_by_block[block_int] = speaker[speaker['block'] == block]['scenario'].iloc[0]\n",
    "    \n",
    "    result['block_1_scenario'] = scenarios_by_block.get(0, '')\n",
    "    result['block_2_scenario'] = scenarios_by_block.get(1, '')\n",
    "    result['block_3_scenario'] = scenarios_by_block.get(2, '')\n",
    "    block_order = [scenario_to_short(scenarios_by_block.get(i, '')) for i in range(3) if scenarios_by_block.get(i, '')]\n",
    "    result['block_order'] = '_'.join(block_order)\n",
    "    \n",
    "    # === ATTENTION CHECKS ===\n",
    "    attn = df[df['task'] == 'attention_check'].sort_values('block')\n",
    "    result['attention_total_failures'] = 0\n",
    "    for _, row in attn.iterrows():\n",
    "        block = int(row.get('block', 0))\n",
    "        result[f'attention_block_{block+1}_passed'] = row.get('attention_passed', None)\n",
    "        result[f'attention_block_{block+1}_round'] = row.get('round', None)\n",
    "        result[f'attention_block_{block+1}_time_elapsed'] = row.get('time_elapsed', None)\n",
    "        # These will be real values for pilot, NaN for main study\n",
    "        result[f'attention_block_{block+1}_num_effective'] = row.get('num_effective', None)\n",
    "        result[f'attention_block_{block+1}_stimulus_variant'] = row.get('stimulus_variant', None)\n",
    "        result[f'attention_block_{block+1}_required_description'] = row.get('required_description', '')\n",
    "        failures = row.get('total_failures', 0)\n",
    "        if pd.notna(failures):\n",
    "            result['attention_total_failures'] = max(result['attention_total_failures'], int(failures))\n",
    "    for i in range(1, 4):\n",
    "        if f'attention_block_{i}_passed' not in result:\n",
    "            result[f'attention_block_{i}_passed'] = None\n",
    "            result[f'attention_block_{i}_round'] = None\n",
    "            result[f'attention_block_{i}_time_elapsed'] = None\n",
    "            result[f'attention_block_{i}_num_effective'] = None\n",
    "            result[f'attention_block_{i}_stimulus_variant'] = None\n",
    "            result[f'attention_block_{i}_required_description'] = ''\n",
    "    \n",
    "    # === COMPREHENSION MODULE 1 ===\n",
    "    comp1_some = df[df['task'] == 'comp1_some']\n",
    "    if len(comp1_some) > 0:\n",
    "        row = comp1_some.iloc[0]\n",
    "        result['comp1_some_correct'] = row.get('comp1_some_correct', None)\n",
    "        result['comp1_some_rt'] = row.get('rt', None)\n",
    "        resp = parse_json_safe(row.get('response', ''))\n",
    "        result['comp1_some_response'] = resp.get('some_def', '') if resp else ''\n",
    "    else:\n",
    "        result['comp1_some_correct'] = result['comp1_some_rt'] = None\n",
    "        result['comp1_some_response'] = ''\n",
    "    \n",
    "    comp1_most = df[df['task'] == 'comp1_most']\n",
    "    if len(comp1_most) > 0:\n",
    "        row = comp1_most.iloc[0]\n",
    "        result['comp1_most_correct'] = row.get('comp1_most_correct', None)\n",
    "        result['comp1_most_rt'] = row.get('rt', None)\n",
    "        resp = parse_json_safe(row.get('response', ''))\n",
    "        result['comp1_most_response'] = resp.get('most_def', '') if resp else ''\n",
    "    else:\n",
    "        result['comp1_most_correct'] = result['comp1_most_rt'] = None\n",
    "        result['comp1_most_response'] = ''\n",
    "    \n",
    "    # === COMPREHENSION MODULE 2 ===\n",
    "    comp2 = df[df['task'] == 'comp2'].sort_values('item_index')\n",
    "    for i, (_, row) in enumerate(comp2.iterrows()):\n",
    "        idx = int(row.get('item_index', i))\n",
    "        result[f'comp2_{idx+1}_correct'] = row.get('comp2_correct', None)\n",
    "        result[f'comp2_{idx+1}_time_elapsed'] = row.get('time_elapsed', None)\n",
    "        item = parse_json_safe(row.get('item', ''))\n",
    "        if item:\n",
    "            result[f'comp2_{idx+1}_num_effective'] = item.get('numEffective', None)\n",
    "            result[f'comp2_{idx+1}_statement'] = item.get('statementPlain', '')\n",
    "    for i in range(1, 3):\n",
    "        if f'comp2_{i}_correct' not in result:\n",
    "            result[f'comp2_{i}_correct'] = None\n",
    "            result[f'comp2_{i}_time_elapsed'] = None\n",
    "            result[f'comp2_{i}_num_effective'] = None\n",
    "            result[f'comp2_{i}_statement'] = ''\n",
    "    \n",
    "    # === COMPREHENSION MODULE 3 ===\n",
    "    comp3 = df[df['task'] == 'comp3']\n",
    "    if len(comp3) > 0:\n",
    "        row = comp3.iloc[0]\n",
    "        result['comp3_correct'] = row.get('comp3_correct', None)\n",
    "        result['comp3_time_elapsed'] = row.get('time_elapsed', None)\n",
    "        selected = row.get('selected', '')\n",
    "        result['comp3_selected'] = selected if pd.notna(selected) else ''\n",
    "    else:\n",
    "        result['comp3_correct'] = None\n",
    "        result['comp3_time_elapsed'] = None\n",
    "        result['comp3_selected'] = ''\n",
    "    \n",
    "    # === ROLE COMPREHENSION ===\n",
    "    role_comp = df[df['task'] == 'role_comprehension']\n",
    "    for scenario in ['informative', 'pers_plus', 'pers_minus']:\n",
    "        scenario_data = role_comp[role_comp['scenario'] == scenario]\n",
    "        prefix = scenario_to_short(scenario)\n",
    "        if len(scenario_data) > 0:\n",
    "            row = scenario_data.iloc[0]\n",
    "            result[f'{prefix}_role_comp_correct'] = row.get('role_comp_correct', None)\n",
    "            result[f'{prefix}_role_comp_selected'] = row.get('selected_option', '')\n",
    "            result[f'{prefix}_role_comp_time_elapsed'] = row.get('time_elapsed', None)\n",
    "        else:\n",
    "            result[f'{prefix}_role_comp_correct'] = None\n",
    "            result[f'{prefix}_role_comp_selected'] = ''\n",
    "            result[f'{prefix}_role_comp_time_elapsed'] = None\n",
    "    \n",
    "    # === SPEAKER TRIALS ===\n",
    "    speaker = df[df['task'] == 'speaker']\n",
    "    result['n_speaker_trials'] = len(speaker)\n",
    "    \n",
    "    for scenario in ['informative', 'pers_plus', 'pers_minus']:\n",
    "        scenario_speaker = speaker[speaker['scenario'] == scenario].sort_values('round')\n",
    "        prefix = scenario_to_short(scenario)\n",
    "        result[f'{prefix}_n_trials'] = len(scenario_speaker)\n",
    "        \n",
    "        prev_time = None\n",
    "        for _, row in scenario_speaker.iterrows():\n",
    "            round_num = int(row.get('round', 1))\n",
    "            col_prefix = f'{prefix}_r{round_num}'\n",
    "            \n",
    "            result[f'{col_prefix}_num_effective'] = row.get('num_effective', None)\n",
    "            result[f'{col_prefix}_variant'] = row.get('stimulus_variant', None)\n",
    "            result[f'{col_prefix}_positions'] = row.get('stimulus_positions', '')\n",
    "            result[f'{col_prefix}_predicate'] = row.get('predicate', '')\n",
    "            result[f'{col_prefix}_quantifier'] = row.get('quantifier', '')\n",
    "            \n",
    "            current_time = row.get('time_elapsed', None)\n",
    "            result[f'{col_prefix}_time_elapsed'] = current_time\n",
    "            \n",
    "            if prev_time is not None and current_time is not None:\n",
    "                result[f'{col_prefix}_rt_approx'] = current_time - prev_time\n",
    "            else:\n",
    "                result[f'{col_prefix}_rt_approx'] = None\n",
    "            prev_time = current_time\n",
    "        \n",
    "        for r in range(1, 11):\n",
    "            col_prefix = f'{prefix}_r{r}'\n",
    "            if f'{col_prefix}_num_effective' not in result:\n",
    "                result[f'{col_prefix}_num_effective'] = None\n",
    "                result[f'{col_prefix}_variant'] = None\n",
    "                result[f'{col_prefix}_positions'] = ''\n",
    "                result[f'{col_prefix}_predicate'] = ''\n",
    "                result[f'{col_prefix}_quantifier'] = ''\n",
    "                result[f'{col_prefix}_time_elapsed'] = None\n",
    "                result[f'{col_prefix}_rt_approx'] = None\n",
    "    \n",
    "    # === FEEDBACK ===\n",
    "    feedback_rows = df[df['trial_type'] == 'survey-text']\n",
    "    if len(feedback_rows) > 0:\n",
    "        last_feedback = feedback_rows.iloc[-1]\n",
    "        resp = parse_json_safe(last_feedback.get('response', ''))\n",
    "        if resp and 'feedback' in resp:\n",
    "            result['feedback_text'] = resp['feedback']\n",
    "        else:\n",
    "            result['feedback_text'] = ''\n",
    "    else:\n",
    "        result['feedback_text'] = ''\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a79e284",
   "metadata": {},
   "source": [
    "## Cell 3: Batch Processing Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dbed7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_participants(input_path, verbose=True):\n",
    "    \"\"\"\n",
    "    Process all CSV files in a folder or a single CSV file.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_path : str or Path\n",
    "        Path to folder containing CSV files, or path to a single CSV file\n",
    "    verbose : bool\n",
    "        Whether to print progress\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Wide-format dataframe with one row per participant\n",
    "    \"\"\"\n",
    "    input_path = Path(input_path)\n",
    "    \n",
    "    if input_path.is_file():\n",
    "        csv_files = [input_path]\n",
    "    elif input_path.is_dir():\n",
    "        csv_files = list(input_path.glob('*.csv'))\n",
    "    else:\n",
    "        raise ValueError(f\"Path does not exist: {input_path}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Found {len(csv_files)} CSV file(s)\")\n",
    "    \n",
    "    all_data = []\n",
    "    for i, filepath in enumerate(csv_files):\n",
    "        if verbose:\n",
    "            print(f\"Processing {i+1}/{len(csv_files)}: {filepath.name}\")\n",
    "        \n",
    "        result = process_participant(filepath)\n",
    "        if result:\n",
    "            result['source_file'] = filepath.name\n",
    "            all_data.append(result)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"\\nSuccessfully processed {len(all_data)} participant(s)\")\n",
    "    \n",
    "    return pd.DataFrame(all_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59247b10",
   "metadata": {},
   "source": [
    "## Cell 4: Summary Statistics Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "667fa867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(df):\n",
    "    \"\"\"Print summary statistics for the processed data.\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\nTotal participants: {len(df)}\")\n",
    "    \n",
    "    print(f\"\\nCompletion status:\")\n",
    "    print(df['completion_status'].value_counts().to_string())\n",
    "    \n",
    "    completed = df[df['completion_status'] == 'completed']\n",
    "    if len(completed) > 0:\n",
    "        print(f\"\\nDuration (completed):\")\n",
    "        print(f\"  Mean: {completed['duration_minutes'].mean():.1f} min\")\n",
    "        print(f\"  Median: {completed['duration_minutes'].median():.1f} min\")\n",
    "        print(f\"  Range: {completed['duration_minutes'].min():.1f} - {completed['duration_minutes'].max():.1f} min\")\n",
    "    \n",
    "    print(f\"\\nBlock order distribution:\")\n",
    "    print(df['block_order'].value_counts().to_string())\n",
    "    \n",
    "    print(f\"\\nComprehension accuracy:\")\n",
    "    for col in ['comp1_some_correct', 'comp1_most_correct', 'comp2_1_correct', 'comp2_2_correct', 'comp3_correct']:\n",
    "        if col in df.columns:\n",
    "            valid = df[col].dropna()\n",
    "            if len(valid) > 0:\n",
    "                print(f\"  {col}: {valid.mean()*100:.1f}% (n={len(valid)})\")\n",
    "    \n",
    "    print(f\"\\nRole comprehension accuracy:\")\n",
    "    for col in ['inf_role_comp_correct', 'persp_role_comp_correct', 'persm_role_comp_correct']:\n",
    "        if col in df.columns:\n",
    "            valid = df[col].dropna()\n",
    "            if len(valid) > 0:\n",
    "                print(f\"  {col}: {valid.mean()*100:.1f}% (n={len(valid)})\")\n",
    "    \n",
    "    print(f\"\\nAttention check pass rate:\")\n",
    "    for col in ['attention_block_1_passed', 'attention_block_2_passed', 'attention_block_3_passed']:\n",
    "        if col in df.columns:\n",
    "            valid = df[col].dropna()\n",
    "            if len(valid) > 0:\n",
    "                print(f\"  {col}: {valid.mean()*100:.1f}% (n={len(valid)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e625f9dd",
   "metadata": {},
   "source": [
    "## Cell 5: Add Participant ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8738cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_participant_id(df):\n",
    "    \"\"\"Add participant_id column (P001, P002, etc.) at the beginning.\"\"\"\n",
    "    df = df.copy()\n",
    "    df.insert(0, 'participant_id', [f'P{i+1:03d}' for i in range(len(df))])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8706bfa1",
   "metadata": {},
   "source": [
    "## Cell 6: Run Processing\n",
    "\n",
    "**Option A: Process a folder of CSV files**\n",
    "```python\n",
    "df = process_all_participants('./raw_data/')\n",
    "```\n",
    "\n",
    "**Option B: Process a single CSV file**\n",
    "```python\n",
    "df = process_all_participants('./path/to/file.csv')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "620a15ff",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 CSV file(s)\n",
      "Processing 1/10: 5f1c2ebf1d8b7c48ad3ec3db.csv\n",
      "Processing 2/10: 64337cc5c12109dc730de8c6.csv\n",
      "Processing 3/10: 696bb3922216926553e87730.csv\n",
      "Processing 4/10: 58d4c4c287b2420001263b6a.csv\n",
      "Processing 5/10: 650b03136ab3d4c832d98b71.csv\n",
      "Processing 6/10: 671e5804035494e1d10359ca.csv\n",
      "Processing 7/10: 59f71f797086f80001941493.csv\n",
      "Processing 8/10: 695c063ad8348d9dbf6896cb.csv\n",
      "Processing 9/10: 627ebc31883c7dd7c2220c7e.csv\n",
      "Processing 10/10: 6713eeea64ca1fb59cbcbce4.csv\n",
      "\n",
      "Successfully processed 10 participant(s)\n",
      "Found 101 CSV file(s)\n",
      "Processing 1/101: 56b8de29e1d0a200051517f8.csv\n",
      "Processing 2/101: 66afcaa60f7d8f58dc21db8e.csv\n",
      "Processing 3/101: 66720d50c394601b811a0e49.csv\n",
      "Processing 4/101: 673feeafa80a279f5cc63e33.csv\n",
      "Processing 5/101: 5cf804f0c5cf0e00010c02d5.csv\n",
      "Processing 6/101: 66cc4ba25cc59f1c17f1a2b4.csv\n",
      "Processing 7/101: 67722f8e3a4f08a288a1f640.csv\n",
      "Processing 8/101: 6709224f70eacdb20761ae3c.csv\n",
      "Processing 9/101: 66e9236f775eae67be04229d.csv\n",
      "Processing 10/101: 63e55f08844f6fb08115851f.csv\n",
      "Processing 11/101: 66d804534578af1dec99f245.csv\n",
      "Processing 12/101: 695f9adb2064ccdd40b77cdd.csv\n",
      "Processing 13/101: 65031ea9cb2723bf741d4151.csv\n",
      "Processing 14/101: 558823c2fdf99b318cb4224b.csv\n",
      "Processing 15/101: 5db48b4d14dab8000de45fdf.csv\n",
      "Processing 16/101: 65de74dd26e4558c2fe828f9.csv\n",
      "Processing 17/101: 6650bfea11f11fd1b150a7ca.csv\n",
      "Processing 18/101: 66594968a8fca6ea85c9ac19.csv\n",
      "Processing 19/101: 60fce0e83002bd32a1a4cfa3.csv\n",
      "Processing 20/101: 55a430ddfdf99b02ff6caf22.csv\n",
      "Processing 21/101: 5b5c8d4e10aec90001ed4bee.csv\n",
      "Processing 22/101: 627a65a2439f59544115d476.csv\n",
      "Processing 23/101: 675f4880d357baddb38a9c2c.csv\n",
      "Processing 24/101: 671e55aeca0216c5762de35a.csv\n",
      "Processing 25/101: 64012ef400259d6b9f41d17e.csv\n",
      "Processing 26/101: 6562177a48308eebbb6359ea.csv\n",
      "Processing 27/101: 694163cd8dce97079e97bd07.csv\n",
      "Processing 28/101: 594a964c215cbd000146de75.csv\n",
      "Processing 29/101: 5fa4f94e535f3b1aa628e0ab.csv\n",
      "Processing 30/101: 5a50f10eeedc32000141fccd.csv\n",
      "Processing 31/101: 6838b21420f8652e717601ab.csv\n",
      "Processing 32/101: 5ff238852178d171f9c973b7.csv\n",
      "Processing 33/101: 67f302367b115d1edb94f0b0.csv\n",
      "Processing 34/101: 5c0b388b9c7b1100015ab55f.csv\n",
      "Processing 35/101: 66463ba8fac8591a6734db5b.csv\n",
      "Processing 36/101: 662d56e5cc343fb58134d622.csv\n",
      "Processing 37/101: 677ca730878568e38dae5234.csv\n",
      "Processing 38/101: 6946085fc4297ee282b32d5a.csv\n",
      "Processing 39/101: 696f3e8d1bc17ee7c02d4318.csv\n",
      "Processing 40/101: 6942f17a27841b72aa21b1d8.csv\n",
      "Processing 41/101: 5b012b19293d310001023590.csv\n",
      "Processing 42/101: 5c4af67cdccaa800016b3192.csv\n",
      "Processing 43/101: 5dd9923922749791b319fc19.csv\n",
      "Processing 44/101: 6631997e0554bb6d25062fd4.csv\n",
      "Processing 45/101: 67001b28a9974a03b6ffec54.csv\n",
      "Processing 46/101: 677ef41490c712fe28812078.csv\n",
      "Processing 47/101: 675de3bd64119cef93edfc75.csv\n",
      "Processing 48/101: 5685850c333cbd000d4e042f.csv\n",
      "Processing 49/101: 66f305037af69346a9a55b42.csv\n",
      "Processing 50/101: 62bdc30224534809032cf056.csv\n",
      "Processing 51/101: 675bb88bd8b8f63130d653ca.csv\n",
      "Processing 52/101: 696130e7ba178082592df860.csv\n",
      "Processing 53/101: 5deb638bff7f7126d6fb5142.csv\n",
      "Processing 54/101: 6111753718d92612abc8f41c.csv\n",
      "Processing 55/101: 67e70a61f963fecfc78db7ab.csv\n",
      "Processing 56/101: 663ea705b761e95caebf67eb.csv\n",
      "Processing 57/101: 695bd4f6bbdb5a636882ce78.csv\n",
      "Processing 58/101: 67094d6215b6d0fe3e8c3e39.csv\n",
      "Processing 59/101: 55dbdc9d50a1f7001190d753.csv\n",
      "Processing 60/101: 6941984defc3cfda7e9cdf6f.csv\n",
      "Processing 61/101: 60ac66a8e1bf5a1c51fa864c.csv\n",
      "Processing 62/101: 69718893ecd7455680ec4676.csv\n",
      "Processing 63/101: 55d5e2c9db6a810006fa078b.csv\n",
      "Processing 64/101: 66c0f464e0ff62798027b486.csv\n",
      "Processing 65/101: 67e268cbf13dcd115337894c.csv\n",
      "Processing 66/101: 66f1b4e2d875c6785ca1d50a.csv\n",
      "Processing 67/101: 668bf688d58206e61f399a88.csv\n",
      "Processing 68/101: 66efa54508a57ab4d42cab20.csv\n",
      "Processing 69/101: 653703627539f3a8b2ed4af3.csv\n",
      "Processing 70/101: 5d537c0cecc84200153075e9.csv\n",
      "Processing 71/101: 6679dc943f855ba978447c25.csv\n",
      "Processing 72/101: 6100372e3a116cb5cb36ebea.csv\n",
      "Processing 73/101: 60db024287fece5e042fc23b.csv\n",
      "Processing 74/101: 671f12c5d2543b67c0f12e57.csv\n",
      "Processing 75/101: 5e9e19d808b9931ef37373c6.csv\n",
      "Processing 76/101: 5e49da75ea0f3c4553a9e3b2.csv\n",
      "Processing 77/101: 678c89e7d72ecd785f475dfd.csv\n",
      "Processing 78/101: 67363452674f3db8059e7888.csv\n",
      "Processing 79/101: 695fe9944e704fd2353901e3.csv\n",
      "Processing 80/101: 628f83dc5398c04bac86f9e6.csv\n",
      "Processing 81/101: 66d8790772c9e0e4ee96259c.csv\n",
      "Processing 82/101: 675cb36b51ef22e70c4da7e2.csv\n",
      "Processing 83/101: 65e0ef4d9b7d8b65fc4ff800.csv\n",
      "Processing 84/101: 695be81cff5933e82a967d04.csv\n",
      "Processing 85/101: 5da640b9640c5d00133f384b.csv\n",
      "Processing 86/101: 5a8f13595292b80001235e09.csv\n",
      "Processing 87/101: 66295cf0dbecaeeb1cddb852.csv\n",
      "Processing 88/101: 65ccca52871269168cc029b5.csv\n",
      "Processing 89/101: 63ed15bed4665c62d9e6dbb2.csv\n",
      "Processing 90/101: 655f95ffd90905e27c938a49.csv\n",
      "Processing 91/101: 57c725d89a5f4f0001f661bd.csv\n",
      "Processing 92/101: 63d7b81d79044b73bfb50952.csv\n",
      "Processing 93/101: 5e87b4d3e28032100ce68eec.csv\n",
      "Processing 94/101: 663d8f0edfd87b4c1f590b99.csv\n",
      "Processing 95/101: 5f0a7dec44c42c19a6c9f6d9.csv\n",
      "Processing 96/101: 65293c257003399b27cc1eb5.csv\n",
      "Processing 97/101: 611b00c6f6cc82766cd07c16.csv\n",
      "Processing 98/101: 670e09973d94a59502e84783.csv\n",
      "Processing 99/101: 61647de75314468d5cb57f93.csv\n",
      "Processing 100/101: 559c3e07fdf99b32b55f2d8d.csv\n",
      "Processing 101/101: 6631449d82a3ec3e7608b189.csv\n",
      "\n",
      "Successfully processed 101 participant(s)\n",
      "============================================================\n",
      "DATA SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total participants: 111\n",
      "\n",
      "Completion status:\n",
      "completion_status\n",
      "completed                     109\n",
      "terminated_attention_check      1\n",
      "terminated_inactivity           1\n",
      "\n",
      "Duration (completed):\n",
      "  Mean: 12.3 min\n",
      "  Median: 11.1 min\n",
      "  Range: 5.0 - 38.7 min\n",
      "\n",
      "Block order distribution:\n",
      "block_order\n",
      "persm_persp_inf    26\n",
      "inf_persp_persm    23\n",
      "inf_persm_persp    19\n",
      "persp_inf_persm    16\n",
      "persp_persm_inf    14\n",
      "persm_inf_persp    11\n",
      "inf_persm           1\n",
      "inf                 1\n",
      "\n",
      "Comprehension accuracy:\n",
      "  comp1_some_correct: 39.6% (n=111)\n",
      "  comp1_most_correct: 66.7% (n=111)\n",
      "  comp2_1_correct: 95.5% (n=111)\n",
      "  comp2_2_correct: 96.4% (n=111)\n",
      "  comp3_correct: 62.2% (n=111)\n",
      "\n",
      "Role comprehension accuracy:\n",
      "  inf_role_comp_correct: 91.9% (n=111)\n",
      "  persp_role_comp_correct: 84.4% (n=109)\n",
      "  persm_role_comp_correct: 94.5% (n=110)\n",
      "\n",
      "Attention check pass rate:\n",
      "  attention_block_1_passed: 97.3% (n=110)\n",
      "  attention_block_2_passed: 96.4% (n=110)\n",
      "  attention_block_3_passed: 98.2% (n=109)\n",
      "\n",
      "--- Full data (first 5 columns) ---\n",
      "  participant_id                subject_id              prolific_pid  \\\n",
      "0           P001  5f1c2ebf1d8b7c48ad3ec3db  5f1c2ebf1d8b7c48ad3ec3db   \n",
      "1           P002  64337cc5c12109dc730de8c6  64337cc5c12109dc730de8c6   \n",
      "2           P003  696bb3922216926553e87730  696bb3922216926553e87730   \n",
      "3           P004  58d4c4c287b2420001263b6a  58d4c4c287b2420001263b6a   \n",
      "4           P005  650b03136ab3d4c832d98b71  650b03136ab3d4c832d98b71   \n",
      "\n",
      "            completion_status  duration_minutes  \n",
      "0  terminated_attention_check          9.800317  \n",
      "1                   completed         11.804617  \n",
      "2                   completed          7.860350  \n",
      "3                   completed         12.351267  \n",
      "4                   completed         17.417950  \n",
      "\n",
      "--- Anonymized data (first 5 columns) ---\n",
      "  participant_id           completion_status  duration_minutes  \\\n",
      "0           P001  terminated_attention_check          9.800317   \n",
      "1           P002                   completed         11.804617   \n",
      "2           P003                   completed          7.860350   \n",
      "3           P004                   completed         12.351267   \n",
      "4           P005                   completed         17.417950   \n",
      "\n",
      "       block_order  n_speaker_trials  \n",
      "0        inf_persm                17  \n",
      "1  persm_persp_inf                30  \n",
      "2  inf_persm_persp                30  \n",
      "3  persm_inf_persp                30  \n",
      "4  persm_persp_inf                30  \n"
     ]
    }
   ],
   "source": [
    "# Process the pilot study data\n",
    "input_path_pilot = './raw_do_not_track/prag_net_speaker_n1_pilot/'  # Folder containing CSV files, or single CSV file path\n",
    "df_raw_pilot = process_all_participants(input_path_pilot)\n",
    "\n",
    "# process the main study data\n",
    "input_path_main = './raw_do_not_track/prag_net_speaker_n1_main/'  # Folder containing CSV files, or single CSV file path\n",
    "df_raw_main = process_all_participants(input_path_main)\n",
    "\n",
    "# Combine pilot and main data\n",
    "df_raw_pilot.insert(0, 'study', 'pilot')\n",
    "df_raw_main.insert(0, 'study', 'main')\n",
    "df_raw = pd.concat([df_raw_pilot, df_raw_main], ignore_index=True)\n",
    "\n",
    "# Add participant_id\n",
    "df = add_participant_id(df_raw)\n",
    "\n",
    "# Print summary\n",
    "print_summary(df)\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n--- Full data (first 5 columns) ---\")\n",
    "print(df[['participant_id', 'subject_id', 'prolific_pid', 'completion_status', 'duration_minutes']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d762f86e",
   "metadata": {},
   "source": [
    "## Cell 7: View Key Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a2e219e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking uniqueness of prolific_pid:  True\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking uniqueness of prolific_pid: \", df_raw[\"prolific_pid\"].is_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a07402ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['participant_id',\n",
       " 'study',\n",
       " 'subject_id',\n",
       " 'prolific_pid',\n",
       " 'study_id',\n",
       " 'session_id',\n",
       " 'experiment_version',\n",
       " 'start_time',\n",
       " 'completion_status',\n",
       " 'completion_time',\n",
       " 'terminated_early',\n",
       " 'termination_reason',\n",
       " 'duration_minutes',\n",
       " 'total_time_elapsed_ms',\n",
       " 'block_1_scenario',\n",
       " 'block_2_scenario',\n",
       " 'block_3_scenario',\n",
       " 'block_order',\n",
       " 'attention_total_failures',\n",
       " 'attention_block_1_passed',\n",
       " 'attention_block_1_round',\n",
       " 'attention_block_1_time_elapsed',\n",
       " 'attention_block_1_num_effective',\n",
       " 'attention_block_1_stimulus_variant',\n",
       " 'attention_block_1_required_description',\n",
       " 'attention_block_2_passed',\n",
       " 'attention_block_2_round',\n",
       " 'attention_block_2_time_elapsed',\n",
       " 'attention_block_2_num_effective',\n",
       " 'attention_block_2_stimulus_variant',\n",
       " 'attention_block_2_required_description',\n",
       " 'attention_block_3_passed',\n",
       " 'attention_block_3_round',\n",
       " 'attention_block_3_time_elapsed',\n",
       " 'attention_block_3_num_effective',\n",
       " 'attention_block_3_stimulus_variant',\n",
       " 'attention_block_3_required_description',\n",
       " 'comp1_some_correct',\n",
       " 'comp1_some_rt',\n",
       " 'comp1_some_response',\n",
       " 'comp1_most_correct',\n",
       " 'comp1_most_rt',\n",
       " 'comp1_most_response',\n",
       " 'comp2_1_correct',\n",
       " 'comp2_1_time_elapsed',\n",
       " 'comp2_1_num_effective',\n",
       " 'comp2_1_statement',\n",
       " 'comp2_2_correct',\n",
       " 'comp2_2_time_elapsed',\n",
       " 'comp2_2_num_effective',\n",
       " 'comp2_2_statement',\n",
       " 'comp3_correct',\n",
       " 'comp3_time_elapsed',\n",
       " 'comp3_selected',\n",
       " 'inf_role_comp_correct',\n",
       " 'inf_role_comp_selected',\n",
       " 'inf_role_comp_time_elapsed',\n",
       " 'persp_role_comp_correct',\n",
       " 'persp_role_comp_selected',\n",
       " 'persp_role_comp_time_elapsed',\n",
       " 'persm_role_comp_correct',\n",
       " 'persm_role_comp_selected',\n",
       " 'persm_role_comp_time_elapsed',\n",
       " 'n_speaker_trials',\n",
       " 'inf_n_trials',\n",
       " 'inf_r1_num_effective',\n",
       " 'inf_r1_variant',\n",
       " 'inf_r1_positions',\n",
       " 'inf_r1_predicate',\n",
       " 'inf_r1_quantifier',\n",
       " 'inf_r1_time_elapsed',\n",
       " 'inf_r1_rt_approx',\n",
       " 'inf_r2_num_effective',\n",
       " 'inf_r2_variant',\n",
       " 'inf_r2_positions',\n",
       " 'inf_r2_predicate',\n",
       " 'inf_r2_quantifier',\n",
       " 'inf_r2_time_elapsed',\n",
       " 'inf_r2_rt_approx',\n",
       " 'inf_r3_num_effective',\n",
       " 'inf_r3_variant',\n",
       " 'inf_r3_positions',\n",
       " 'inf_r3_predicate',\n",
       " 'inf_r3_quantifier',\n",
       " 'inf_r3_time_elapsed',\n",
       " 'inf_r3_rt_approx',\n",
       " 'inf_r4_num_effective',\n",
       " 'inf_r4_variant',\n",
       " 'inf_r4_positions',\n",
       " 'inf_r4_predicate',\n",
       " 'inf_r4_quantifier',\n",
       " 'inf_r4_time_elapsed',\n",
       " 'inf_r4_rt_approx',\n",
       " 'inf_r5_num_effective',\n",
       " 'inf_r5_variant',\n",
       " 'inf_r5_positions',\n",
       " 'inf_r5_predicate',\n",
       " 'inf_r5_quantifier',\n",
       " 'inf_r5_time_elapsed',\n",
       " 'inf_r5_rt_approx',\n",
       " 'inf_r6_num_effective',\n",
       " 'inf_r6_variant',\n",
       " 'inf_r6_positions',\n",
       " 'inf_r6_predicate',\n",
       " 'inf_r6_quantifier',\n",
       " 'inf_r6_time_elapsed',\n",
       " 'inf_r6_rt_approx',\n",
       " 'inf_r7_num_effective',\n",
       " 'inf_r7_variant',\n",
       " 'inf_r7_positions',\n",
       " 'inf_r7_predicate',\n",
       " 'inf_r7_quantifier',\n",
       " 'inf_r7_time_elapsed',\n",
       " 'inf_r7_rt_approx',\n",
       " 'inf_r8_num_effective',\n",
       " 'inf_r8_variant',\n",
       " 'inf_r8_positions',\n",
       " 'inf_r8_predicate',\n",
       " 'inf_r8_quantifier',\n",
       " 'inf_r8_time_elapsed',\n",
       " 'inf_r8_rt_approx',\n",
       " 'inf_r9_num_effective',\n",
       " 'inf_r9_variant',\n",
       " 'inf_r9_positions',\n",
       " 'inf_r9_predicate',\n",
       " 'inf_r9_quantifier',\n",
       " 'inf_r9_time_elapsed',\n",
       " 'inf_r9_rt_approx',\n",
       " 'inf_r10_num_effective',\n",
       " 'inf_r10_variant',\n",
       " 'inf_r10_positions',\n",
       " 'inf_r10_predicate',\n",
       " 'inf_r10_quantifier',\n",
       " 'inf_r10_time_elapsed',\n",
       " 'inf_r10_rt_approx',\n",
       " 'persp_n_trials',\n",
       " 'persp_r1_num_effective',\n",
       " 'persp_r1_variant',\n",
       " 'persp_r1_positions',\n",
       " 'persp_r1_predicate',\n",
       " 'persp_r1_quantifier',\n",
       " 'persp_r1_time_elapsed',\n",
       " 'persp_r1_rt_approx',\n",
       " 'persp_r2_num_effective',\n",
       " 'persp_r2_variant',\n",
       " 'persp_r2_positions',\n",
       " 'persp_r2_predicate',\n",
       " 'persp_r2_quantifier',\n",
       " 'persp_r2_time_elapsed',\n",
       " 'persp_r2_rt_approx',\n",
       " 'persp_r3_num_effective',\n",
       " 'persp_r3_variant',\n",
       " 'persp_r3_positions',\n",
       " 'persp_r3_predicate',\n",
       " 'persp_r3_quantifier',\n",
       " 'persp_r3_time_elapsed',\n",
       " 'persp_r3_rt_approx',\n",
       " 'persp_r4_num_effective',\n",
       " 'persp_r4_variant',\n",
       " 'persp_r4_positions',\n",
       " 'persp_r4_predicate',\n",
       " 'persp_r4_quantifier',\n",
       " 'persp_r4_time_elapsed',\n",
       " 'persp_r4_rt_approx',\n",
       " 'persp_r5_num_effective',\n",
       " 'persp_r5_variant',\n",
       " 'persp_r5_positions',\n",
       " 'persp_r5_predicate',\n",
       " 'persp_r5_quantifier',\n",
       " 'persp_r5_time_elapsed',\n",
       " 'persp_r5_rt_approx',\n",
       " 'persp_r6_num_effective',\n",
       " 'persp_r6_variant',\n",
       " 'persp_r6_positions',\n",
       " 'persp_r6_predicate',\n",
       " 'persp_r6_quantifier',\n",
       " 'persp_r6_time_elapsed',\n",
       " 'persp_r6_rt_approx',\n",
       " 'persp_r7_num_effective',\n",
       " 'persp_r7_variant',\n",
       " 'persp_r7_positions',\n",
       " 'persp_r7_predicate',\n",
       " 'persp_r7_quantifier',\n",
       " 'persp_r7_time_elapsed',\n",
       " 'persp_r7_rt_approx',\n",
       " 'persp_r8_num_effective',\n",
       " 'persp_r8_variant',\n",
       " 'persp_r8_positions',\n",
       " 'persp_r8_predicate',\n",
       " 'persp_r8_quantifier',\n",
       " 'persp_r8_time_elapsed',\n",
       " 'persp_r8_rt_approx',\n",
       " 'persp_r9_num_effective',\n",
       " 'persp_r9_variant',\n",
       " 'persp_r9_positions',\n",
       " 'persp_r9_predicate',\n",
       " 'persp_r9_quantifier',\n",
       " 'persp_r9_time_elapsed',\n",
       " 'persp_r9_rt_approx',\n",
       " 'persp_r10_num_effective',\n",
       " 'persp_r10_variant',\n",
       " 'persp_r10_positions',\n",
       " 'persp_r10_predicate',\n",
       " 'persp_r10_quantifier',\n",
       " 'persp_r10_time_elapsed',\n",
       " 'persp_r10_rt_approx',\n",
       " 'persm_n_trials',\n",
       " 'persm_r1_num_effective',\n",
       " 'persm_r1_variant',\n",
       " 'persm_r1_positions',\n",
       " 'persm_r1_predicate',\n",
       " 'persm_r1_quantifier',\n",
       " 'persm_r1_time_elapsed',\n",
       " 'persm_r1_rt_approx',\n",
       " 'persm_r2_num_effective',\n",
       " 'persm_r2_variant',\n",
       " 'persm_r2_positions',\n",
       " 'persm_r2_predicate',\n",
       " 'persm_r2_quantifier',\n",
       " 'persm_r2_time_elapsed',\n",
       " 'persm_r2_rt_approx',\n",
       " 'persm_r3_num_effective',\n",
       " 'persm_r3_variant',\n",
       " 'persm_r3_positions',\n",
       " 'persm_r3_predicate',\n",
       " 'persm_r3_quantifier',\n",
       " 'persm_r3_time_elapsed',\n",
       " 'persm_r3_rt_approx',\n",
       " 'persm_r4_num_effective',\n",
       " 'persm_r4_variant',\n",
       " 'persm_r4_positions',\n",
       " 'persm_r4_predicate',\n",
       " 'persm_r4_quantifier',\n",
       " 'persm_r4_time_elapsed',\n",
       " 'persm_r4_rt_approx',\n",
       " 'persm_r5_num_effective',\n",
       " 'persm_r5_variant',\n",
       " 'persm_r5_positions',\n",
       " 'persm_r5_predicate',\n",
       " 'persm_r5_quantifier',\n",
       " 'persm_r5_time_elapsed',\n",
       " 'persm_r5_rt_approx',\n",
       " 'persm_r6_num_effective',\n",
       " 'persm_r6_variant',\n",
       " 'persm_r6_positions',\n",
       " 'persm_r6_predicate',\n",
       " 'persm_r6_quantifier',\n",
       " 'persm_r6_time_elapsed',\n",
       " 'persm_r6_rt_approx',\n",
       " 'persm_r7_num_effective',\n",
       " 'persm_r7_variant',\n",
       " 'persm_r7_positions',\n",
       " 'persm_r7_predicate',\n",
       " 'persm_r7_quantifier',\n",
       " 'persm_r7_time_elapsed',\n",
       " 'persm_r7_rt_approx',\n",
       " 'persm_r8_num_effective',\n",
       " 'persm_r8_variant',\n",
       " 'persm_r8_positions',\n",
       " 'persm_r8_predicate',\n",
       " 'persm_r8_quantifier',\n",
       " 'persm_r8_time_elapsed',\n",
       " 'persm_r8_rt_approx',\n",
       " 'persm_r9_num_effective',\n",
       " 'persm_r9_variant',\n",
       " 'persm_r9_positions',\n",
       " 'persm_r9_predicate',\n",
       " 'persm_r9_quantifier',\n",
       " 'persm_r9_time_elapsed',\n",
       " 'persm_r9_rt_approx',\n",
       " 'persm_r10_num_effective',\n",
       " 'persm_r10_variant',\n",
       " 'persm_r10_positions',\n",
       " 'persm_r10_predicate',\n",
       " 'persm_r10_quantifier',\n",
       " 'persm_r10_time_elapsed',\n",
       " 'persm_r10_rt_approx',\n",
       " 'feedback_text',\n",
       " 'source_file']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View key columns (full version with IDs)\n",
    "df.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3946d76",
   "metadata": {},
   "source": [
    "## Cell 8: Save Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e2c86216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved full data to: ./raw_do_not_track/processed_speaker_n1_full.csv\n",
      "  Columns: 279\n",
      "  Includes: participant_id, subject_id, prolific_pid, timestamps, etc.\n"
     ]
    }
   ],
   "source": [
    "# === EDIT THESE PATHS ===\n",
    "OUTPUT_PATH_FULL = './raw_do_not_track/processed_speaker_n1_full.csv'\n",
    "\n",
    "# Save full version (with all IDs)\n",
    "df.to_csv(OUTPUT_PATH_FULL, index=False)\n",
    "print(f\"Saved full data to: {OUTPUT_PATH_FULL}\")\n",
    "print(f\"  Columns: {len(df.columns)}\")\n",
    "print(f\"  Includes: participant_id, subject_id, prolific_pid, timestamps, etc.\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "programming-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
